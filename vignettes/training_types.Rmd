---
title: "Training Types"
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r, eval=FALSE, message=FALSE}
devtools::install_github("GenomeNet/deepG")
library(deepG)
library(magrittr)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
devtools::load_all(path = "~/deepGdev")
library(deepG)
library(magrittr)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{css, echo=FALSE}
mark.in {
  background-color: CornflowerBlue;
}

mark.out {
  background-color: IndianRed;
}

```

The deepG library offers several options to extract input/target pairs from data. We can differentiate between to main 
approach: 

* **Language model:** predict a character or several characters in a sequence.
* **Label Classification:** map a label to a sequence.

## Language model

With language model, we mean a model that predicts a character in a sequence.
We have several options to determine the output format of the data generator using the `output_format` argument. 

The `output_format` determines the shape of the output for a language model, i.e. part of a sequence is the input $X$ and another the
target $Y$. Assume a sequence <tt>abcdefg</tt> and `maxlen = 6`. Output correspond as follows

**"target_right"**: $X=$  <tt>abcdef</tt>, $Y=$  <tt>g</tt> 

**"target_middle_lstm"**: $X =$ ($X_1 =$ <tt>abc</tt>, $X_2 =$ <tt>gfe</tt>), $Y=$ <tt>d</tt> (note reversed order of $X_2$)

**"target_middle_cnn"**: $X =$ <tt>abcefg</tt>, $Y =$ <tt>d</tt> 

**"wavenet"**: $X =$ <tt>abcdef</tt>, $Y =$ <tt>bcdefg</tt>


### Create dummy data

To test the different language model options, we create a simple dummy data set consisting of a repetition of the sequence "AAACCCGGGTTTAAACCC...". 

```{r warning = FALSE}
vocabulary <- c("A", "C", "G", "T")
base_seq <- "AAACCCGGGTTT"
full_seq <- rep(base_seq, 50) %>% paste(collapse = "")
df <- data.frame(Header = "header", Sequence = full_seq)

# create training fasta file
train_dir <- tempfile()
dir.create(train_dir)
microseq::writeFasta(df, file.path(train_dir, "train_1.fasta"))
# create validation fasta file (use same data as training)
val_dir <- tempfile()
dir.create(val_dir)
microseq::writeFasta(df, file.path(val_dir, "val_1.fasta"))
```


### Predict next character

Say we want to predict the next character in a sequence given the last 5 characters and our text consists of the letters <tt>A,C,G,T</tt> . First we have to create a model. We may use a model with 1 LSTM and 1 dense layer for predictions.   

```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 5,
  layer_lstm = c(8),
  layer_dense = c(4),
  learning_rate = 0.1,
  vocabulary_size = 4 # text consists of A,C,G,T
)
```

Next we have to specify the location of our training and validation data and the output format of the data generator    

```{r warning = FALSE, message = FALSE}
hist <- train_model(train_type = "lm", # running a language model 
                    model = model,
                    path = train_dir,
                    path_val = val_dir,
                    steps_per_epoch = 5, # use 5 batches per epoch
                    train_val_ratio = 0.2, # use 20% of samples for validation compared to train
                    batch_size = 16,
                    epochs = 4, 
                    output_format = "target_right" # predict target at end of sequence 
)
plot(hist)
```

### Predict character in middle of sequence 

If we want to predict a character in the middle of a sequence and use LSTM layers, we should split our input into two layers. One layer handles the sequence 
before and one the input after the target. If, for example 

sequence:
<tt> 
     ACCG<mark class="in">T</mark>GGAA<br>
</tt>

then first input corresponds to <tt>ACCG</tt> and second to <tt>AAGG</tt>. We may create a model with two input layers using the `create_model_cnn_lstm_target_middle`

```{r warning = FALSE}
model <- create_model_lstm_cnn_target_middle(
  maxlen = 5,
  layer_lstm = c(8),
  layer_dense = c(4),
  learning_rate = 0.1,
  vocabulary_size = 4 # text consists of A,C,G,T
)
```

The `train_model` call is identical to the previous model, except we have to change the output format of the generator by setting `output_format = "target_middle_lstm"`. This reverses the order of the sequence after the target.  

```{r warning = FALSE, message = FALSE}
hist <- train_model(train_type = "lm", # running a language model 
                    model = model,
                    path = train_dir,
                    path_val = val_dir,
                    steps_per_epoch = 5, # use 5 batches per epoch
                    train_val_ratio = 0.2, # use 20% of samples for validation compared to train
                    batch_size = 16,
                    epochs = 4, 
                    output_format = "target_middle_lstm" # predict target at end of sequence 
)
plot(hist)
```


## Label classification

With label classification, we describe the task of mapping a label to a sequence. 
For example: given the sequence \texttt{ACGACCG}, does the sequence belong to a viral or bacterial genome?

deepG offers three options to map a label to a sequence 

1. the label gets read from the fasta header

2. files from every class are in separate folders

3. get label from csv file


### Create dummy data

To test label classification, we create a simple dummy data set. One class consists of random sequences using just "A" and "C" and second class
uses just "G" and "T".

```{r warning = FALSE}
# create training fasta files
train_dir_1 <- tempfile()
train_dir_2 <- tempfile()
dir.create(train_dir_1)
dir.create(train_dir_2)
train_dir <- list(train_dir_1, train_dir_2)

for (i in 1:2) {
  
  if (i == 1) {
    vocabulary <- c("A", "C")
    header <- "label_1"
    fasta_name_start <- "label_1_train_file"
  } else {
    vocabulary <- c("G", "T")
    header <- "label_2"
    fasta_name_start <- "label_2_train_file"
  }
  
  create_dummy_data(file_path = train_dir[[i]],
                    num_files = 3,
                    seq_length = 20, 
                    num_seq = 5,
                    header = header,
                    fasta_name_start = fasta_name_start,
                    vocabulary = vocabulary)
}  

# create validation fasta files
val_dir_1 <- tempfile()
val_dir_2 <- tempfile()
dir.create(val_dir_1)
dir.create(val_dir_2)
val_dir <- list(val_dir_1, val_dir_2)

for (i in 1:2) {
  
  if (i == 1) {
    vocabulary <- c("A", "C")
    header <- "label_1"
    fasta_name_start <- "label_1_val_file"
  } else {
    vocabulary <- c("G", "T")
    header <- "label_2"
    fasta_name_start <- "label_2_val_file"
  }
  
  create_dummy_data(file_path = val_dir[[i]],
                    num_files = 3,
                    seq_length = 20, 
                    num_seq = 5,
                    header = header,
                    fasta_name_start = fasta_name_start,
                    vocabulary = vocabulary)
}  
```

### Label by folder

In this approach, we put all data from one class into a separate folder. Say we want to classify if a sequence belongs to a viral or bacterial genome. We may put all virus and bacteria files into their own folder. In this case the `path` and `path_val` arguments should be vectors, where each entry is the path to one class.    

First we have to create a model. We may use a model with 1 LSTM and 1 dense layer for predictions. An input sequence has length 5.     

```{r warning = FALSE}
model <- create_model_lstm_cnn(
  maxlen = 5,
  layer_lstm = c(8),
  learning_rate = 0.1,
  layer_dense = c(2), # binary classification
  vocabulary_size = 4 # text consists of A,C,G,T
)
```


```{r warning = FALSE, message = FALSE}
train_model(train_type = "label_folder", # reading label from folder  
            model = model,
            path = c(train_dir_1, # note that path has two entries 
                     train_dir_2),
            path_val = c(val_dir_1,
                         val_dir_2),
            steps_per_epoch = 5, # use 5 batches per epoch
            train_val_ratio = 0.2, 
            batch_size = 8,
            epochs = 2,
            vocabulary_label = c("label_1", "label_2") # names of classes
)
```


### Label by fasta header 

The fasta headers in our dummy data have the names "label_1" or "label_2"

```{r warning = FALSE, message = FALSE}
files <- list.files(train_dir_1, full.names = TRUE)
fasta_file <- microseq::readFasta(files[1])
head(fasta_file)
```

```{r warning = FALSE, message = FALSE}
train_model(train_type = "label_header", # reading label from fasta header  
            model = model,
            path = train_dir,
            path_val = val_dir,
            steps_per_epoch = 5, 
            train_val_ratio = 0.2, 
            batch_size = 8,
            epochs = 2,
            vocabulary_label = c("label_1", "label_2") # names of labels
)
```


### Label from csv file

In this approach we extract the sequence label by mapping the current file name to a csv table.  

```{r warning = FALSE, message = FALSE}
files_1 <- basename(list.files(c(train_dir_1, val_dir_1)))
files_2 <- basename(list.files(c(train_dir_2, val_dir_2)))
file <- c(files_1, files_2)
label_1 <- stringr::str_detect(file, "label_1") %>% as.integer()
label_2 <- stringr::str_detect(file, "label_2") %>% as.integer()
df <- data.frame(file, label_1, label_2)
df

csv_path <- tempfile(fileext = ".csv")
write.csv(df, csv_path, row.names = FALSE)

hist <- train_model(train_type = "label_csv",
                    target_from_csv = csv_path,
                    model = model,
                    path = train_dir,
                    path_val = val_dir,
                    steps_per_epoch = 5,
                    train_val_ratio = 0.2,
                    batch_size = 8,
                    epochs = 2)

plot(hist)
```
