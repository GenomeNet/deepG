---
title: "Integrated Gradient"
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r, eval=FALSE, message=FALSE}
devtools::install_github("GenomeNet/deepG")
library(deepG)
library(magrittr)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
devtools::load_all(path = "~/deepGdev")
library(deepG)
library(magrittr)
```

```{css, echo=FALSE}
mark.in {
  background-color: CornflowerBlue;
}

mark.out {
  background-color: IndianRed;
}

```

# Introduction 

The  <a href="https://arxiv.org/abs/1703.01365">Integrated Gradient</a> (IG) method can be used to determine what parts of an input sequence are important for the models decision.
We start with training a model that can differentiate sequences based on the GC content 
(as described in the <a href="articles/getting_started.html">Getting started tutorial</a>). 


### Model Training

We create two simple dummy training and validation data sets. Both consist of random <tt>ACGT</tt> sequences but the first category has 
a probabiltiy of 40% each for drawing <tt>G</tt> or <tt>C</tt> and the second has equal probability for each nucleotide (first category has around 80% <tt>GC</tt> content and second one around 50%).   

```{r warning = FALSE}
set.seed(123)

# Create data 
vocabulary <- c("A", "C", "G", "T")
data_type <- c("train_1", "train_2", "val_1", "val_2")

for (i in 1:length(data_type)) {
  
  temp_file <- tempfile()
  assign(paste0(data_type[i], "_dir"), temp_file)
  dir.create(temp_file)
  
  if (i %% 2 == 1) {
    header <- "label_1"
    prob <- c(0.1, 0.4, 0.4, 0.1)
  } else {
    header <- "label_2"
    prob <- rep(0.25, 4)
  }
  fasta_name_start <- paste0(header, "_", data_type[i], "file")
  
  create_dummy_data(file_path = temp_file,
                    num_files = 1,
                    seq_length = 10000, 
                    num_seq = 1,
                    header = header,
                    prob = prob,
                    fasta_name_start = fasta_name_start,
                    vocabulary = vocabulary)
  
}

# Create model
maxlen <- 50
model <- create_model_lstm_cnn(maxlen = maxlen,
                               layer_lstm = 16,
                               layer_dense = c(8, 2))

model <- create_model_lstm_cnn(maxlen = maxlen,
                               filters = c(8, 16),
                               kernel_size = c(8, 8),
                               pool_size = c(3, 3),
                               layer_lstm = 8,
                               layer_dense = c(4, 2))

# Train model
hist <- train_model(model,
                    train_type = "label_folder",
                    run_name = "gc_model_1",
                    path = c(train_1_dir, train_2_dir),
                    path_val = c(val_1_dir, val_2_dir),
                    epochs = 6, 
                    batch_size = 64,
                    steps_per_epoch = 50, 
                    step = 50, 
                    vocabulary_label = c("high_gc", "equal_dist"))

plot(hist)
```


## Integrated Gradient

We can try to visualize what parts of an input sequence is impotant for the models decision, using Integrated Gradient.
Let's create a sequence with a high GC content.

```{r warning = FALSE}
set.seed(321)
high_gc_seq <- c(rep("G", 20), rep("C", 20), rep("A", 5), rep("T", 5))
high_gc_seq <- high_gc_seq[sample(maxlen)] %>% paste(collapse = "")
high_gc_seq
```

We need to one-hot encode the sequence before applying Integrated Gradient.

```{r warning = FALSE}
high_gc_seq_one_hot <- seq_encoding_label(char_sequence = high_gc_seq,
                                          maxlen = 50,
                                          start_ind = 1,
                                          vocabulary = vocabulary)
head(high_gc_seq_one_hot[1,,])
```

Our model should be pretty confident, this sequences belongs to the first class

```{r warning = FALSE}
pred <- predict(model, high_gc_seq_one_hot, verbose = 0)
colnames(pred) <- c("high_gc", "equal_dist")
pred
```

We can visualize what parts where important for the prediction.

```{r warning = FALSE}
ig <- integrated_gradients(
  input_seq = high_gc_seq_one_hot,
  target_class_idx = 1,
  model = model)

heatmaps_integrated_grad(integrated_grads = ig,
                         input_seq = high_gc_seq_one_hot)
```
We may test how our models prediction changes if we exchange certain nucleotides in the inout sequence.
First, we look for the positions with the biggest and smalles IG sums.

```{r warning = FALSE}
ig <- as.array(ig)
row_sum_ig <- rowSums(ig) 
row_sum_ig
ig_order <- order(row_sum_ig)
```

We may change the nucleotide with the lowest score and observe the change in prediction confidence

```{r warning = FALSE}
# copy original sequence
high_gc_seq_one_hot_changed <- high_gc_seq_one_hot 

# prediction for original sequence
predict(model, high_gc_seq_one_hot, verbose = 0)

# change nts 
for (i in 1:20) {
  lowest_index <- ig_order[i]
  new_row <- rep(0, 4)
  nt_index_old <- which.min(ig[lowest_index, ])
  nt_index_new <- which.max(ig[lowest_index, ])
  new_row[nt_index_new] <- 1
  high_gc_seq_one_hot_changed[1, lowest_index, ] <- new_row
  cat("At position", lowest_index, "changing", vocabulary[nt_index_old], "to", vocabulary[nt_index_new], "\n")
  
  pred <- predict(model, high_gc_seq_one_hot_changed, verbose = 0)
  print(pred)
}
```

