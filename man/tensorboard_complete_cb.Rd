% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/callbacks.R
\name{tensorboard_complete_cb}
\alias{tensorboard_complete_cb}
\title{Tensorboard callback wrapper}
\usage{
tensorboard_complete_cb(
  default_arguments,
  model,
  path_tensorboard,
  run_name,
  train_type,
  path,
  train_val_ratio,
  batch_size,
  epochs,
  max_queue_size,
  lr_plateau_factor,
  patience,
  cooldown,
  steps_per_epoch,
  step,
  shuffle_file_order,
  initial_epoch,
  vocabulary,
  learning_rate,
  shuffle_input,
  vocabulary_label,
  solver,
  file_limit,
  reverse_complement,
  wavenet_format,
  cnn_format,
  create_model_function,
  vocabulary_size,
  gen_cb,
  argumentList,
  maxlen,
  labelGen,
  labelByFolder,
  vocabulary_label_size,
  tb_images = FALSE,
  stateful,
  target_middle,
  num_train_files,
  path_file_log,
  proportion_per_seq,
  skip_amb_nuc,
  max_samples,
  proportion_entries,
  train_with_gen,
  count_files = TRUE
)
}
\arguments{
\item{model}{A keras model.}

\item{path_tensorboard}{Path to tensorboard directory or \code{NULL}. If \code{NULL}, training not tracked on tensorboard.}

\item{run_name}{Name of the run. Name will be used to identify output from callbacks. If \code{NULL}, will use date as run name.
If name already present, will add \code{"_2"} to name or \code{"_{x+1}"} if name ends with \verb{_x}, where \code{x} is some integer.}

\item{train_type}{Either \code{"lm"}, \code{"lm_rds"}, \code{"masked_lm"} for language model; \code{"label_header"}, \code{"label_folder"}, \code{"label_csv"}, \code{"label_rds"} for classification or \code{"dummy_gen"}.
\itemize{
\item Language model is trained to predict character(s) in a sequence. \cr
\item \code{"label_header"}/\code{"label_folder"}/\code{"label_csv"} are trained to predict a corresponding class given a sequence as input.
\item If \code{"label_header"}, class will be read from fasta headers.
\item If \code{"label_folder"}, class will be read from folder, i.e. all files in one folder must belong to the same class.
\item If \code{"label_csv"}, targets are read from a csv file. This file should have one column named "file". The targets then correspond to entries in that row (except "file"
column). Example: if we are currently working with a file called "a.fasta" and corresponding label is "label_1", there should be a row in our csv file\tabular{lll}{
   file \tab label_1 \tab label_2 \cr
   "a.fasta" \tab 1 \tab 0 \cr
}


\item If \code{"label_rds"}, generator will iterate over set of .rds files containing each a list of input and target tensors. Not implemented for model
with multiple inputs.
\item If \code{"lm_rds"}, generator will iterate over set of .rds files and will split tensor according to \code{target_len} argument
(targets are last \code{target_len} nucleotides of each sequence).
\item  If \code{"dummy_gen"}, generator creates random data once and repeatedly feeds these to model.
\item  If \code{"masked_lm"}, generator maskes some parts of the input. See \code{masked_lm} argument for details.
}}

\item{path}{Path to training data. If \code{train_type} is \code{label_folder}, should be a vector or list
where each entry corresponds to a class (list elements can be directories and/or individual files). If \code{train_type} is not \code{label_folder},
can be a single directory or file or a list of directories and/or files.}

\item{train_val_ratio}{For generator defines the fraction of batches that will be used for validation (compared to size of training data), i.e. one validation iteration
processes \code{batch_size} \eqn{*} \code{steps_per_epoch} \eqn{*} \code{train_val_ratio} samples. If you use dataset instead of generator and \code{dataset_val} is \code{NULL}, splits \code{dataset}
into train/validation data.}

\item{batch_size}{Number of samples used for one network update.}

\item{epochs}{Number of iterations.}

\item{max_queue_size}{Maximum size for the generator queue.}

\item{lr_plateau_factor}{Factor of decreasing learning rate when plateau is reached.}

\item{patience}{Number of epochs waiting for decrease in validation loss before reducing learning rate.}

\item{cooldown}{Number of epochs without changing learning rate.}

\item{steps_per_epoch}{Number of training batches per epoch.}

\item{step}{Frequency of sampling steps.}

\item{shuffle_file_order}{Boolean, whether to go through files sequentially or shuffle beforehand.}

\item{initial_epoch}{Epoch at which to start training. Note that network
will run for (\code{epochs} - \code{initial_epochs}) rounds and not \code{epochs} rounds.}

\item{vocabulary}{Vector of allowed characters. Characters outside vocabulary get encoded as specified in \code{ambiguous_nuc}.}

\item{shuffle_input}{Whether to shuffle entries in file.}

\item{vocabulary_label}{Character vector of possible targets. Targets outside \code{vocabulary_label} will get discarded if
\code{train_type = "label_header"}.}

\item{file_limit}{Integer or \code{NULL}. If integer, use only specified number of randomly sampled files for training. Ignored if greater than number of files in \code{path}.}

\item{reverse_complement}{Boolean, for every new file decide randomly to use original data or its reverse complement.}

\item{maxlen}{Length of predictor sequence.}

\item{tb_images}{Whether to show custom images (confusion matrix) in tensorboard "IMAGES" tab.}

\item{path_file_log}{Write name of files used for training to csv file if path is specified.}

\item{proportion_per_seq}{Numerical value between 0 and 1. Proportion of sequence to take samples from (use random subsequence).}

\item{skip_amb_nuc}{Threshold of ambiguous nucleotides to accept in fasta entry. Complete entry will get discarded otherwise.}

\item{max_samples}{Maximum number of samples to use from one file. If not \code{NULL} and file has more than \code{max_samples} samples, will randomly choose a
subset of \code{max_samples} samples.}

\item{proportion_entries}{Proportion of fasta entries to keep. For example, if fasta file has 50 entries and \code{proportion_entries = 0.1},
will randomly select 5 entries.}
}
\description{
Tensorboard callback wrapper
}
\keyword{internal}
