% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/generators.R
\name{generator_random}
\alias{generator_random}
\title{Randomly select samples from fasta files}
\usage{
generator_random(
  train_type = "label_folder",
  output_format = NULL,
  seed = 123,
  format = "fasta",
  reverse_complement = TRUE,
  path = NULL,
  batch_size = c(100),
  maxlen = 4,
  ambiguous_nuc = "equal",
  padding = FALSE,
  vocabulary = c("a", "c", "g", "t"),
  number_target_nt = NULL,
  n_gram = NULL,
  n_gram_stride = NULL,
  sample_by_file_size = TRUE,
  max_samples = 1,
  skip_amb_nuc = NULL,
  vocabulary_label = NULL,
  target_from_csv = NULL,
  target_split = NULL,
  max_iter = 10000,
  verbose = TRUE,
  set_learning = NULL,
  shuffle_input = TRUE,
  reverse_complement_encoding = FALSE,
  proportion_entries = NULL,
  concat_seq = NULL
)
}
\arguments{
\item{train_type}{Either "lm", "lm_rds" for language model; "label_header", "label_folder", "label_csv", "label_rds" for classification or "dummy_gen".
Language model is trained to predict character in sequence.
"label_header"/"label_folder"/"label_csv" are trained to predict a corresponding class, given a sequence as input. If "label_header", class will be read from fasta headers.
If "label_folder", class will be read from folder, i.e. all files in one folder must belong to the same class.
If "label_csv", targets are read from a csv file. This file should have one column named "file". The targets then correspond to entries in that row (except "file"
column). Example: if we are currently working with a file called "a.fasta", there should be a row in our csv file
   file  |  label_1 | label_2
"a.fasta"     1          8
If "label_rds", generator will iterate over set of .rds files containing each a list of input and target tensors. Not implemented for model
with multiple inputs. If "lm_rds", generator will iterate over set of .rds files and will split tensor according to target_len argument
(targets are last target_len nucleotides of each sequence).
If "dummy_gen", generator creates random data once and repeatedly feeds these to model.}

\item{output_format}{Determines shape of output tensor for language model.
Either "target_right", "target_middle_lstm", "target_middle_cnn" or "wavenet".
Assume a sequence "AACCGTA". Output correspond as follows
"target_right": X = "AACCGT", Y = "A"
"target_middle_lstm": X = (X_1 = "AAC", X_2 = "ATG"), Y = "C" (note reversed order of X_2)
"target_middle_cnn": X = "AACGTA", Y = "C"
"wavenet": X = "AACCGT", Y = "ACCGTA"}

\item{seed}{Sets seed for set.seed function, for reproducible results when using \code{shuffle_file_order} or \code{shuffle_input}}

\item{format}{File format, either "fasta" or "fastq".}

\item{reverse_complement}{Logical, for every new file decide randomly to use original data or its reverse complement.}

\item{path}{Path to folder where individual or multiple FASTA or FASTQ files are located for training. If \code{train_type} is \code{label_folder}, should be a vector or list
where each entry corresponds to a class. If \code{train_type} is not \code{label_folder}, can be a list of directories and/or single files.}

\item{batch_size}{Number of batches.}

\item{maxlen}{Length of predictor sequence.}

\item{ambiguous_nuc}{How to handle nucleotides outside vocabulary, either "zero", "discard", "empirical" or "equal". If "zero", input gets encoded as zero vector;
if "equal" input is 1/length(vocabulary) x length(vocabulary). If "discard" samples containing nucleotides outside vocabulary get discarded.
If "empirical" use nucleotide distribution of current file.}

\item{padding}{Whether to pad sequences too short for one sample with zeros.}

\item{vocabulary}{Vector of allowed characters, character outside vocabulary get encoded as 0-vector.}

\item{n_gram}{Integer, encode target not nucleotide wise but combine n nucleotides at once. For example for n=2, "AA" ->  (1, 0,..., 0),
"AC" ->  (0, 1, 0,..., 0), "TT" -> (0,..., 0, 1), where the one-hot vectors have length length(vocabulary)^n.}

\item{n_gram_stride}{Step size for n-gram encoding. For AACCGGTT with n-gram = 4 and n_gram_stride = 2, generator encodes
(AACC), (CCGG), (GGTT); for n_gram_stride = 4 generator encodes (AACC), (GGTT).}

\item{sample_by_file_size}{Sample new file weighted by file size (possible to repeatedly sample the same file).}

\item{max_samples}{Maximum number of samples to use from one file. If not NULL and file has more than \code{max_samples} samples, will randomly choose a
subset of \code{max_samples} samples.}

\item{skip_amb_nuc}{Threshold of ambiguous nucleotides to accept in fasta entry. Complete entry will get discarded otherwise.}

\item{vocabulary_label}{Character vector of possible targets. Targets outside \code{vocabulary_label} will get discarded.}

\item{target_from_csv}{Path to csv file with target mapping. One column should be called "file" and other entries in row are the targets.}

\item{target_split}{If target gets read from csv file, list of names to devide target tensor into list of tensors.
Example: if csv file has header names "file", "label_1", "label_2", "label_3" und target_split = list(c("label_1", "label_2"), "label_3"),
this will devide target matrix to list of length 2, where the first element contains columns named "label_1" and "label_2" and the
second entry contains the column named "label_3".}

\item{max_iter}{Stop after max_iter number of iterations failed to produce a new batch.}

\item{verbose}{Whether to show message.}

\item{set_learning}{When you want to assign one label to set of samples. Only implemented for train_type = "label_folder".
Input is a list with the following parameters
(1) \code{samples_per_target}, how many samples to use for one target; (2) \code{maxlen} length of one sample
(3) \code{reshape_mode} "time_dist", "multi_input" or "concat". If reshape_mode is "concat", there is an additional (4) \code{buffer_len}
argmument. If reshape_mode is "multi_input", generator will produce samples_per_target separate inputs, each of length maxlen (model should have
samples_per_target input layers). If reshape_mode is "time_dist", generator will produce a 4D input array. The dimensions correspond to
(batch_size, samples_per_target, maxlen, length(vocabulary)). If reshape mode is "concat", generator will concatenate samples_per_target sequences
of length maxlen to one long sequence; if buffer_len is an integer, the subsequences are interspaced with buffer_len rows. The input length is
(maxlen \* samples_per_target) + buffer_len \* (samples_per_target - 1)}

\item{shuffle_input}{Logical, shuffle entries in every fasta file before connecting them to sequence.}

\item{reverse_complement_encoding}{Boolean, use both original sequence and reverse complement as two input sequences.}

\item{proportion_entries}{Proportion of fasta entries to keep. For example, if fasta file has 50 entries and proportion_entries = 0.1,
will randomly select 5 entries.}

\item{concat_seq}{Character string or NULL. If not NULL all entries from file get concatenated to one sequence with concat_seq string between them.
Example: If 1.entry AACC, 2. entry TTTG and concat_seq = "ZZZ" this becomes AACCZZZTTTG.}
}
\description{
Other generator like \code{\link{generator_fasta_lm}}, \code{\link{generator_fasta_label_header_csv}}
or \code{\link{generator_fasta_label_folder}} will randomly choose a consecutive sequence of samples when
a \code{max_samples} argument is supplied. \code{generator_random} will choose samples at random.
}
