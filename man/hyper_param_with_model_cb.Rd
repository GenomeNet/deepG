% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/callbacks.R
\name{hyper_param_with_model_cb}
\alias{hyper_param_with_model_cb}
\title{hyperparameter callback}
\usage{
hyper_param_with_model_cb(
  default_arguments,
  model,
  tensorboard.log,
  run.name,
  train_type,
  model_path,
  path,
  validation.split,
  batch.size,
  epochs,
  max.queue.size,
  lr.plateau.factor,
  patience,
  cooldown,
  steps.per.epoch,
  step,
  randomFiles,
  initial_epoch,
  vocabulary,
  learning.rate,
  shuffleFastaEntries,
  labelVocabulary,
  solver,
  numberOfFiles,
  reverseComplements,
  wavenet_format,
  cnn_format
)
}
\arguments{
\item{model}{A keras model.}

\item{tensorboard.log}{Path to tensorboard log directory.}

\item{run.name}{Name of the run (without file ending). Name will be used to identify output from callbacks.}

\item{train_type}{Either "lm" for language model, "label_header", "label_folder" or "label_csv". Language model is trained to predict character in sequence.
"label_header"/"label_folder" are trained to predict a corresponding class, given a sequence as input. If "label_header", class will be read from fasta headers.
If "label_folder", class will be read from folder, i.e. all files in one folder must belong to the same class.
If "label_csv", targets are read from a csv file. This file should have one column names "file". The targets then correspond to entries in that row (except "file"
column). Example: if we are currently working with a file called "a.fasta", there should be a row in our csv file
   file  |  label_1 | label_2
"a.fasta"     1          8}

\item{path}{Path to folder where individual or multiple FASTA or FASTQ files are located for validation. If \code{train_type} is \code{label_folder}, should be a vector
containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.}

\item{validation.split}{Defines the fraction of the batches that will be used for validation (compared to size of training data), i.e. one validtion iteration
processed \code{batch.size} * \code{steps.per.epoch} * \code{validation.split samples}.}

\item{batch.size}{Number of samples that are used for one network update.}

\item{epochs}{Number of iterations.}

\item{max.queue.size}{Queue on fit_generator().}

\item{lr.plateau.factor}{Factor of decreasing learning rate when plateau is reached.}

\item{patience}{Number of epochs waiting for decrease in loss before reducing learning rate.}

\item{cooldown}{Number of epochs without changing learning rate.}

\item{steps.per.epoch}{Number of batches per epoch.}

\item{step}{Frequency of sampling steps.}

\item{randomFiles}{Boolean, whether to go through files sequentially or shuffle beforehand.}

\item{initial_epoch}{Epoch at which to start training. Note that network
will run for (\code{epochs} - \code{initial_epochs}) rounds and not \code{epochs} rounds.}

\item{vocabulary}{Vector of allowed characters. Character outside vocabulary get encoded as specified in \code{ambiguous_nuc}.}

\item{shuffleFastaEntries}{Logical, shuffle entries in file.}

\item{labelVocabulary}{Character vector of possible targets. Targets outside \code{labelVocabulary} will get discarded if
\code{train_type = "label_header"}.}

\item{numberOfFiles}{Use only specified number of files, ignored if greater than number of files in \code{path}.}

\item{reverseComplements}{Logical, for every new file decide randomly to use original data or its reverse complement.}
}
\description{
hyperparameter callback
}
