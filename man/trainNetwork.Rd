% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{trainNetwork}
\alias{trainNetwork}
\title{Trains a neural network on genomic data. Designed for developing genome based language models (GenomeNet)}
\usage{
trainNetwork(
  train_type = "lm",
  built_model = list(create_model_function = NULL, function_args = list()),
  model = NULL,
  path = NULL,
  path.val = NULL,
  dataset = NULL,
  dataset_val = NULL,
  checkpoint_path = NULL,
  validation.split = 0.2,
  run.name = "run",
  batch.size = 64,
  epochs = 10,
  max.queue.size = 100,
  reduce_lr_on_plateau = TRUE,
  lr.plateau.factor = 0.9,
  patience = 20,
  cooldown = 1,
  steps.per.epoch = 1000,
  step = 1,
  randomFiles = TRUE,
  initial_epoch = 0,
  vocabulary = c("a", "c", "g", "t"),
  tensorboard.log = NULL,
  save_best_only = TRUE,
  save_weights_only = FALSE,
  seed = c(1234, 4321),
  shuffleFastaEntries = TRUE,
  output = list(none = FALSE, checkpoints = FALSE, tensorboard = FALSE, log = FALSE,
    serialize_model = FALSE, full_model = FALSE),
  tb_images = FALSE,
  format = "fasta",
  fileLog = NULL,
  labelVocabulary = NULL,
  numberOfFiles = NULL,
  reverseComplements = FALSE,
  reverseComplementEncoding = FALSE,
  output_format = "target_right",
  reset_states = FALSE,
  ambiguous_nuc = "equal",
  proportion_per_file = NULL,
  read_data = FALSE,
  use_quality_score = FALSE,
  padding = FALSE,
  early_stopping_time = NULL,
  added_label_path = NULL,
  add_input_as_seq = NULL,
  target_from_csv = NULL,
  target_split = NULL,
  validation_only_after_training = FALSE,
  skip_amb_nuc = NULL,
  max_samples = NULL,
  split_seq = FALSE,
  class_weight = NULL,
  concat_seq = NULL,
  target_len = 1,
  print_scores = TRUE,
  train_val_split_csv = NULL,
  use_coverage = FALSE,
  set_learning = NULL,
  proportion_entries = NULL,
  sample_by_file_size = FALSE,
  n_gram = NULL,
  n_gram_stride = 1
)
}
\arguments{
\item{train_type}{Either "lm" for language model, "label_header", "label_folder", "label_csv" or "label_rds". Language model is trained to predict character in sequence.
"label_header"/"label_folder" are trained to predict a corresponding class, given a sequence as input. If "label_header", class will be read from fasta headers.
If "label_folder", class will be read from folder, i.e. all files in one folder must belong to the same class.
If "label_csv", targets are read from a csv file. This file should have one column names "file". The targets then correspond to entries in that row (except "file"
column). Example: if we are currently working with a file called "a.fasta", there should be a row in our csv file
   file  |  label_1 | label_2
"a.fasta"     1          8
If "label_rds", generator will iterate over set of .rds files containing each a list of input and target tensors. Not implemented for model
with multiple inputs. If "lm_rds", generator will iterate over set of .rds files and will split tensor according to target_len argument 
(targets are last target_len nucleotides of each sequence).}

\item{built_model}{Call to a function that creates a model. \code{create_model_function} can be either "create_model_lstm_cnn", "create_model_wavenet"
or "create_model_lstm_cnn_target_middle".
In \code{function_args} arguments of the corresponding can be specified, if no argument is given default values will be used.
Example: \code{built_model = list(create_model_function = "create_model_lstm_cnn", function_args = list(maxlen = 50, lstm_layer_size = 32, layers.lstm = 1)}}

\item{model}{A keras model.}

\item{path}{Path to folder where individual or multiple FASTA or FASTQ files are located for training. If \code{train_type} is \code{label_folder}, should be a vector
containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.}

\item{path.val}{Path to folder where individual or multiple FASTA or FASTQ files are located for validation. If \code{train_type} is \code{label_folder}, should be a vector
containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.}

\item{dataset}{List of training data, holding training samples in RAM instead of using generator. Should be list with two entries called "X" and "Y".}

\item{dataset_val}{List of validation data. Should be list with two entries.}

\item{checkpoint_path}{Path to checkpoints folder.}

\item{validation.split}{For generator, defines the fraction of batches that will be used for validation (compared to size of training data), i.e. one validtion iteration
processes \code{batch.size} x \code{steps.per.epoch} x \code{validation.split} samples. If you use dataset instead of generator and \code{dataset_val} is NULL, splits \code{dataset} 
into train/validation data.}

\item{run.name}{Name of the run (without file ending). Name will be used to identify output from callbacks.}

\item{batch.size}{Number of samples that are used for one network update.}

\item{epochs}{Number of iterations.}

\item{max.queue.size}{Queue on fit_generator().}

\item{reduce_lr_on_plateau}{Whether to use learning rate scheduler.}

\item{lr.plateau.factor}{Factor of decreasing learning rate when plateau is reached.}

\item{patience}{Number of epochs waiting for decrease in loss before reducing learning rate.}

\item{cooldown}{Number of epochs without changing learning rate.}

\item{steps.per.epoch}{Number of batches per epoch.}

\item{step}{Frequency of sampling steps.}

\item{randomFiles}{Boolean, whether to go through files sequentially or shuffle beforehand.}

\item{initial_epoch}{Epoch at which to start training. Note that network
will run for (\code{epochs} - \code{initial_epochs}) rounds and not \code{epochs} rounds.}

\item{vocabulary}{Vector of allowed characters. Character outside vocabulary get encoded as specified in \code{ambiguous_nuc}.}

\item{tensorboard.log}{Path to tensorboard log directory.}

\item{save_best_only}{Only save model that improved on best val_loss score.}

\item{save_weights_only}{Whether to save weights only.}

\item{seed}{Sets seed for set.seed function, for reproducible results when using \code{randomFiles} or \code{shuffleFastaEntries}}

\item{shuffleFastaEntries}{Logical, shuffle entries in file.}

\item{output}{List of optional outputs, no output if none is TRUE.}

\item{tb_images}{Boolean, whether to show plots in tensorboard.}

\item{format}{File format, "fasta" or "fastq".}

\item{fileLog}{Write name of files used for training to csv file if path is specified.}

\item{labelVocabulary}{Character vector of possible targets. Targets outside \code{labelVocabulary} will get discarded if
\code{train_type = "label_header"}.}

\item{numberOfFiles}{Use only specified number of files, ignored if greater than number of files in \code{path}.}

\item{reverseComplements}{Logical, for every new file decide randomly to use original data or its reverse complement.}

\item{reverseComplementEncoding}{Logical, use both original sequence and reverse.complement as two input sequences.}

\item{output_format}{Determines shape of output tensor for language model (if \code{train_type = "lm"}).
Either "target_right", "target_middle_lstm", "target_middle_cnn" or "wavenet".
Assume a sequence "AACCGTA". Output correspond as follows
"target_right": X = "AACCGT", Y = "A"
"target_middle_lstm": X = (X_1 = "AAC", X_2 = "ATG"), Y = "C" (note reversed order of X_2)
"target_middle_cnn": X = "AACGTA", Y = "C" (nucleotide in middle encoded as 0-vector)
"wavenet": X = "AACCGT", Y = "ACCGTA"
"dummy_gen": generator creates random data}

\item{reset_states}{Boolean, whether to reset hidden states of RNN layer at every new input file.}

\item{ambiguous_nuc}{How to handle nucleotides outside vocabulary, either "zero", "discard", "empirical" or "equal". If "zero", input gets encoded as zero vector; 
if "equal" input is 1/length(vocabulary) x length(vocabulary). If "discard" samples containing nucleotides outside vocabulary get discarded. 
If "empirical" use nucleotide distribution of current file.}

\item{proportion_per_file}{Numerical value between 0 and 1. Proportion of possible samples to take from one file. Takes samples from random subsequence.}

\item{read_data}{If true the first element of output is a list of length 2, each containing one part of paired read. Maxlen should be 2*length of one read.}

\item{use_quality_score}{Whether to use fastq qualitiy scores. If TRUE input is not one-hot-encoding but corresponds to probabilities.
For example (0.97, 0.01, 0.01, 0.01) instead of (1, 0, 0, 0).}

\item{padding}{Whether to pad sequences too short for one sample with zeros.}

\item{early_stopping_time}{Time in seconds after which to stop training.}

\item{added_label_path}{Path to folder with additional labels. Should be a csv file with one column named "file". Other columns should correspond to one label.}

\item{add_input_as_seq}{Boolean vector specifying for each entry in \code{added_label_path} if rows from csv should be encoded as a sequence or used directly.
If a row in your csv file is a sequence this should be true. For example you may want to add another sequence, say ACCGT. Then this would correspond to 1,2,2,3,4 in
csv file (if vocabulary = c("A", "C", "G", "T")).  If \code{add_input_as_seq} is TRUE, 12234 gets one-hot encoded, so added input is a 3D tensor.  If \code{add_input_as_seq} is 
FALSE this will feed network just raw data (a 2D tensor).}

\item{target_from_csv}{Path to csv file with target mapping. One column should be called "file" and other entries in row are the targets.}

\item{target_split}{If target gets read from csv file, list of names to devide target tensor into list of tensors.
Example: if csv file has header names "file", "label_1", "label_2", "label_3" und target_split = list(c("label_1", "label_2"), "label_3"),
this will devide target matrix to list of length 2, where the first element contains columns named "label_1" and "label_2" and the 
second entry contains the column named "label_3".}

\item{validation_only_after_training}{Boolean, whether to skip validation during training and only do one validation after training.}

\item{skip_amb_nuc}{Threshold of ambiguous nucleotides to accept in fasta entry. Complete entry will get discarded otherwise.}

\item{max_samples}{Maximum number of samples to use from one file. If not NULL and file has more than \code{max_samples} samples, will randomly choose a 
subset of \code{max_samples} samples.}

\item{split_seq}{Split input sequence into two sequences while removing nucleotide in middle. If input is x_1,..., x_(n+1), input gets split into 
input_1 = x_1,..., x_m and input_2 = x_(n+1),..., x_(m+2) where m = ceiling((n+1)/2) and n = maxlen. Note that x_(m+1) is not used. Can be used for transfer learning,
when switching from language model trained with target in middle to label classification.}

\item{class_weight}{List of weights for output. Order should correspond to \code{labelVocabulary}.
You can use \code{get_class_weight} function to estimates class weights: class_weights <- get_class_weights(path = path, train_type = train_type)
If train_type = "label_csv" you need to add path to csv file:
class_weights <- get_class_weights(path = path, train_type = train_type, csv_path = target_from_csv)}

\item{concat_seq}{Character string or NULL. If not NULL all entries from file get concatenated to one sequence with concat_seq string between them.
Example: If 1.entry AACC, 2. entry TTTG and concat_seq = "ZZZ" this becomes AACCZZZTTTG.}

\item{target_len}{Number of nucleotides to predict at once.}

\item{print_scores}{Whether to print train/validation scores during training.}

\item{train_val_split_csv}{A csv file specifying train/validation split. csv file should contain one column named "file" and one columnn named 
"type". The "file" column contains names of fasta/fastq files and "type" column specifies if file is used for training or validation. 
Entries in "type" must be named "train" or "val", otherwise file will not be used for either. path and path.val arguments should be the same.
Not implemented for train_type = "label_folder".}

\item{use_coverage}{Boolean. Whether to use coverage as encoding rather than one-hot encoding.}

\item{set_learning}{When you want to assign one label to set of samples. Only implemented for train_type = "label_folder".
Input is a list with the following parameters
(1) \code{samples_per_target}, how many samples to use for one target; (2) \code{maxlen} length of one sample 
(3) \code{reshape_mode} "time_dist" or "multi_input".}

\item{proportion_entries}{Proportion of fasta entries to keep. For example, if fasta file has 50 entries and proportion_entries = 0.1, 
will randomly select 5 entries.}

\item{sample_by_file_size}{Sample new file weighted by file size (possible to repeatedly sample the same file).}

\item{n_gram}{Encode n nucleotides at once.}
}
\description{
Depth and number of neurons per layer of the netwok can be specified.
If a path to a folder where FASTA files are located is provided, batches will be generated using an external generator which
is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()})
and keeps them in RAM.
}
