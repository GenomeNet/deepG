% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/create_model.R
\name{create_model_transformer}
\alias{create_model_transformer}
\title{Create transformer model}
\usage{
create_model_transformer(
  maxlen,
  vocabulary_size = 4,
  pos_encoding = "sinusoid",
  head_size = 4L,
  num_heads = 5L,
  dropout = 0,
  res_connection = TRUE,
  n = 10000,
  layer_dense = 2,
  dropout_dense = NULL,
  flatten_method = "flatten",
  last_layer_activation = "softmax",
  loss_fn = "categorical_crossentropy",
  solver = "adam",
  learning_rate = 0.01,
  label_noise_matrix = NULL,
  bal_acc = FALSE,
  f1_metric = FALSE,
  auc_metric = FALSE,
  label_smoothing = 0,
  verbose = TRUE,
  model_seed = NULL
)
}
\arguments{
\item{maxlen}{Length of predictor sequence.}

\item{vocabulary_size}{Number of unique character in vocabulary.}

\item{pos_encoding}{Either \code{"sinusoid"} or \code{"embedding"}. How to add positional information.
If \code{"sinusoid"}, will add sine waves of different frequencies to input.
If \code{"embedding"}, model learns positional embedding.}

\item{head_size}{Vector of dimensions of attention keys for each block.}

\item{num_heads}{Number of attention heads.}

\item{dropout}{Vector of dropout rates after attention block(s).}

\item{res_connection}{Whether to add residual connection between attention blocks.}

\item{n}{Frequency of sine waves for positional encoding. Only applied if \code{pos_encoding = "sinusoid"}.}

\item{layer_dense}{Vector specifying number of neurons per dense layer after last LSTM or CNN layer (if no LSTM used).}

\item{dropout_dense}{Dropout for dense layers.}

\item{flatten_method}{How to process output of last attention block. Can be \code{"gap_channels_last"}, \code{"gap_channels_first"}
or \code{"flatten"}. If \code{"gap_channels_last"} or \code{"gap_channels_first"}, will apply global average pooling. If \code{"flatten"}, will flatten
output after last attention block.}

\item{last_layer_activation}{Either \code{"sigmoid"} or \code{"softmax"}.}

\item{loss_fn}{Either \code{"categorical_crossentropy"} or \code{"binary_crossentropy"}. If \code{label_noise_matrix} given, will use custom \code{"noisy_loss"}.}

\item{solver}{Optimization method, options are \verb{"adam", "adagrad", "rmsprop"} or \code{"sgd"}.}

\item{learning_rate}{Learning rate for optimizer.}

\item{label_noise_matrix}{Matrix of label noises. Every row stands for one class and columns for percentage of labels in that class.
If first label contains 5 percent wrong labels and second label no noise, then

\code{label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE )}}

\item{bal_acc}{Whether to add balanced accuracy.}

\item{f1_metric}{Whether to add F1 metric.}

\item{auc_metric}{Whether to add AUC metric.}

\item{label_smoothing}{Float in [0, 1]. If 0, no smoothing is applied. If > 0, loss between the predicted
labels and a smoothed version of the true labels, where the smoothing squeezes the labels towards 0.5.
The closer the argument is to 1 the more the labels get smoothed.}

\item{verbose}{Boolean.}

\item{model_seed}{Set seed for model parameters in tensorflow if not \code{NULL}.}
}
\description{
Creates transformer network for classification. Model can consist of several stacked attention blocks.
}
\examples{

model <- create_model_transformer(maxlen = 50,
            head_size=c(4,7), num_heads=c(5,8), dropout=c(0.3,0.5))

}
