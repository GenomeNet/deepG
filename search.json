[{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Data Generator","text":"common use case deepG data generator extract samples collection fasta (fastq) files. generator always return list length 2. first element input XX second target YY. can differentiate 2 approaches Example: Predict next nucleotide given previous 100 nucleotides. Example: Assign label “virus” “bacteria” sequence length 100. Suppose given 2 fasta files called “.fasta” “b.fasta” look follows:  want extract sequences length 4 files, 17 possible samples (5 AACCAAGG, 3 TTTGGG, …). naive approach extract samples sequential manner: 1. sample:  2. sample:  …  17. sample:  18. sample:  … longer sequences desirable strategy since data redundant (often just one nucleotide difference) model often see long stretches data source. Choosing samples completely random can also problematic since constantly open new files. deepG generators offers several option navigate data sampling strategy achieve good balance two approaches.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"data-generator-options","dir":"Articles","previous_headings":"","what":"Data generator options","title":"Data Generator","text":"following code examples, mostly use sequence  abcdefghiiii  demonstrate deepG data generator options. (real world application usually sequences ACGT vocabulary.) may store sequence fasta file Since neural networks can work numeric data, encode sequences characters numeric data. Usually achieved one-hot-encoding; approaches implemented: see use_coverage, use_quality_score ambiguous_nuc sections.","code":"sequence <- paste0(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"i\", \"i\", \"i\") vocabulary <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\") temp_dir <- tempfile() dir.create(temp_dir) dir_path <- paste0(temp_dir, \"/dummy_data\") dir.create(dir_path) df <- data.frame(Sequence = sequence, Header = \"label_1\", stringsAsFactors = FALSE) file_path <- file.path(dir_path, \"a.fasta\") # sequence as fasta file microseq::writeFasta(fdta = dplyr::as_tibble(df), out.file = file_path) # one-hot encoding example s <-  c(\"a\", \"c\", \"a\", \"f\", \"i\", \"b\") s_as_int_seq <- vector(\"integer\", length(s)) for (i in 1:length(s)) {   s_as_int_seq[i] <- which(s[i] == vocabulary) - 1 } one_hot_sample <- keras::to_categorical(s_as_int_seq) colnames(one_hot_sample) <- vocabulary one_hot_sample"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"maxlen","dir":"Articles","previous_headings":"Data generator options","what":"maxlen","title":"Data Generator","text":"length input sequence.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"vocabulary","dir":"Articles","previous_headings":"Data generator options","what":"vocabulary","title":"Data Generator","text":"set allowed characters sequence. happens characters outside vocabulary can controlled ambiguous_nuc argument.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"train_type","dir":"Articles","previous_headings":"Data generator options","what":"train_type","title":"Data Generator","text":"generator always return list length 2. first element input XX second target YY. train_type argument determines XX YY get extracted. Possible arguments  language models  : “lm” “lm_rds”: Given sequence ss, take subset sequence input rest target. split ss can specified output_format argument. Besides language model approach, can use  label classification . means map label sequence. example, target nucleotide sequence one labels “bacteria” “virus”. specify extract label corresponding sequence. Possible arguments : “label_header”: get label fasta headers. “label_folder”: get label folder, .e. files one folder must belong class. “label_csv”: get label csv file. Csv file one column named “file”. targets correspond entries row (except “file” column). Example: currently working file called “.fasta”, row csv file target information file “label_rds”: rds file contains preprocessed list input target tensors. Another option “dummy_gen”: generator creates random data repeatedly returns . Extract target fasta header (fasta header “label_1” example file): Extract target fasta folder: Extract target csv file: Examples language models follow next section.","code":"# get target from header vocabulary_label <- paste0(\"label_\", 1:5) gen <- get_generator(path = file_path,                      train_type = \"label_header\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary,                      vocabulary_label = vocabulary_label)  z <- gen() x <- z[[1]][1,,]  y <- z[[2]]  colnames(x) <- vocabulary colnames(y) <- vocabulary_label  x # abcdef y # label_1 # create data for second class df <- data.frame(Sequence = \"AABAACAADAAE\", Header = \"header_1\") file_path_2 <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, file_path_2)  # get target from folder vocabulary_label <- paste0(\"label_\", 1:2) gen <- get_generator(path = c(file_path, file_path_2), # one entry for each class                      train_type = \"label_folder\",                      batch_size = 8,                      maxlen = 6,                      vocabulary = vocabulary,                      vocabulary_label = vocabulary_label)  z <- gen() x <- z[[1]] y <- z[[2]]  x_1_1 <- x[1, , ] colnames(x_1_1) <- vocabulary x_1_1 # first sample from first class x_2_1 <- x[5, , ] colnames(x_2_1) <- vocabulary x_2_1 # first sample from second class colnames(y) <- vocabulary_label  y # 4 samples from each class # get target from csv file <- c(basename(file_path), \"xyz.fasta\", \"abc.fasta\", \"x_123.fasta\") vocabulary_label <- paste0(\"label\", 1:4) label_1 <- c(1, 0, 0, 0) label_2 <- c(0, 1, 0, 0) label_3 <- c(0, 0, 1, 0) label_4 <- c(0, 0, 0, 1) df <- data.frame(file, label_1, label_2, label_3, label_4) df csv_file <- tempfile(fileext = \".csv\") write.csv(df, csv_file, row.names = FALSE)  gen <- get_generator(path = file_path,                      train_type = \"label_csv\",                      batch_size = 1,                      maxlen = 6,                      target_from_csv = csv_file,                      vocabulary = vocabulary,                      vocabulary_label = vocabulary_label)  z <- gen() x <- z[[1]][1,,]  y <- z[[2]]  colnames(x) <- vocabulary colnames(y) <- vocabulary_label  x # abcdef y # label_1"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"output_format","dir":"Articles","previous_headings":"Data generator options","what":"output_format","title":"Data Generator","text":"output_format determines shape output language model, .e. part sequence input XX another target YY. Assume sequence abcdefg maxlen = 6. Output correspond follows “target_right”: X=X=abcdef, Y=Y=g “target_middle_lstm”: X=X = (X1=X_1 =abc, X2=X_2 =gfe), Y=Y=d (note reversed order X2X_2) “target_middle_cnn”: X=X =abcefg, Y=Y =d “wavenet”: X=X =abcdef, Y=Y =bcdefg","code":"# target_right gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary,                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1,,]  y <- z[[2]]  colnames(x) <- vocabulary colnames(y) <- vocabulary x # abcdef y # g # target_middle_lstm gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary,                      output_format = \"target_middle_lstm\")  z <- gen() x_1 <- z[[1]][[1]][1,,]  x_2 <- z[[1]][[2]][1,,]  y <- z[[2]]  colnames(x_1) <- vocabulary colnames(x_2) <- vocabulary colnames(y) <- vocabulary x_1 # abc x_2 # gfe y # d # target_middle_cnn gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary,                      output_format = \"target_middle_cnn\")  z <- gen() x <- z[[1]][1,,] y <- z[[2]] colnames(x) <- vocabulary colnames(y) <- vocabulary x # abcefg y # d # wavenet gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary,                      output_format = \"wavenet\")  z <- gen() x <- z[[1]][1,,]  y <- z[[2]][1,,] colnames(x) <- vocabulary colnames(y) <- vocabulary x # abcdef y # bcdefg"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"batch_size","dir":"Articles","previous_headings":"Data generator options","what":"batch_size","title":"Data Generator","text":"Number samples one batch.","code":"# target_right gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 7,                      maxlen = 6,                      vocabulary = vocabulary,                      output_format = \"target_right\")  z <- gen() x <- z[[1]] y <- z[[2]]  dim(x) dim(y)"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"step","dir":"Articles","previous_headings":"Data generator options","what":"step","title":"Data Generator","text":"may determine frequently want take sample. step = 1 take sample every possible step. Let’s assume want predict next character, .e. part sequence input next character target. maxlen = 3, step = 1: sample: abcdefghiiii sample: abcdefghiiii sample: abcdefghiiii step = 3 sample: abcdefghiiii sample: abcdefghiiii sample: abcdefghiiii","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 3,                      vocabulary = vocabulary,                      step = 3,                       output_format = \"target_right\")  z <- gen() x <- z[[1]][1,,] #encodes abc y <- z[[2]] # encodes d colnames(x) <- vocabulary colnames(y) <- vocabulary x y # go 3 steps forward z <- gen() x <- z[[1]][1,,] #encodes def y <- z[[2]] # encodes g colnames(x) <- vocabulary colnames(y) <- vocabulary x y"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"padding","dir":"Articles","previous_headings":"Data generator options","what":"padding","title":"Data Generator","text":"sequence short create single sample, can pad sequence zero-vectors. padding = FALSE generator go next file/ fasta entry finds sequence long enough sample.","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 15, # maxlen is longer than sequence                      vocabulary = vocabulary,                      step = 3,                      padding = TRUE,                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1,,]  y <- z[[2]]  colnames(x) <- vocabulary colnames(y) <- vocabulary x # first 4 entries are zero-vectors y"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"ambiguous_nuc","dir":"Articles","previous_headings":"Data generator options","what":"ambiguous_nuc","title":"Data Generator","text":"sequence might contain character lie inside vocabulary. example, let’s assume discard e vocabulary. 4 options handle situation encode zero vector equal probability use distribution current file discard","code":"vocabulary_2 <- c(\"a\", \"b\", \"c\", \"d\", \"f\", \"g\", \"h\", \"i\") # exclude \"e\" from vocabulary  # zero gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary_2,                      output_format = \"target_right\",                      ambiguous_nuc = \"zeros\") z <- gen() x <- z[[1]][1,,]  colnames(x) <- vocabulary_2 x # fifth row is zero vector # equal gen <- get_generator(path = file_path,                     train_type = \"lm\",                     batch_size = 1,                     maxlen = 6,                     vocabulary = vocabulary_2,                     output_format = \"target_right\",                     ambiguous_nuc = \"equal\")   z <- gen() x <- z[[1]][1,,] colnames(x) <- vocabulary_2 x # fifth row is 1/8 for every entry # empirical gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary_2,                      output_format = \"target_right\",                      ambiguous_nuc = \"empirical\")   z <- gen() x <- z[[1]][1,,]  colnames(x) <- vocabulary_2 x # fifth row is distribuation of file # discard gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 6,                      vocabulary = vocabulary_2,                      output_format = \"target_right\",                      ambiguous_nuc = \"discard\")   z <- gen() x <- z[[1]][1,,] colnames(x) <- vocabulary_2 x # first sample with only characters from vocabulary is fghiii|i"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"proportion_per_seq","dir":"Articles","previous_headings":"Data generator options","what":"proportion_per_seq","title":"Data Generator","text":"proportion_per_seq argument gives option use random subset instead full sequence.","code":"cat(\"sequence is \", nchar(sequence), \"characters long \\n\") gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 5,                      seed = 1,                      vocabulary = vocabulary,                      output_format = \"target_right\",                      # take random subsequence using 50% of sequence                       proportion_per_seq = 0.5)  z <- gen() x <- z[[1]][1, , ] y <- z[[2]] colnames(x) <- vocabulary colnames(y) <- vocabulary x # defgh y # i"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"file_limit","dir":"Articles","previous_headings":"Data generator options","what":"file_limit","title":"Data Generator","text":"Integer NULL. integer, use specified number randomly sampled files training.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"delete_used_files","dir":"Articles","previous_headings":"Data generator options","what":"delete_used_files","title":"Data Generator","text":"true, delete file used. applies rds files.","code":"x <- array(0, dim = c(1,5,4)) y <- matrix(0, ncol = 1) rds_path <- tempfile(fileext = \".rds\") saveRDS(list(x, y), rds_path)  gen <- get_generator(path = rds_path,                      delete_used_files = TRUE,                      train_type = \"label_rds\",                      batch_size = 1,                      maxlen = 5)  z <- gen() file.exists(rds_path) # z <- gen() # When calling the generator again, it will wait until it finds a file again from the files listed in  # the initial `path` argument. Can be used if another process(es) create rds files."},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"max_samples","dir":"Articles","previous_headings":"Data generator options","what":"max_samples","title":"Data Generator","text":"use fixed number samples per file. Randomly choose samples use. (random_sampling = FALSE, samples consecutive.)","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 2,                      maxlen = 5,                      step = 1,                      seed = 3,                      vocabulary = vocabulary,                      output_format = \"target_right\",                      max_samples = 2)  z <- gen() x1 <- z[[1]][1, , ] x2 <- z[[1]][2, , ] colnames(x1) <- vocabulary colnames(x2) <- vocabulary x1 # bcdef x2 # cdefg"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"random_sampling","dir":"Articles","previous_headings":"Data generator options","what":"random_sampling","title":"Data Generator","text":"use max_samples, generator randomly choose subset possible samples, samples consecutive. random_sampling = TRUE, samples completely random.","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 2,                      maxlen = 5,                      seed = 66,                      random_sampling = TRUE,                      vocabulary = vocabulary,                      output_format = \"target_right\",                      max_samples = 2)  z <- gen() x1 <- z[[1]][1, , ] x2 <- z[[1]][2, , ] colnames(x1) <- vocabulary colnames(x2) <- vocabulary x1 # efghi x2 # defgh"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"target_len","dir":"Articles","previous_headings":"Data generator options","what":"target_len","title":"Data Generator","text":"Target length language model.","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      target_len = 3,                       maxlen = 5,                      vocabulary = vocabulary,                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1, , ] y1 <- z[[2]][ , 1, ] y2 <- z[[2]][ , 2, ] y3 <- z[[2]][ , 3, ] colnames(x) <- vocabulary names(y1) <- vocabulary names(y2) <- vocabulary names(y3) <- vocabulary x # abcde y1 # f y2 # g y3 # h"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"n_gram-n_gram_stride","dir":"Articles","previous_headings":"Data generator options","what":"n_gram / n_gram_stride","title":"Data Generator","text":"Encode target language model character wise combine n characters one target. n_gram_stride determines frequency n-gram encoding.","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      target_len = 6,                       n_gram = 3,                      n_gram_stride = 3,                      maxlen = 3,                      vocabulary = vocabulary,                      output_format = \"target_right\")  z <- gen() x <- z[[1]] y1 <- z[[2]][ , 1, ] y2 <- z[[2]][ , 2, ]  dim(x)[3] == length(vocabulary)^3 # x = abc as 3-gram # y1 = def as 3-gram # y2 = ghi as 3-gram"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"add_noise","dir":"Articles","previous_headings":"Data generator options","what":"add_noise","title":"Data Generator","text":"Add noise input. Must list specifies noise distribution NULL (noise). List contains arguments noise_type: either \"normal\" \"uniform\". Optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1).","code":"gen <- get_generator(path = file_path,                      train_type = \"lm\",                      batch_size = 1,                      add_noise = list(noise_type = \"normal\", mean = 0, sd = 0.01),                      maxlen = 5,                      vocabulary = vocabulary,                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1, , ] y <- z[[2]]  colnames(x) <- vocabulary colnames(y) <- vocabulary round(x, 3) # abcde + noise y # f"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"proportion_entries","dir":"Articles","previous_headings":"Data generator options","what":"proportion_entries","title":"Data Generator","text":"fasta file multiple entries, can randomly choose subset. example, file 6 entries proportion_entries = 0.5 generator randomly choose 3 entries.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"shuffle_file_order","dir":"Articles","previous_headings":"Data generator options","what":"shuffle_file_order","title":"Data Generator","text":"Shuffle file order iterating files. Order gets reshuffled every iteration.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"shuffle_input","dir":"Articles","previous_headings":"Data generator options","what":"shuffle_input","title":"Data Generator","text":"Whether shuffle fasta entries fasta file multiple entries.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"reverse_complement","dir":"Articles","previous_headings":"Data generator options","what":"reverse_complement","title":"Data Generator","text":"TRUE, randomly decide every batch use original sequence reverse complement. implemented ACGT vocabulary.","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"sample_by_file_size","dir":"Articles","previous_headings":"Data generator options","what":"sample_by_file_size","title":"Data Generator","text":"Randomly choose new file sampling according file size (bigger files likely).","code":""},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"concat_seq","dir":"Articles","previous_headings":"Data generator options","what":"concat_seq","title":"Data Generator","text":"Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Use concat_seq = \"\" don’t want add new token.","code":"df <- data.frame(Sequence = c(\"AC\", \"AG\", \"AT\"), Header = paste0(\"header\", 1:3)) fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) gen <- get_generator(path = fasta_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 9,                      vocabulary = c(\"A\", \"C\", \"G\", \"T\", \"Z\"),                      concat_seq = \"ZZ\",                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1, , ] y <- z[[2]]  colnames(x) <- c(\"A\", \"C\", \"G\", \"T\", \"Z\") colnames(y) <- c(\"A\", \"C\", \"G\", \"T\", \"Z\") x # ACZZAGZZA y # T"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"set_learning","dir":"Articles","previous_headings":"Data generator options","what":"set_learning","title":"Data Generator","text":"want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target many samples use one target maxlen length one sample reshape_mode = \"multi_input\", generator produce samples_per_target separate inputs, length maxlen. reshape_mode = \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. buffer_len integer, sub-sequences inter spaced buffer_len rows. input length (maxlen * samples_per_target) + buffer_len * (samples_per_target - 1)","code":"# create data for second label df <- data.frame(Sequence = \"AABAACAADAAE\", Header = \"header_1\") file_path_2 <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, file_path_2)  # multi_input  set_learning <- list(reshape_mode = \"multi_input\",                      maxlen = 4,                      samples_per_target = 3)  gen <- get_generator(path = c(file_path, file_path_2), # path has length 2 => 2 classes                      train_type = \"label_folder\",                      batch_size = 2,                      maxlen = 4,                      step = 1,                       vocabulary = vocabulary,                      set_learning = set_learning)  z <- gen() x <- z[[1]] y <- z[[2]] length(x) # 3 samples per target x_1_1 <- x[[1]][1, , ] x_1_1 # abcd x_1_2 <- x[[2]][1, , ] x_1_2 # bcde x_1_3 <- x[[3]][1, , ] x_1_3 # cdef  x_2_1 <- x[[1]][2, , ] x_2_1 # aaba x_2_2 <- x[[2]][2, , ] x_2_2 # abaa x_2_3 <- x[[3]][2, , ] x_2_3 # baac  colnames(y) <- c(\"label_1\", \"label_2\") y # concat  set_learning <- list(reshape_mode = \"concat\",                      maxlen = 4,                      samples_per_target = 3)  gen <- get_generator(path = c(file_path, file_path_2), # path has length 2 => 2 classes                      train_type = \"label_folder\",                      batch_size = 2,                      maxlen = 4,                      step = 2,                       vocabulary = vocabulary,                      set_learning = set_learning)  z <- gen() x <- z[[1]] y <- z[[2]] dim(x)  x_1 <- x[1, , ] colnames(x_1) <- vocabulary x_1 # abcd | cdef | efgh x_2 <- x[2, , ] colnames(x_2) <- vocabulary x_2 # aaba | baac | acaa  colnames(y) <- c(\"label_1\", \"label_2\") y"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"use_quality_score","dir":"Articles","previous_headings":"Data generator options","what":"use_quality_score","title":"Data Generator","text":"TRUE, instead one-hot encoding, use quality score fastq file.","code":"df <- data.frame(Sequence = \"ACAGAT\", Header = \"header_1\", Quality = \"!#*=?I\") fastq_path <- tempfile(fileext = \".fastq\") fastq_file <- microseq::writeFastq(df, fastq_path) gen <- get_generator(path = fastq_path,                      train_type = \"lm\",                      batch_size = 1,                      maxlen = 5,                      format = \"fastq\",                      vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                      use_quality_score = TRUE,                      output_format = \"target_right\")  z <- gen() x <- z[[1]][1, , ] y <- z[[2]]  colnames(x) <- c(\"A\", \"C\", \"G\", \"T\") colnames(y) <- c(\"A\", \"C\", \"G\", \"T\") x # ACAGA y # T"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"use_coverage","dir":"Articles","previous_headings":"Data generator options","what":"use_coverage","title":"Data Generator","text":"Integer NULL. NULL, use coverage encoding rather one-hot encoding. Coverage information must contained fasta header: must string “cov_n” header, n integer.","code":"df <- data.frame(Sequence = \"ACAGAT\", Header = \"header_1_cov_8\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) gen <-  get_generator(path = fasta_path,                       train_type = \"lm\",                       batch_size = 1,                       maxlen = 5,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                       use_coverage = 25,                       output_format = \"target_right\")  z <- gen() x <- z[[1]][1, , ] y <- z[[2]]  colnames(x) <- c(\"A\", \"C\", \"G\", \"T\") colnames(y) <- c(\"A\", \"C\", \"G\", \"T\") x # ACAGA; 0.32 = 8/25 y # T"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"added_label_path","dir":"Articles","previous_headings":"Data generator options","what":"added_label_path","title":"Data Generator","text":"possible feed network additional information associated sequence. information needs csv file. sequences one file share label, csv file one column named “file”. may add additional input dummy data add path csv file, generator map additional input sequences: want train network additional labels, add additional input layer.","code":"file <- c(basename(file_path), \"some_file_name.fasta\") df <- data.frame(file = file,                  label_1 = c(0, 1), label_2 = c(1, 0), label_3 = c(1, 0)) df write.csv(x = df, file = file.path(dir_path, \"add_input.csv\"), row.names = FALSE) gen <-  get_generator(path = dir_path,                       train_type = \"lm\",                        batch_size = 1,                       maxlen = 5,                       output_format = \"target_right\",                       vocabulary = vocabulary,                       added_label_path = file.path(dir_path, \"add_input.csv\"),                       add_input_as_seq = FALSE)  # don't treat added input as sequence                        z <- gen() added_label_input <- z[[1]][[1]] added_label_input x <- z[[1]][[2]] x[1, , ] y <- z[[2]]  y model <- create_model_lstm_cnn(   maxlen = 5,   layer_lstm = c(8, 8),   layer_dense = c(4),   label_input = 3 # additional input vector has length 3 )  # train_model(train_type = \"lm\",  #             model = model, #             path = file.path(dir_path, \"train_files_1\"), #             path_val = file.path(dir_path, \"validation_files_1\"), #             added_label_path = file.path(dir_path, \"add_input.csv\"), #             steps_per_epoch = 5, #             batch_size = 8, #             epochs = 2)"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"return_int","dir":"Articles","previous_headings":"Data generator options","what":"return_int","title":"Data Generator","text":"Whether return integer encoding rather one-hot encoding. Can also combined n-gram encoding:","code":"df <- data.frame(Sequence = \"ATCGC\", Header = \"seq_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) gen <-  get_generator(path = fasta_path,                       train_type = \"lm\",                       batch_size = 1,                       return_int = TRUE,                       padding = TRUE,                       maxlen = 8,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                       output_format = \"target_right\")  z <- gen() x <- z[[1]] y <- z[[2]] colnames(x) <- c(\"pad\", \"pad\", \"pad\", \"pad\", \"A\", \"T\", \"C\", \"G\") x colnames(y) <- \"C\" y df <- data.frame(Sequence = \"AAACCCTTT\", Header = \"seq_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) gen <-  get_generator(path = fasta_path,                       train_type = \"lm\",                       batch_size = 1,                       n_gram = 3,                       n_gram_stride = 3,                       return_int = TRUE,                       maxlen = 6,                       target_len = 3,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                       output_format = \"target_right\")  z <- gen() x <- z[[1]] y <- z[[2]] colnames(x) <- c(\"AAA\", \"CCC\") x colnames(y) <- \"TTT\" y"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"reshape_xy","dir":"Articles","previous_headings":"Data generator options","what":"reshape_xy","title":"Data Generator","text":"Apply function output generator call.","code":"df <- data.frame(Sequence = \"AAAATTTT\", Header = \"header_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) fx <- function(x = NULL, y = NULL) {   return(x - 1) } fy <- function(x = NULL, y = NULL) {   return(exp(y * 5)) }  gen <-  get_generator(path = fasta_path,                       reshape_xy = list(x = fx, y = fy),                       train_type = \"label_folder\",                       batch_size = 1,                       maxlen = 8)  z <- gen() x <- z[[1]] x[1,,] y <- z[[2]] y"},{"path":"https://genomenet.github.io/deepG/articles/data_generator.html","id":"masked_lm","dir":"Articles","previous_headings":"Data generator options","what":"masked_lm","title":"Data Generator","text":"Masks parts input sequence. Can used training BERT-like models. Whenever sw (sample weight) column 0, x y columns identical. Let’s look rows sw 1: 5 mask token, always size vocabulary + 1. Can combined n-gram encoding masking fixed block size: can check sample weights appear blocks. 65 mask token (4^3 + 1 = size vocabulary + 1).","code":"nt_seq <- rep(c(\"A\", \"C\", \"G\", \"T\"), each = 25) %>% paste(collapse = \"\") df <- data.frame(Sequence = nt_seq, Header = \"seq_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) masked_lm <- list(mask_rate = 0.10, # replace 10% of input with special mask token                   random_rate = 0.025, # set 2.5% of input to random value                   identity_rate = 0.05, # leave 5% unchanged                   include_sw = TRUE) # 0,1 matrix showing where masking was applied gen <-  get_generator(path = fasta_path,                       train_type = \"masked_lm\",                       masked_lm = masked_lm,                       batch_size = 1,                       n_gram = 1,                       n_gram_stride = 1,                       return_int = TRUE,                       maxlen = 100,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"))  z <- gen() x <- z[[1]] y <- z[[2]] sw <- z[[3]] df <- data.frame(x = x[1, ], y = y[1, ], sw = sw[1, ]) head(df) df %>% dplyr::filter(sw == 1) df %>% dplyr::filter(sw == 1 & x == 5) # 10% masked part df %>% dplyr::filter(sw == 1 & x != 5) # 5% identity part and 2.5% random part (can randomly be the true value) nt_seq <- rep(c(\"A\", \"C\", \"G\", \"T\"), each = 25) %>% paste(collapse = \"\") df <- data.frame(Sequence = nt_seq, Header = \"seq_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) masked_lm <- list(mask_rate = 0.10, # replace 10% of input with special mask token                   random_rate = 0.05, # set 5% of input to random value                   identity_rate = 0.05, # leave 5% unchanged                   include_sw = TRUE, # 0,1 matrix showing where masking was applied                   block_len = 3) # always mask at least 3 tokens in a row  gen <-  get_generator(path = fasta_path,                       train_type = \"masked_lm\",                       masked_lm = masked_lm,                       batch_size = 1,                       n_gram = 3,                       seed = 12,                       n_gram_stride = 1,                       return_int = TRUE,                       maxlen = 100,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"))  z <- gen() x <- z[[1]] y <- z[[2]] sw <- z[[3]] df <- data.frame(x = x[1, ], y = y[1, ], sw = sw[1, ], position = 1:ncol(x)) head(df) tail(df) which(sw == 1) df %>% dplyr::filter(sw == 1 & x == 65) # 10% masked part df %>% dplyr::filter(sw == 1 & x != 65) # 5% identity part and 5% random part (can randomly be the true value)"},{"path":"https://genomenet.github.io/deepG/articles/getting_started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started","text":"goal deepG package speed development bioinformatical tools sequence classification, homology detection bioinformatical tasks. package offers several functions Data (pre-) processing Deep learning architectures Model training Model evaluation Visualizing training progress","code":""},{"path":"https://genomenet.github.io/deepG/articles/getting_started.html","id":"create-dummy-data","dir":"Articles","previous_headings":"Introduction","what":"Create dummy data","title":"Getting started","text":"create two simple dummy training validation data sets. consist random ACGT sequences first category probability 40% drawing G C second equal probability nucleotide (first category around 80% GC content second one around 50%).","code":"set.seed(123) vocabulary <- c(\"A\", \"C\", \"G\", \"T\")  data_type <- c(\"train_1\", \"train_2\", \"val_1\", \"val_2\")  for (i in 1:length(data_type)) {      temp_file <- tempfile()   assign(paste0(data_type[i], \"_dir\"), temp_file)   dir.create(temp_file)      if (i %% 2 == 1) {     header <- \"high_gc\"     prob <- c(0.1, 0.4, 0.4, 0.1)   } else {     header <- \"equal_dist\"     prob <- rep(0.25, 4)   }      fasta_name_start <- paste0(header, \"_\", data_type[i], \"file\")      create_dummy_data(file_path = temp_file,                     num_files = 1,                     seq_length = 10000,                      num_seq = 1,                     header = header,                     prob = prob,                     fasta_name_start = fasta_name_start,                     vocabulary = vocabulary)    }"},{"path":"https://genomenet.github.io/deepG/articles/getting_started.html","id":"training","dir":"Articles","previous_headings":"Introduction","what":"Training","title":"Getting started","text":"can now train model can differentiate two categories. First, can create network architecture. take input size 50 nucleotides. model one lstm layer 16 cells two dense layers 8 2 neurons. Next can train model using train_model function. Function internally build data generator training.","code":"maxlen <- 50 model <- create_model_lstm_cnn(maxlen = maxlen,                                layer_lstm = 16,                                layer_dense = c(8, 2)) hist <- train_model(model,                     train_type = \"label_folder\",                     run_name = \"gc_model_1\",                     path = c(train_1_dir, train_2_dir),                     path_val = c(val_1_dir, val_2_dir),                     epochs = 4,                     steps_per_epoch = 25, # one epoch = 25 batches                     batch_size = 64,                     step = 50, # take a sample every 50 nt                     vocabulary_label = c(\"high_gc\", \"equal_dist\"))                      plot(hist)"},{"path":"https://genomenet.github.io/deepG/articles/getting_started.html","id":"evaluation","dir":"Articles","previous_headings":"Introduction","what":"Evaluation","title":"Getting started","text":"can now evaluate trained model validation data can check model made mistakes sequence high GC content. can check nucleotide distribution sequences Finally, may want aggregate predictions, made sequence. can using summarize_states function. function returns mean confidence, maximum prediction vote percentages (percentage predictions per class).","code":"eval <- evaluate_model(path_input = c(val_1_dir, val_2_dir),                        model = model,                        batch_size = 100,                        step = 25, # take a sample every 25 nt                         vocabulary_label = list(c(\"high_gc\", \"equal_dist\")),                        mode = \"label_folder\",                        evaluate_all_files = TRUE,                        verbose = FALSE,                        auc = TRUE,                        auprc = TRUE) eval high_gc_file <- microseq::readFasta(list.files(val_1_dir, full.names = TRUE)[1]) high_gc_seq <- high_gc_file$Sequence  pred_high_gc <- predict_model(model = model,                                sequence = high_gc_seq,                               filename = NULL,                                step = 25,                               batch_size = 512,                               verbose = TRUE,                               return_states = TRUE,                               mode = \"label\")  pred_df <- cbind(pred_high_gc$states, pred_high_gc$sample_end_position) %>%    as.data.frame() names(pred_df) <- c(\"high_gc_conf\", \"equal_dist_conf\", \"sample_end_position\") head(pred_df)  wrong_pred <- pred_df %>% dplyr::filter(high_gc_conf < 0.5) wrong_pred  if (nrow(wrong_pred) == 0) {   print(\"All predictions for high GC content class correct\") } else {      # extract samples where model was wrong   wrong_pred_seq <- vector(\"character\", nrow(wrong_pred))   for (i in 1:length(wrong_pred_seq)) {     sample_end <- wrong_pred$sample_end_position[i]     sample_start <- sample_end - maxlen + 1     wrong_pred_seq[i] <- substr(high_gc_seq, sample_start, sample_end)   }      wrong_pred_seq } l <- list() for (i in 1:length(wrong_pred_seq)) {   l[[i]] <- stringr::str_split(wrong_pred_seq[i], \"\") %>% table() %>% prop.table() %>% t() %>% as.matrix() } dist_matrix <- do.call(rbind, l) dist_matrix  df <- data.frame(distribution = as.vector(dist_matrix),                  nt = factor(rep(vocabulary, each = nrow(dist_matrix))),                  sample_id = rep(1:nrow(dist_matrix), 4))  ggplot(df, aes(fill=nt, y=distribution, x=nt)) +      geom_bar(position=\"dodge\", stat=\"identity\")  + facet_wrap(~sample_id) label_names <- c(\"high_gc\", \"equal_dist\") pred_summary <- summarize_states(label_names = label_names, df = pred_df[, 1:2]) print(pred_summary)"},{"path":"https://genomenet.github.io/deepG/articles/integrated_gradient.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Integrated Gradient","text":"Integrated Gradient (IG) method can used determine parts input sequence important models decision. start training model can differentiate sequences based GC content (described Getting started tutorial).","code":""},{"path":"https://genomenet.github.io/deepG/articles/integrated_gradient.html","id":"model-training","dir":"Articles","previous_headings":"","what":"Model Training","title":"Integrated Gradient","text":"create two simple dummy training validation data sets. consist random ACGT sequences first category probability 40% drawing G C second equal probability nucleotide (first category around 80% GC content second one around 50%).","code":"set.seed(123)  # Create data  vocabulary <- c(\"A\", \"C\", \"G\", \"T\") data_type <- c(\"train_1\", \"train_2\", \"val_1\", \"val_2\")  for (i in 1:length(data_type)) {      temp_file <- tempfile()   assign(paste0(data_type[i], \"_dir\"), temp_file)   dir.create(temp_file)      if (i %% 2 == 1) {     header <- \"label_1\"     prob <- c(0.1, 0.4, 0.4, 0.1)   } else {     header <- \"label_2\"     prob <- rep(0.25, 4)   }   fasta_name_start <- paste0(header, \"_\", data_type[i], \"file\")      create_dummy_data(file_path = temp_file,                     num_files = 1,                     seq_length = 20000,                      num_seq = 1,                     header = header,                     prob = prob,                     fasta_name_start = fasta_name_start,                     vocabulary = vocabulary)    }  # Create model maxlen <- 50 model <- create_model_lstm_cnn(maxlen = maxlen,                                filters = c(8, 16),                                kernel_size = c(8, 8),                                pool_size = c(3, 3),                                layer_lstm = 8,                                layer_dense = c(4, 2),                                model_seed = 3)  # Train model hist <- train_model(model,                     train_type = \"label_folder\",                     run_name = \"gc_model_1\",                     path = c(train_1_dir, train_2_dir),                     path_val = c(val_1_dir, val_2_dir),                     epochs = 6,                      batch_size = 64,                     steps_per_epoch = 50,                      step = 50,                      vocabulary_label = c(\"high_gc\", \"equal_dist\"))  plot(hist)"},{"path":"https://genomenet.github.io/deepG/articles/integrated_gradient.html","id":"integrated-gradient","dir":"Articles","previous_headings":"","what":"Integrated Gradient","title":"Integrated Gradient","text":"can try visualize parts input sequence important models decision, using Integrated Gradient. Let’s create sequence high GC content. use number Cs Gs Ts. need one-hot encode sequence applying Integrated Gradient. model confident, sequences belongs first class can visualize parts important prediction. may test models prediction changes exchange certain nucleotides input sequence. First, look positions smallest IG score. may change nucleotide lowest score observe change prediction confidence Let’s repeatedly apply previous step change sequence iteration. can try opposite direction, .e. replace big IG scores.","code":"set.seed(321) g_count <- 17 stopifnot(g_count < 25) a_count <- (50 - (2*g_count))/2   high_gc_seq <- c(rep(\"G\", g_count), rep(\"C\", g_count), rep(\"A\", a_count), rep(\"T\", a_count)) high_gc_seq <- high_gc_seq[sample(maxlen)] %>% paste(collapse = \"\") # shuffle nt order high_gc_seq high_gc_seq_one_hot <- seq_encoding_label(char_sequence = high_gc_seq,                                           maxlen = 50,                                           start_ind = 1,                                           vocabulary = vocabulary) head(high_gc_seq_one_hot[1,,]) pred <- predict(model, high_gc_seq_one_hot, verbose = 0) colnames(pred) <- c(\"high_gc\", \"equal_dist\") pred ig <- integrated_gradients(   input_seq = high_gc_seq_one_hot,   target_class_idx = 1,   model = model)  if (requireNamespace(\"ComplexHeatmap\", quietly = TRUE)) {   heatmaps_integrated_grad(integrated_grads = ig,                            input_seq = high_gc_seq_one_hot) } else {   message(\"Skipping ComplexHeatmap-related code because the package is not installed.\") } ig <- as.array(ig) smallest_index <- which(ig == min(ig), arr.ind = TRUE) smallest_index # copy original sequence high_gc_seq_one_hot_changed <- high_gc_seq_one_hot   # prediction for original sequence predict(model, high_gc_seq_one_hot, verbose = 0)  # change nt smallest_index <- which(ig == min(ig), arr.ind = TRUE) smallest_index row_index <- smallest_index[ , \"row\"] col_index <- smallest_index[ , \"col\"]                new_row <- rep(0, 4) nt_index_old <- col_index nt_index_new <- which.max(ig[row_index, ]) new_row[nt_index_new] <- 1 high_gc_seq_one_hot_changed[1, row_index, ] <- new_row cat(\"At position\", row_index, \"changing\", vocabulary[nt_index_old], \"to\", vocabulary[nt_index_new], \"\\n\")  pred <- predict(model, high_gc_seq_one_hot_changed, verbose = 0) print(pred) # copy original sequence high_gc_seq_one_hot_changed <- high_gc_seq_one_hot   pred_list <- list() pred_list[[1]] <- pred <- predict(model, high_gc_seq_one_hot, verbose = 0)  # change nts for (i in 1:20) {      # update ig scores for changed input   ig <- integrated_gradients(     input_seq = high_gc_seq_one_hot_changed,     target_class_idx = 1,     model = model) %>% as.array()      smallest_index <- which(ig == min(ig), arr.ind = TRUE)   smallest_index   row_index <- smallest_index[ , \"row\"]   col_index <- smallest_index[ , \"col\"]                  new_row <- rep(0, 4)   nt_index_old <- col_index   nt_index_new <- which.max(ig[row_index, ])   new_row[nt_index_new] <- 1   high_gc_seq_one_hot_changed[1, row_index, ] <- new_row   cat(\"At position\", row_index, \"changing\", vocabulary[nt_index_old],       \"to\", vocabulary[nt_index_new], \"\\n\")   pred <- predict(model, high_gc_seq_one_hot_changed, verbose = 0)   pred_list[[i + 1]] <- pred     }  pred_df <- do.call(rbind, pred_list) pred_df <- data.frame(pred_df, iteration = 0:(nrow(pred_df) - 1)) names(pred_df) <- c(\"high_gc\", \"equal_dist\", \"iteration\") ggplot(pred_df, aes(x = iteration, y = high_gc)) + geom_line() + ylab(\"high GC confidence\") # copy original sequence high_gc_seq_one_hot_changed <- high_gc_seq_one_hot   pred_list <- list() pred <- predict(model, high_gc_seq_one_hot, verbose = 0) pred_list[[1]] <- pred  # change nts for (i in 1:20) {      # update ig scores for changed input   ig <- integrated_gradients(     input_seq = high_gc_seq_one_hot_changed,     target_class_idx = 1,     model = model) %>% as.array()      biggest_index <- which(ig == max(ig), arr.ind = TRUE)   biggest_index   row_index <- biggest_index[ , \"row\"]   row_index <- row_index[1]   col_index <- biggest_index[ , \"col\"]                  new_row <- rep(0, 4)   nt_index_old <- col_index   nt_index_new <- which.min(ig[row_index, ])   new_row[nt_index_new] <- 1   high_gc_seq_one_hot_changed[1, row_index, ] <- new_row   cat(\"At position\", row_index, \"changing\", vocabulary[nt_index_old], \"to\", vocabulary[nt_index_new], \"\\n\")      pred <- predict(model, high_gc_seq_one_hot_changed, verbose = 0)   pred_list[[i + 1]] <- pred     }  pred_df <- do.call(rbind, pred_list) pred_df <- data.frame(pred_df, iteration = 0:(nrow(pred_df) - 1)) names(pred_df) <- c(\"high_gc\", \"equal_dist\", \"iteration\") ggplot(pred_df, aes(x = iteration, y = high_gc)) + geom_line() + ylab(\"high GC confidence\")"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"language-model","dir":"Articles","previous_headings":"","what":"Language model","title":"Training Types","text":"language model, mean model predicts character sequence. several options determine output format data generator using output_format argument. output_format determines shape output language model, .e. part sequence input XX another target YY. Assume sequence abcdefg maxlen = 6. Output correspond follows “target_right”: X=X=abcdef, Y=Y=g “target_middle_lstm”: X=X = (X1=X_1 =abc, X2=X_2 =gfe), Y=Y=d (note reversed order X2X_2) “target_middle_cnn”: X=X =abcefg, Y=Y =d “wavenet”: X=X =abcdef, Y=Y =bcdefg","code":""},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"create-dummy-data","dir":"Articles","previous_headings":"Language model","what":"Create dummy data","title":"Training Types","text":"test different language model options, create simple dummy data set consisting repetition sequence AAACCCGGGTTTAAACCC….","code":"vocabulary <- c(\"A\", \"C\", \"G\", \"T\") base_seq <- \"AAACCCGGGTTT\" full_seq <- strrep(base_seq, 50) df <- data.frame(Header = \"header\", Sequence = full_seq)  # create training fasta file train_dir <- tempfile() dir.create(train_dir) microseq::writeFasta(df, file.path(train_dir, \"train_1.fasta\")) # create validation fasta file (use same data as training) val_dir <- tempfile() dir.create(val_dir) microseq::writeFasta(df, file.path(val_dir, \"val_1.fasta\"))"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"predict-next-character","dir":"Articles","previous_headings":"Language model","what":"Predict next character","title":"Training Types","text":"Say want predict next character sequence given last 5 characters text consists letters ,C,G,T . First create model. may use model 1 LSTM 1 dense layer predictions. Next specify location training validation data output format data generator","code":"model <- create_model_lstm_cnn(   maxlen = 5,   layer_lstm = c(8),   layer_dense = c(4),   learning_rate = 0.1,   vocabulary_size = 4 # text consists of A,C,G,T ) hist <- train_model(train_type = \"lm\", # running a language model                     output_format = \"target_right\", # predict target at end of sequence                     model = model,                     path = train_dir,                     path_val = val_dir,                     steps_per_epoch = 5, # use 5 batches per epoch                     train_val_ratio = 0.2, # use 20% of samples for validation compared to train                     batch_size = 16,                     epochs = 4) plot(hist)"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"predict-character-in-middle-of-sequence","dir":"Articles","previous_headings":"Language model","what":"Predict character in middle of sequence","title":"Training Types","text":"want predict character middle sequence use LSTM layers, split input two layers. One layer handles sequence one input target. , example sequence:  ACCGTGGAA first input corresponds ACCG second AAGG. may create model two input layers using create_model_cnn_lstm_target_middle train_model call identical previous model, except change output format generator setting output_format = \"target_middle_lstm\". reverses order sequence target.","code":"model <- create_model_lstm_cnn_target_middle(   maxlen = 5,   layer_lstm = c(8),   layer_dense = c(4),   learning_rate = 0.1,   vocabulary_size = 4  ) hist <- train_model(train_type = \"lm\", # running a language model                     output_format = \"target_middle_lstm\", # predict target in middle of sequence                      model = model,                     path = train_dir,                     path_val = val_dir,                     steps_per_epoch = 5, # use 5 batches per epoch                     train_val_ratio = 0.2, # use 20% of samples for validation compared to train                     batch_size = 16,                     epochs = 4) plot(hist)"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"masked-language-model","dir":"Articles","previous_headings":"Language model","what":"Masked language model","title":"Training Types","text":"mask parts input sequence model tries predict masked regions. Can used training BERT-like models. See also notebook. can first check generator works. Create model architecture. Train model. Evaluate trained model.","code":"# create dummy training data nt_seq <- rep(c(\"A\", \"C\", \"G\", \"T\"), each = 25) %>% paste(collapse = \"\") %>% strrep(10) df <- data.frame(Sequence = nt_seq, Header = \"seq_1\") fasta_path <- tempfile(fileext = \".fasta\") fasta_file <- microseq::writeFasta(df, fasta_path) masked_lm <- list(mask_rate = 0.10, # replace 10% of input with special mask token                   random_rate = 0.03, # set 3% of input to random value                   identity_rate = 0.02, # leave 2% unchanged (and set sample weight to 1)                   include_sw = TRUE) # 0,1 matrix showing where masking was applied  gen <-  get_generator(path = fasta_path,                       train_type = \"masked_lm\",                       masked_lm = masked_lm,                       batch_size = 1,                       n_gram = 1,                       n_gram_stride = 1,                       return_int = TRUE,                       maxlen = 100,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"))  z <- gen() x <- z[[1]] y <- z[[2]] sw <- z[[3]] df <- data.frame(x = x[1, ], y = y[1, ], sw = sw[1, ]) print(head(df), 10) print(df[ df$sw == 1, ]) model <- create_model_transformer(   maxlen = 100,   vocabulary_size = 6,   embed_dim = 16,   ff_dim = 32,   pos_encoding = \"embedding\",   head_size = 20,   num_heads = 4,   layer_dense = 6,   flatten_method = \"none\",   last_layer_activation = \"softmax\",   loss_fn = \"sparse_categorical_crossentropy\",   solver = \"adam\",   learning_rate = 0.005 ) batch_size <- 128 masked_lm <- list(mask_rate = 0.10, random_rate = 0.03, identity_rate = 0.02, include_sw = TRUE)  hist <- train_model(model = model,             # training args             run_name = \"bert_1\",             epochs = 8,             steps_per_epoch = 75,             # generator args             maxlen = 100,             train_type = \"masked_lm\",             path = fasta_path,             path_val = NULL,             batch_size = batch_size,             step = 25,             masked_lm = masked_lm,             proportion_per_seq = 0.97,             return_int = TRUE)  plot(hist) gen <-  get_generator(path = fasta_path,                       train_type = \"masked_lm\",                       masked_lm = masked_lm,                       batch_size = 1,                       n_gram = 1,                       n_gram_stride = 1,                       return_int = TRUE,                       maxlen = 100,                       vocabulary = c(\"A\", \"C\", \"G\", \"T\"))  z <- gen() x <- z[[1]] y <- z[[2]] sw <- z[[3]] pred <- model$predict(x)  pred <- apply(pred[1,,], 1, which.max) - 1  df <- data.frame(x = x[1, ], sw = sw[1, ], y = y[1, ], pred = pred) head(df) df[df$sw == 1, ] df2 <- df[df$sw == 1, ] table(df2$pred, df2$y)"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"label-classification","dir":"Articles","previous_headings":"","what":"Label classification","title":"Training Types","text":"label classification, describe task mapping label sequence. example: given input ACGACCG, sequence belong viral bacterial genome? deepG offers three options map label sequence label gets read fasta header files every class separate folders get label csv file","code":""},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"create-dummy-data-1","dir":"Articles","previous_headings":"Label classification","what":"Create dummy data","title":"Training Types","text":"test label classification, create simple dummy data set. One class consists random sequences using just C second class uses just G T.","code":"# create training fasta files train_dir_1 <- tempfile() train_dir_2 <- tempfile() dir.create(train_dir_1) dir.create(train_dir_2) train_dir <- list(train_dir_1, train_dir_2)  for (i in 1:2) {      if (i == 1) {     vocabulary <- c(\"A\", \"C\")     header <- \"label_1\"     fasta_name_start <- \"label_1_train_file\"   } else {     vocabulary <- c(\"G\", \"T\")     header <- \"label_2\"     fasta_name_start <- \"label_2_train_file\"   }      create_dummy_data(file_path = train_dir[[i]],                     num_files = 3,                     seq_length = 20,                      num_seq = 5,                     header = header,                     fasta_name_start = fasta_name_start,                     vocabulary = vocabulary) }    # create validation fasta files val_dir_1 <- tempfile() val_dir_2 <- tempfile() dir.create(val_dir_1) dir.create(val_dir_2) val_dir <- list(val_dir_1, val_dir_2)  for (i in 1:2) {      if (i == 1) {     vocabulary <- c(\"A\", \"C\")     header <- \"label_1\"     fasta_name_start <- \"label_1_val_file\"   } else {     vocabulary <- c(\"G\", \"T\")     header <- \"label_2\"     fasta_name_start <- \"label_2_val_file\"   }      create_dummy_data(file_path = val_dir[[i]],                     num_files = 3,                     seq_length = 20,                      num_seq = 5,                     header = header,                     fasta_name_start = fasta_name_start,                     vocabulary = vocabulary) }"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"label-by-folder","dir":"Articles","previous_headings":"Label classification","what":"Label by folder","title":"Training Types","text":"approach, put data one class separate folder. Say want classify sequence belongs viral bacterial genome. may put virus bacteria files folder. case path path_val arguments vectors, entry path one class. First create model. may use model 1 LSTM 1 dense layer predictions. input sequence length 5.","code":"model <- create_model_lstm_cnn(   maxlen = 5,   layer_lstm = c(8),   learning_rate = 0.1,   layer_dense = c(2), # binary classification   vocabulary_size = 4 # text consists of A,C,G,T ) train_model(train_type = \"label_folder\", # reading label from folder               model = model,             path = c(train_dir_1, # note that path has two entries                       train_dir_2),             path_val = c(val_dir_1,                          val_dir_2),             steps_per_epoch = 5, # use 5 batches per epoch             train_val_ratio = 0.2,              batch_size = 8,             epochs = 2,             vocabulary_label = c(\"label_1\", \"label_2\") # names of classes )"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"label-by-fasta-header","dir":"Articles","previous_headings":"Label classification","what":"Label by fasta header","title":"Training Types","text":"fasta headers dummy data names “label_1” “label_2”","code":"files <- list.files(train_dir_1, full.names = TRUE) fasta_file <- microseq::readFasta(files[1]) head(fasta_file) train_model(train_type = \"label_header\", # reading label from fasta header               model = model,             path = train_dir,             path_val = val_dir,             steps_per_epoch = 5,              train_val_ratio = 0.2,              batch_size = 8,             epochs = 2,             vocabulary_label = c(\"label_1\", \"label_2\") # names of labels )"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"label-from-csv-file","dir":"Articles","previous_headings":"Label classification","what":"Label from csv file","title":"Training Types","text":"approach extract sequence label mapping current file name csv table.","code":"files_1 <- basename(list.files(c(train_dir_1, val_dir_1))) files_2 <- basename(list.files(c(train_dir_2, val_dir_2))) file <- c(files_1, files_2) label_1 <- stringr::str_detect(file, \"label_1\") %>% as.integer() label_2 <- stringr::str_detect(file, \"label_2\") %>% as.integer() df <- data.frame(file, label_1, label_2) df  csv_path <- tempfile(fileext = \".csv\") write.csv(df, csv_path, row.names = FALSE)  hist <- train_model(train_type = \"label_csv\",                     target_from_csv = csv_path,                     model = model,                     path = train_dir,                     path_val = val_dir,                     steps_per_epoch = 5,                     train_val_ratio = 0.2,                     batch_size = 8,                     epochs = 2)  plot(hist)"},{"path":"https://genomenet.github.io/deepG/articles/training_types.html","id":"training-with-rds-files","dir":"Articles","previous_headings":"Label classification","what":"Training with rds files","title":"Training Types","text":"can also use rds files files input, data must already preprocessed. may use dataset_from_gen function create rds files fasta files. created 25 files training validation preprocessed data. can now use files training.","code":"rds_folder_train <- tempfile() rds_folder_val <- tempfile() dir.create(rds_folder_train) dir.create(rds_folder_val)  for (data_type in c(\"train\", \"val\")) {      if (data_type == \"train\") {     output_path <- rds_folder_train     path_corpus <- train_dir   } else {     output_path <- rds_folder_val     path_corpus <- val_dir   }      dataset_from_gen(output_path = output_path,                    iterations = 25, # create 25 rds files                     train_type = \"label_folder\",                    path_corpus = path_corpus,                     batch_size = 128,                    maxlen = 5,                    step = 5,                    vocabulary = c(\"a\", \"c\", \"g\", \"t\"),                    file_name_start = \"batch_\")    } train_files <- list.files(rds_folder_train, full.names = TRUE) basename(train_files)  example_batch <- readRDS(train_files[1]) x <- example_batch[[1]] y <- example_batch[[2]] dim(x) dim(y) x[1,,] y[1,] model <- create_model_lstm_cnn(   maxlen = 5,   layer_lstm = c(8),   learning_rate = 0.1,   layer_dense = c(2))  hist <- train_model(train_type = \"label_rds\",                     model = model,                     path = rds_folder_train,                     path_val = rds_folder_val,                     steps_per_epoch = 5,                     format = \"rds\",                     batch_size = 8,                     epochs = 2)  plot(hist)"},{"path":"https://genomenet.github.io/deepG/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Philipp Münch. Author. René Mreches. Author, maintainer. Martin Binder. Author. Hüseyin Anil Gündüz. Author. Xiao-Yin . Author. Alice McHardy. Author.","code":""},{"path":"https://genomenet.github.io/deepG/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Münch P, Mreches R, Binder M, Gündüz H, X, McHardy (2024). deepG: Deep Learning Genome Sequence Data. R package version 0.3.1, https://deepg.de/, https://github.com/GenomeNet/deepG.","code":"@Manual{,   title = {deepG: Deep Learning for Genome Sequence Data},   author = {Philipp Münch and René Mreches and Martin Binder and Hüseyin Anil Gündüz and Xiao-Yin To and Alice McHardy},   year = {2024},   note = {R package version 0.3.1,     https://deepg.de/},   url = {https://github.com/GenomeNet/deepG}, }"},{"path":"https://genomenet.github.io/deepG/index.html","id":"deepg-","dir":"","previous_headings":"","what":"Deep Learning for Genome Sequence Data","title":"Deep Learning for Genome Sequence Data","text":"deepG: toolbox deep neural networks optimized genomic datasets goal package speed development bioinformatical tools sequence classification, homology detection bioinformatical tasks. developed biologists advanced AI researchers. DeepG collaborative effort McHardy Lab Helmholtz Centre Infection Research, Chair Statistical Learning Data Science Ludwig Maximilian University Munich Huttenhower lab Harvard T.H. Chan School Public Health.","code":""},{"path":"https://genomenet.github.io/deepG/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Deep Learning for Genome Sequence Data","text":"package offers several functions create, train evaluate neural networks well data processing. Create data generator handle large collections files. Different options encode fasta/fastq file (one-hot encoding, coverage quality score encoding). Different options handle ambiguous nucleotides. Create network architectures single function call. Custom loss metric functions available. Automatically create model/data pipeline. Visualize training progress metrics tensorboard. Evaluate trained models. Use Integrated Gradient visualize relationship model’s predictions regard input.","code":""},{"path":"https://genomenet.github.io/deepG/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Deep Learning for Genome Sequence Data","text":"Install tensorflow python package afterwards install latest version deepG github","code":"install.packages(\"tensorflow\") tensorflow::install_tensorflow() devtools::install_github(\"GenomeNet/deepG\")"},{"path":"https://genomenet.github.io/deepG/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Deep Learning for Genome Sequence Data","text":"See Package website https://deepg.de documentation example code.","code":""},{"path":"https://genomenet.github.io/deepG/reference/auc_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean AUC score — auc_wrapper","title":"Mean AUC score — auc_wrapper","text":"Compute AUC score additional metric. model several output neurons binary crossentropy loss, use average score.","code":""},{"path":"https://genomenet.github.io/deepG/reference/auc_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean AUC score — auc_wrapper","text":"","code":"auc_wrapper(model_output_size, loss = \"binary_crossentropy\")"},{"path":"https://genomenet.github.io/deepG/reference/auc_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean AUC score — auc_wrapper","text":"model_output_size Number neurons model output layer. loss Loss function model, metric applied ; must \"binary_crossentropy\" \"categorical_crossentropy\".","code":""},{"path":"https://genomenet.github.io/deepG/reference/auc_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean AUC score — auc_wrapper","text":"keras metric.","code":""},{"path":"https://genomenet.github.io/deepG/reference/auc_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean AUC score — auc_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\")  y_true <- c(1,0,0,1,1,0,1,0,0) %>% matrix(ncol = 3) y_pred <- c(0.9,0.05,0.05,0.9,0.05,0.05,0.9,0.05,0.05) %>% matrix(ncol = 3)  auc_metric <- auc_wrapper(3L, \"binary_crossentropy\")  auc_metric$update_state(y_true, y_pred) auc_metric$result()    # add metric to a model num_targets <- 4 model <- create_model_lstm_cnn(maxlen = 20,                                layer_lstm = 8,                                bal_acc = FALSE,                                last_layer_activation = \"sigmoid\",                                loss_fn = \"binary_crossentropy\",                                layer_dense = c(8, num_targets))  auc_metric <- auc_wrapper(num_targets, loss = model$loss) model %>% keras::compile(loss = model$loss,                           optimizer = model$optimizer,                          metrics = c(model$metrics, auc_metric)) }"},{"path":"https://genomenet.github.io/deepG/reference/balanced_acc_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Balanced accuracy metric — balanced_acc_wrapper","title":"Balanced accuracy metric — balanced_acc_wrapper","text":"Compute balanced accuracy additional score. Useful imbalanced data. implemented model mutually exclusive targets.","code":""},{"path":"https://genomenet.github.io/deepG/reference/balanced_acc_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Balanced accuracy metric — balanced_acc_wrapper","text":"","code":"balanced_acc_wrapper(num_targets, cm_dir)"},{"path":"https://genomenet.github.io/deepG/reference/balanced_acc_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Balanced accuracy metric — balanced_acc_wrapper","text":"num_targets Number targets. cm_dir Directory confusion matrix used compute balanced accuracy.","code":""},{"path":"https://genomenet.github.io/deepG/reference/balanced_acc_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Balanced accuracy metric — balanced_acc_wrapper","text":"keras metric.","code":""},{"path":"https://genomenet.github.io/deepG/reference/balanced_acc_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Balanced accuracy metric — balanced_acc_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\")  y_true <- c(1,0,0,1,             0,1,0,0,             0,0,1,0) %>% matrix(ncol = 3) y_pred <- c(0.9,0.1,0.2,0.1,             0.05,0.7,0.2,0.0,             0.05,0.2,0.6,0.9) %>% matrix(ncol = 3)  cm_dir <- tempfile()  dir.create(cm_dir) bal_acc_metric <- balanced_acc_wrapper(num_targets = 3L, cm_dir = cm_dir) bal_acc_metric$update_state(y_true, y_pred) bal_acc_metric$result() as.array(bal_acc_metric$cm) }"},{"path":"https://genomenet.github.io/deepG/reference/compile_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Compile model — compile_model","title":"Compile model — compile_model","text":"Compile model","code":""},{"path":"https://genomenet.github.io/deepG/reference/compile_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compile model — compile_model","text":"","code":"compile_model(   model,   solver,   learning_rate,   loss_fn,   label_smoothing = 0,   num_output_layers = 1,   label_noise_matrix = NULL,   bal_acc = FALSE,   f1_metric = FALSE,   auc_metric = FALSE,   layer_dense = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/compile_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compile model — compile_model","text":"model keras model. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. num_output_layers Number output layers. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) bal_acc Whether add balanced accuracy. f1_metric Whether add F1 metric. auc_metric Whether add AUC metric. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used).","code":""},{"path":"https://genomenet.github.io/deepG/reference/compile_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compile model — compile_model","text":"compiled keras model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/compile_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compile model — compile_model","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_lstm_cnn(layer_lstm = 8, compile = FALSE) model <- compile_model(model = model,                        solver = 'adam',                        learning_rate = 0.01,                        loss_fn = 'categorical_crossentropy') }"},{"path":"https://genomenet.github.io/deepG/reference/conf_matrix_cb.html","id":null,"dir":"Reference","previous_headings":"","what":"Confusion matrix callback. — conf_matrix_cb","title":"Confusion matrix callback. — conf_matrix_cb","text":"Create confusion matrix display tensorboard images.","code":""},{"path":"https://genomenet.github.io/deepG/reference/conf_matrix_cb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confusion matrix callback. — conf_matrix_cb","text":"","code":"conf_matrix_cb(path_tensorboard, run_name, confMatLabels, cm_dir)"},{"path":"https://genomenet.github.io/deepG/reference/conf_matrix_cb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confusion matrix callback. — conf_matrix_cb","text":"path_tensorboard Path tensorboard directory NULL. NULL, training tracked tensorboard. run_name Name run. Name used identify output callbacks. NULL, use date run name. name already present, add \"_2\" name \"_{x+1}\" name ends _x, x integer. confMatLabels Names classes. cm_dir Directory contains confusion matrix files.","code":""},{"path":"https://genomenet.github.io/deepG/reference/conf_matrix_cb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confusion matrix callback. — conf_matrix_cb","text":"Keras callback, plot confusion matrix tensorboard.","code":""},{"path":"https://genomenet.github.io/deepG/reference/conf_matrix_cb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confusion matrix callback. — conf_matrix_cb","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") cm <- conf_matrix_cb(path_tensorboard = tempfile(), run_name = 'run_1',                      confMatLabels = c('label_1', 'label_2'), cm_dir = tempfile()) }"},{"path":"https://genomenet.github.io/deepG/reference/create_dummy_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Write random sequences to fasta file — create_dummy_data","title":"Write random sequences to fasta file — create_dummy_data","text":"Create random sequences predefined vocabulary write fasta file.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_dummy_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write random sequences to fasta file — create_dummy_data","text":"","code":"create_dummy_data(   file_path,   num_files,   header = \"header\",   seq_length,   num_seq,   fasta_name_start = \"file\",   write_to_file_path = FALSE,   prob = NULL,   vocabulary = c(\"a\", \"c\", \"g\", \"t\") )"},{"path":"https://genomenet.github.io/deepG/reference/create_dummy_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write random sequences to fasta file — create_dummy_data","text":"file_path Output directory; can also file name possible write_to_file_path = TRUE num_files = 1). num_files Number files create. header Fasta header name. seq_length Length one sequence. vector longer 1, randomly sample vector. num_seq Number sequences per file. fasta_name_start Beginning string file name. Output files named fasta_name_start + _i.fasta integer index. write_to_file_path Whether write output directly file_path, .e. file_path directory. prob Probability character vocabulary sampled. NULL character probability. vocabulary Set characters sample sequences .","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_dummy_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write random sequences to fasta file — create_dummy_data","text":"None. Writes data files.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_dummy_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write random sequences to fasta file — create_dummy_data","text":"","code":"path_output <- tempfile() dir.create(path_output) create_dummy_data(file_path = path_output,                   num_files = 3,                   seq_length = 11,                    num_seq = 5,                                      vocabulary = c(\"a\", \"c\", \"g\", \"t\")) list.files(path_output)                 #> [1] \"file_1.fasta\" \"file_2.fasta\" \"file_3.fasta\""},{"path":"https://genomenet.github.io/deepG/reference/create_model_genomenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","title":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","text":"Create GenomeNet Model Given Architecture Parameters","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_genomenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","text":"","code":"create_model_genomenet(   maxlen = 300,   learning_rate = 0.001,   number_of_cnn_layers = 1,   conv_block_count = 1,   kernel_size_0 = 16,   kernel_size_end = 16,   filters_0 = 256,   filters_end = 512,   dilation_end = 1,   max_pool_end = 1,   dense_layer_num = 1,   dense_layer_units = 100,   dropout_lstm = 0,   dropout = 0,   batch_norm_momentum = 0.8,   leaky_relu_alpha = 0,   dense_activation = \"relu\",   skip_block_fraction = 0,   residual_block = FALSE,   reverse_encoding = FALSE,   optimizer = \"adam\",   model_type = \"gap\",   recurrent_type = \"lstm\",   recurrent_layers = 1,   recurrent_bidirectional = FALSE,   recurrent_units = 100,   vocabulary_size = 4,   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   auc_metric = FALSE,   num_targets = 2,   model_seed = NULL,   bal_acc = FALSE,   f1_metric = FALSE,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_genomenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","text":"maxlen (integer numeric(1)) Input sequence length. learning_rate (numeric(1)) Used keras optimizer specified optimizer. number_of_cnn_layers (integer numeric(1)) Target number CNN-layers use total. number_of_cnn_layers greater conv_block_count, effective number CNN layers set closest integer divisible conv_block_count. conv_block_count (integer numeric(1)) Number convolutional blocks, CNN layers divided. greater number_of_cnn_layers, set number_of_cnn_layers (convolutional block size 1). Convolutional blocks used model_type \"gap\" (output last conv_block_count * (1 - skip_block_fraction) blocks fed global average pooling concatenated), also residual_block TRUE (number filters held constant within blocks). neither case, conv_block_count little effect besides fact number_of_cnn_layers set closest integer divisible conv_block_count. kernel_size_0 (numeric(1)) Target CNN kernel size first CNN-layer. Although CNN kernel size always integer, value can non-integer, potentially affecting kernel-sizes intermediate layers (geometrically interpolated kernel_size_0 kernel_size_end). kernel_size_end (numeric(1)) Target CNN kernel size last CNN-layer; ignored one CNN-layer used (.e. number_of_cnn_layers 1). Although CNN kernel size always integer, value can non-integer, potentially affecting kernel-sizes intermediate layers (geometrically interpolated kernel_size_0 kernel_size_end). filters_0 (numeric(1)) Target filter number first CNN-layer. Although CNN filter number always integer, value can non-integer, potentially affecting filter-numbers intermediate layers (geometrically interpolated filters_0 filters_end). Note filters constant within convolutional blocks residual_block TRUE. filters_end (numeric(1)) Target filter number last CNN-layer; ignored one CNN-layer used (.e. number_of_cnn_layers 1). Although CNN filter number always integer, value can non-integer, potentially affecting filter-numbers intermediate dilation_rates layers (geometrically interpolated kernel_size_0 kernel_size_end). Note filters constant within convolutional blocks residual_block TRUE. dilation_end (numeric(1)) Dilation last CNN-layer within block. Dilation rates within convolutional block grows exponentially 1 (dilation) first CNN-layer block, value. Set 1 (default) disable dilation. max_pool_end (numeric(1)) Target total effective pooling CNN part network. \"Effective pooling\" product pooling rates previous CNN-layers. network three CNN-layers, followed pooling layers size 2, therefore effective pooling 8, effective pooling intermediate positions 1 (beginning), 2, 4. Effective pooling layer set power 2 , logarithmic scale, closest max_pool_end ^ (<CNN layer number> / <total number CNN layers>). Therefore, even though total effective pooling size whole CNN part network always power 2, different, possibly non-integer values max_pool_end, still lead different networks. dense_layer_num (integer numeric(1)) number dense layers end network, counting output layer. dense_layer_units (integer numeric(1)) Number units dense layer, except output layer. dropout_lstm Fraction units drop inputs. dropout (numeric(1)) Dropout rate dense layers, except output layer. batch_norm_momentum (numeric(1))momentum-parameter layer_batch_normalization layers used convolutional part network. leaky_relu_alpha (numeric(1))alpha-parameter layer_activation_leaky_relu activation layers used convolutional part network. dense_activation (character(1)) activation function use dense layers. one \"relu\", \"sigmoid\", \"tanh\". skip_block_fraction (numeric(1)) fraction first convolutional blocks skip. used model_type \"gap\". residual_block (logical(1)) Whether use residual layers convolutional part network. reverse_encoding (logical(1)) Whether network second input reverse-complement sequences. optimizer (character(1)) optimizer use. One \"adam\", \"adagrad\", \"rmsprop\", \"sgd\". model_type (character(1)) Whether use global average pooling (\"gap\") recurrent (\"recurrent\") model type. recurrent_type (character(1)) recurrent network type use. One \"lstm\" \"gru\". used model_type \"recurrent\". recurrent_layers (integer numeric(1)) Number recurrent layers. used model_type \"recurrent\". recurrent_bidirectional (logical(1)) Whether use bidirectional recurrent layers. used model_type \"recurrent\". recurrent_units (integer numeric(1)) Number units recurrent layer. used model_type \"recurrent\". vocabulary_size (integer numeric(1)) Vocabulary size (one-hot encoded) input strings. determines input tensor shape, together maxlen. last_layer_activation Either \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". auc_metric Whether add AUC metric. num_targets (integer numeric(1)) Number output units create. model_seed Set seed model parameters tensorflow NULL. bal_acc Whether add balanced accuracy. f1_metric Whether add F1 metric. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_genomenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","text":"keras model. keras model implementing genomenet architecture.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_genomenet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create GenomeNet Model with Given Architecture Parameters — create_model_genomenet","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_genomenet() model }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LSTM/CNN network — create_model_lstm_cnn","title":"Create LSTM/CNN network — create_model_lstm_cnn","text":"Creates network consisting arbitrary number CNN, LSTM dense layers. Last layer dense layer.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LSTM/CNN network — create_model_lstm_cnn","text":"","code":"create_model_lstm_cnn(   maxlen = 50,   dropout_lstm = 0,   recurrent_dropout_lstm = 0,   layer_lstm = NULL,   layer_dense = c(4),   dropout_dense = NULL,   kernel_size = NULL,   filters = NULL,   strides = NULL,   pool_size = NULL,   solver = \"adam\",   learning_rate = 0.001,   vocabulary_size = 4,   bidirectional = FALSE,   stateful = FALSE,   batch_size = NULL,   compile = TRUE,   padding = \"same\",   dilation_rate = NULL,   gap = FALSE,   use_bias = TRUE,   residual_block = FALSE,   residual_block_length = 1,   size_reduction_1Dconv = FALSE,   label_input = NULL,   zero_mask = FALSE,   label_smoothing = 0,   label_noise_matrix = NULL,   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   num_output_layers = 1,   auc_metric = FALSE,   f1_metric = FALSE,   bal_acc = FALSE,   verbose = TRUE,   batch_norm_momentum = 0.99,   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LSTM/CNN network — create_model_lstm_cnn","text":"maxlen Length predictor sequence. dropout_lstm Fraction units drop inputs. recurrent_dropout_lstm Fraction units drop recurrent state. layer_lstm Number cells per network layer. Can scalar vector. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). dropout_dense Dropout rates dense layers. dropout NULL. kernel_size Size 1d convolutional layers. multiple layers, assign vector. (e.g, rep(3,2) two layers kernel size 3) filters Number filters. multiple layers, assign vector. strides Stride values. multiple layers, assign vector. pool_size Integer, size max pooling windows. multiple layers, assign vector. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. vocabulary_size Number unique character vocabulary. bidirectional Use bidirectional wrapper lstm layers. stateful Boolean. Whether use stateful LSTM layer. batch_size Number samples used one network update. used stateful = TRUE. compile Whether compile model. padding Padding CNN layers, e.g. \"\", \"valid\" \"causal\". dilation_rate Integer, dilation rate use dilated convolution. gap Whether apply global average pooling last CNN layer. use_bias Boolean. Usage bias CNN layers. residual_block Boolean. true, residual connections used CNN. used first convolutional layer. residual_block_length Integer. Determines many convolutional layers (triplets size_reduction_1D_conv TRUE) exist size_reduction_1Dconv Boolean. TRUE, number filters convolutional layers reduced 1/4 number filters label_input Integer NULL. NULL, adds additional input layer label_input size. zero_mask Boolean, whether apply zero masking LSTM layer. used model use CNN layers. label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". num_output_layers Number output layers. auc_metric Whether add AUC metric. f1_metric Whether add F1 metric. bal_acc Whether add balanced accuracy. verbose Boolean. batch_norm_momentum Momentum moving mean moving variance. model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create LSTM/CNN network — create_model_lstm_cnn","text":"keras model, stacks CNN, LSTM dense layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LSTM/CNN network — create_model_lstm_cnn","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") create_model_lstm_cnn(   maxlen = 500,   vocabulary_size = 4,   kernel_size = c(8, 8, 8),   filters = c(16, 32, 64),   pool_size = c(3, 3, 3),   layer_lstm = c(32, 64),   layer_dense = c(128, 4),   learning_rate = 0.001) }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_multi_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","title":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","text":"Creates network consisting arbitrary number CNN, LSTM dense layers multiple input layers. LSTM/CNN part representations get aggregated summation. Can used make single prediction combination multiple input sequences. Implements approach described ","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_multi_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","text":"","code":"create_model_lstm_cnn_multi_input(   maxlen = 50,   dropout_lstm = 0,   recurrent_dropout_lstm = 0,   layer_lstm = NULL,   layer_dense = c(4),   dropout_dense = NULL,   solver = \"adam\",   learning_rate = 0.001,   vocabulary_size = 4,   bidirectional = FALSE,   batch_size = NULL,   compile = TRUE,   kernel_size = NULL,   filters = NULL,   strides = NULL,   pool_size = NULL,   padding = \"same\",   dilation_rate = NULL,   gap_inputs = NULL,   use_bias = TRUE,   zero_mask = FALSE,   label_smoothing = 0,   label_noise_matrix = NULL,   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   auc_metric = FALSE,   f1_metric = FALSE,   bal_acc = FALSE,   samples_per_target,   batch_norm_momentum = 0.99,   aggregation_method = c(\"sum\"),   verbose = TRUE,   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_multi_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","text":"maxlen Length predictor sequence. dropout_lstm Fraction units drop inputs. recurrent_dropout_lstm Fraction units drop recurrent state. layer_lstm Number cells per network layer. Can scalar vector. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). dropout_dense Vector dropout rates dense layers. dropout NULL. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. vocabulary_size Number unique character vocabulary. bidirectional Use bidirectional wrapper lstm layers. batch_size Number samples used one network update. used stateful = TRUE. compile Whether compile model. kernel_size Size 1d convolutional layers. multiple layers, assign vector. (e.g, rep(3,2) two layers kernel size 3) filters Number filters. multiple layers, assign vector. strides Stride values. multiple layers, assign vector. pool_size Integer, size max pooling windows. multiple layers, assign vector. padding Padding CNN layers, e.g. \"\", \"valid\" \"causal\". dilation_rate Integer, dilation rate use dilated convolution. gap_inputs Global pooling method apply. options flatten_method argument create_model_transformer function. use_bias Boolean. Usage bias CNN layers. zero_mask Boolean, whether apply zero masking LSTM layer. used model use CNN layers. label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". auc_metric Whether add AUC metric. f1_metric Whether add F1 metric. bal_acc Whether add balanced accuracy. samples_per_target Number samples combine one target. batch_norm_momentum Momentum moving mean moving variance. aggregation_method least one options \"sum\", \"mean\", \"max\". verbose Boolean. model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_multi_input.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","text":"keras model multiple input layers. Input goes shared LSTM/CNN layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_multi_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LSTM/CNN network that can process multiple samples for one target — create_model_lstm_cnn_multi_input","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") create_model_lstm_cnn_multi_input(   maxlen = 50,   vocabulary_size = 4,   samples_per_target = 7,   kernel_size = c(10, 10),   filters = c(64, 128),   pool_size = c(2, 2),   layer_lstm = c(32),   layer_dense = c(64, 2),   aggregation_method = c(\"max\"),   learning_rate = 0.001)    }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_target_middle.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","title":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","text":"Creates network consisting arbitrary number CNN, LSTM dense layers. Function creates two sub networks consisting (optional) CNN layers followed arbitrary number LSTM layers. Afterwards last LSTM layers get concatenated followed one dense layers. Last layer dense layer. Network tries predict target middle sequence. input AACCTAAGG, input tensors correspond x1 = AACC, x2 = GGAA y = T.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_target_middle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","text":"","code":"create_model_lstm_cnn_target_middle(   maxlen = 50,   dropout_lstm = 0,   recurrent_dropout_lstm = 0,   layer_lstm = 128,   solver = \"adam\",   learning_rate = 0.001,   vocabulary_size = 4,   bidirectional = FALSE,   stateful = FALSE,   batch_size = NULL,   padding = \"same\",   compile = TRUE,   layer_dense = NULL,   kernel_size = NULL,   filters = NULL,   pool_size = NULL,   strides = NULL,   label_input = NULL,   zero_mask = FALSE,   label_smoothing = 0,   label_noise_matrix = NULL,   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   num_output_layers = 1,   f1_metric = FALSE,   auc_metric = FALSE,   bal_acc = FALSE,   verbose = TRUE,   batch_norm_momentum = 0.99,   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_target_middle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","text":"maxlen Length predictor sequence. dropout_lstm Fraction units drop inputs. recurrent_dropout_lstm Fraction units drop recurrent state. layer_lstm Number cells per network layer. Can scalar vector. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. vocabulary_size Number unique character vocabulary. bidirectional Use bidirectional wrapper lstm layers. stateful Boolean. Whether use stateful LSTM layer. batch_size Number samples used one network update. used stateful = TRUE. padding Padding CNN layers, e.g. \"\", \"valid\" \"causal\". compile Whether compile model. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). kernel_size Size 1d convolutional layers. multiple layers, assign vector. (e.g, rep(3,2) two layers kernel size 3) filters Number filters. multiple layers, assign vector. pool_size Integer, size max pooling windows. multiple layers, assign vector. strides Stride values. multiple layers, assign vector. label_input Integer NULL. NULL, adds additional input layer label_input size. zero_mask Boolean, whether apply zero masking LSTM layer. used model use CNN layers. label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". num_output_layers Number output layers. f1_metric Whether add F1 metric. auc_metric Whether add AUC metric. bal_acc Whether add balanced accuracy. verbose Boolean. batch_norm_momentum Momentum moving mean moving variance. model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_target_middle.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","text":"keras model two input layers. Consists LSTN, CNN dense layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_target_middle.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LSTM/CNN network to predict middle part of a sequence — create_model_lstm_cnn_target_middle","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") create_model_lstm_cnn_target_middle(   maxlen = 500,   vocabulary_size = 4,   kernel_size = c(8, 8, 8),   filters = c(16, 32, 64),   pool_size = c(3, 3, 3),   layer_lstm = c(32, 64),   layer_dense = c(128, 4),   learning_rate = 0.001)   }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_time_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","title":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","text":"Creates network consisting arbitrary number CNN, LSTM dense layers. Input 4D tensor, axis correspond : batch size number samples one batch length one sample size vocabulary LSTM/CNN part representations get aggregated summation. Can used make single prediction combination multiple input sequences. Architecture equivalent create_model_lstm_cnn_multi_input instead multiple input layers 3D input, input one 4D tensor.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_time_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","text":"","code":"create_model_lstm_cnn_time_dist(   maxlen = 50,   dropout_lstm = 0,   recurrent_dropout_lstm = 0,   layer_lstm = NULL,   layer_dense = c(4),   solver = \"adam\",   learning_rate = 0.001,   vocabulary_size = 4,   bidirectional = FALSE,   stateful = FALSE,   batch_size = NULL,   compile = TRUE,   kernel_size = NULL,   filters = NULL,   strides = NULL,   pool_size = NULL,   padding = \"same\",   dilation_rate = NULL,   gap_time_dist = NULL,   use_bias = TRUE,   zero_mask = FALSE,   label_smoothing = 0,   label_noise_matrix = NULL,   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   auc_metric = FALSE,   f1_metric = FALSE,   samples_per_target,   batch_norm_momentum = 0.99,   verbose = TRUE,   model_seed = NULL,   aggregation_method = NULL,   transformer_args = NULL,   lstm_time_dist = NULL,   mixed_precision = FALSE,   bal_acc = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_time_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","text":"maxlen Length predictor sequence. dropout_lstm Fraction units drop inputs. recurrent_dropout_lstm Fraction units drop recurrent state. layer_lstm Number cells per network layer. Can scalar vector. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. vocabulary_size Number unique character vocabulary. bidirectional Use bidirectional wrapper lstm layers. stateful Boolean. Whether use stateful LSTM layer. batch_size Number samples used one network update. used stateful = TRUE. compile Whether compile model. kernel_size Size 1d convolutional layers. multiple layers, assign vector. (e.g, rep(3,2) two layers kernel size 3) filters Number filters. multiple layers, assign vector. strides Stride values. multiple layers, assign vector. pool_size Integer, size max pooling windows. multiple layers, assign vector. padding Padding CNN layers, e.g. \"\", \"valid\" \"causal\". dilation_rate Integer, dilation rate use dilated convolution. gap_time_dist Pooling flatten method last time distribution wrapper. options flatten_method argument create_model_transformer function. use_bias Boolean. Usage bias CNN layers. zero_mask Boolean, whether apply zero masking LSTM layer. used model use CNN layers. label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". auc_metric Whether add AUC metric. f1_metric Whether add F1 metric. samples_per_target Number samples combine one target. batch_norm_momentum Momentum moving mean moving variance. verbose Boolean. model_seed Set seed model parameters tensorflow NULL. aggregation_method least one options \"sum\", \"mean\", \"max\". transformer_args List arguments transformer blocks; see layer_transformer_block_wrapper. Additionally, list can contain pool_flatten argument apply global pooling flattening last transformer block (options flatten_method argument create_model_transformer function). lstm_time_dist Vector containing number units per LSTM cell. Applied time distribution part. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). bal_acc Whether add balanced accuracy. mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_time_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","text":"keras model time distribution wrapper applied LSTM CNN layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_lstm_cnn_time_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LSTM/CNN network for combining multiple sequences — create_model_lstm_cnn_time_dist","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") create_model_lstm_cnn_time_dist(   maxlen = 50,   vocabulary_size = 4,   samples_per_target = 7,   kernel_size = c(10, 10),   filters = c(64, 128),   pool_size = c(2, 2),   layer_lstm = c(32),   aggregation_method = c(\"max\"),   layer_dense = c(64, 2),   learning_rate = 0.001) }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_transformer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create transformer model — create_model_transformer","title":"Create transformer model — create_model_transformer","text":"Creates transformer network classification. Model can consist several stacked attention blocks.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_transformer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create transformer model — create_model_transformer","text":"","code":"create_model_transformer(   maxlen,   vocabulary_size = 4,   embed_dim = 64,   pos_encoding = \"embedding\",   head_size = 4L,   num_heads = 5L,   ff_dim = 8,   dropout = 0,   n = 10000,   layer_dense = 2,   dropout_dense = NULL,   flatten_method = \"flatten\",   last_layer_activation = \"softmax\",   loss_fn = \"categorical_crossentropy\",   solver = \"adam\",   learning_rate = 0.01,   label_noise_matrix = NULL,   bal_acc = FALSE,   f1_metric = FALSE,   auc_metric = FALSE,   label_smoothing = 0,   verbose = TRUE,   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_transformer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create transformer model — create_model_transformer","text":"maxlen Length predictor sequence. vocabulary_size Number unique character vocabulary. embed_dim Dimension token embedding. embedding set 0. used input one-hot encoded (integer sequence). pos_encoding Either \"sinusoid\" \"embedding\". add positional information. \"sinusoid\", add sine waves different frequencies input. \"embedding\", model learns positional embedding. head_size Dimensions attention key. num_heads Number attention heads. ff_dim Units first dense layer attention blocks. dropout Vector dropout rates attention block(s). n Frequency sine waves positional encoding. applied pos_encoding = \"sinusoid\". layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). dropout_dense Dropout dense layers. flatten_method process output last attention block. Can \"max_ch_first\", \"max_ch_last\", \"average_ch_first\", \"average_ch_last\", \"both_ch_first\", \"both_ch_last\", \"\", \"none\" \"flatten\". \"average_ch_last\" /  \"max_ch_last\"  \"average_ch_first\" / \"max_ch_first\", apply global average/max pooling. _ch_first / _ch_last decide along axis. \"both_ch_first\" / \"both_ch_last\" use max average together. \"\" use 4 global pooling options together. \"flatten\", flatten output last attention block. \"none\" flattening applied. last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. label_noise_matrix Matrix label noises. Every row stands one class columns percentage labels class. first label contains 5 percent wrong labels second label noise, label_noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE ) bal_acc Whether add balanced accuracy. f1_metric Whether add F1 metric. auc_metric Whether add AUC metric. label_smoothing Float [0, 1]. 0, smoothing applied. > 0, loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. closer argument 1 labels get smoothed. verbose Boolean. model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_transformer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create transformer model — create_model_transformer","text":"keras model implementing transformer architecture.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_transformer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create transformer model — create_model_transformer","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_transformer(maxlen = 50,                                   head_size=c(10,12),                                   num_heads=c(7,8),                                   ff_dim=c(5,9),                                    dropout=c(0.3, 0.5)) }"},{"path":"https://genomenet.github.io/deepG/reference/create_model_twin_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Create twin network — create_model_twin_network","title":"Create twin network — create_model_twin_network","text":"Twin network can trained maximize distance embeddings inputs. Implements approach described .","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_twin_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create twin network — create_model_twin_network","text":"","code":"create_model_twin_network(   maxlen = 50,   dropout_lstm = 0,   recurrent_dropout_lstm = 0,   layer_lstm = NULL,   layer_dense = c(4),   dropout_dense = NULL,   kernel_size = NULL,   filters = NULL,   strides = NULL,   pool_size = NULL,   solver = \"adam\",   learning_rate = 0.001,   vocabulary_size = 4,   bidirectional = FALSE,   compile = TRUE,   padding = \"same\",   dilation_rate = NULL,   gap_inputs = NULL,   use_bias = TRUE,   residual_block = FALSE,   residual_block_length = 1,   size_reduction_1Dconv = FALSE,   zero_mask = FALSE,   verbose = TRUE,   batch_norm_momentum = 0.99,   distance_method = \"euclidean\",   last_layer_activation = \"sigmoid\",   loss_fn = loss_cl(margin = 1),   metrics = \"acc\",   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/create_model_twin_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create twin network — create_model_twin_network","text":"maxlen Length predictor sequence. dropout_lstm Fraction units drop inputs. recurrent_dropout_lstm Fraction units drop recurrent state. layer_lstm Number cells per network layer. Can scalar vector. layer_dense Vector containing number neurons per dense layer, euclidean distance layer. dropout_dense Dropout rates dense layers. dropout NULL. kernel_size Size 1d convolutional layers. multiple layers, assign vector. (e.g, rep(3,2) two layers kernel size 3) filters Number filters. multiple layers, assign vector. strides Stride values. multiple layers, assign vector. pool_size Integer, size max pooling windows. multiple layers, assign vector. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. vocabulary_size Number unique character vocabulary. bidirectional Use bidirectional wrapper lstm layers. compile Whether compile model. padding Padding CNN layers, e.g. \"\", \"valid\" \"causal\". dilation_rate Integer, dilation rate use dilated convolution. gap_inputs Global pooling method apply. options flatten_method argument create_model_transformer function. use_bias Boolean. Usage bias CNN layers. residual_block Boolean. true, residual connections used CNN. used first convolutional layer. residual_block_length Integer. Determines many convolutional layers (triplets size_reduction_1D_conv TRUE) exist size_reduction_1Dconv Boolean. TRUE, number filters convolutional layers reduced 1/4 number filters zero_mask Boolean, whether apply zero masking LSTM layer. used model use CNN layers. verbose Boolean. batch_norm_momentum Momentum moving mean moving variance. distance_method Either \"euclidean\" \"cosine\". last_layer_activation Activation function output layer(s). example \"sigmoid\" \"softmax\". loss_fn Either \"categorical_crossentropy\" \"binary_crossentropy\". label_noise_matrix given, use custom \"noisy_loss\". metrics Vector list metrics. model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_twin_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create twin network — create_model_twin_network","text":"keras model implementing twin network architecture.","code":""},{"path":"https://genomenet.github.io/deepG/reference/create_model_twin_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create twin network — create_model_twin_network","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_twin_network(   maxlen = 50,   layer_dense = 16,   kernel_size = 12,   filters = 4,   pool_size = 3,   learning_rate = 0.001)    }"},{"path":"https://genomenet.github.io/deepG/reference/crispr_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"CRISPR data — crispr_sample","title":"CRISPR data — crispr_sample","text":"Example training dataset consisting sequence nucleotides CRISPR loci Filtered unambiguous characters contains characters vocabulary {,G,G,T }. Can loaded workspace via data(crispr_sample).","code":""},{"path":"https://genomenet.github.io/deepG/reference/crispr_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CRISPR data — crispr_sample","text":"","code":"data(crispr_sample)"},{"path":"https://genomenet.github.io/deepG/reference/crispr_sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"CRISPR data — crispr_sample","text":"Large character 442.41 kB","code":""},{"path":"https://genomenet.github.io/deepG/reference/crispr_sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"CRISPR data — crispr_sample","text":"https://github.com/philippmuench","code":""},{"path":"https://genomenet.github.io/deepG/reference/dataset_from_gen.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","title":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","text":"Repeatedly generate samples data generator store output. Creates separate rds pickle file output_path batch.","code":""},{"path":"https://genomenet.github.io/deepG/reference/dataset_from_gen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","text":"","code":"dataset_from_gen(   output_path,   iterations = 10,   train_type = \"lm\",   output_format = \"target_right\",   path_corpus,   batch_size = 32,   maxlen = 250,   step = NULL,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   shuffle = FALSE,   set_learning = NULL,   seed = NULL,   random_sampling = FALSE,   store_format = \"rds\",   file_name_start = \"batch_\",   masked_lm = NULL,   ... )"},{"path":"https://genomenet.github.io/deepG/reference/dataset_from_gen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","text":"output_path Output directory. Output files named output_path + file_name_start + x + \".rds\" \".pickle\", x index (1 iterations) file ending depends store_format argument. iterations Number batches (output files) create. train_type Either \"lm\", \"lm_rds\", \"masked_lm\" language model; \"label_header\", \"label_folder\", \"label_csv\", \"label_rds\" classification \"dummy_gen\". Language model trained predict character(s) sequence. \"label_header\"/\"label_folder\"/\"label_csv\" trained predict corresponding class given sequence input. \"label_header\", class read fasta headers. \"label_folder\", class read folder, .e. files one folder must belong class. \"label_csv\", targets read csv file. file one column named \"file\". targets correspond entries row (except \"file\" column). Example: currently working file called \".fasta\" corresponding label \"label_1\", row csv file \"label_rds\", generator iterate set .rds files containing list input target tensors. implemented model multiple inputs. \"lm_rds\", generator iterate set .rds files split tensor according target_len argument (targets last target_len nucleotides sequence). \"dummy_gen\", generator creates random data repeatedly feeds model. \"masked_lm\", generator maskes parts input. See masked_lm argument details. output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" path_corpus Input directory fasta files located path single file ending fasta fastq (specified format argument). Can also list directories /files. batch_size Number samples one batch. maxlen Length predictor sequence. step often take sample. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. shuffle Whether shuffle samples within batch. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). seed Sets seed set.seed function reproducible results. random_sampling Whether samples taken random positions using max_samples argument. FALSE random samples taken consecutive subsequence. store_format Either \"rds\" \"pickle\". file_name_start Start output file names. masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. ... generator options. See get_generator.","code":""},{"path":"https://genomenet.github.io/deepG/reference/dataset_from_gen.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","text":"None. Function writes data files return value.","code":""},{"path":"https://genomenet.github.io/deepG/reference/dataset_from_gen.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect samples from generator and store in rds or pickle file. — dataset_from_gen","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy fasta files temp_dir <- tempfile() dir.create(temp_dir) create_dummy_data(file_path = temp_dir,                   num_files = 3,                   seq_length = 8,                    num_seq = 2)  # extract samples out_dir <- tempfile() dir.create(out_dir) dataset_from_gen(output_path = out_dir,                  iterations = 10,                  train_type = \"lm\",                  output_format = \"target_right\",                  path_corpus = temp_dir,                   batch_size = 32,                  maxlen = 5,                  step = 1,                  file_name_start = \"batch_\")  list.files(out_dir) }"},{"path":"https://genomenet.github.io/deepG/reference/deepG-package.html","id":null,"dir":"Reference","previous_headings":"","what":"deepG for GenomeNet — deepG-package","title":"deepG for GenomeNet — deepG-package","text":"deepG open source software library building deep neural networks genomic modeling.","code":""},{"path":"https://genomenet.github.io/deepG/reference/deepG-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"deepG for GenomeNet — deepG-package","text":"package generates deepG additional documentation deepG package see https://genomenet.de","code":""},{"path":[]},{"path":"https://genomenet.github.io/deepG/reference/deepG-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"deepG for GenomeNet — deepG-package","text":"Maintainer: René Mreches rene.mreches@helmholtz-hzi.de Authors: Philipp Münch philipp.muench@helmholtz-hzi.de Martin Binder developer.mb706@mb706.com Hüseyin Anil Gündüz anil.guenduez@stat.uni-muenchen.de Xiao-Yin x.@stat.uni-muenchen.de Alice McHardy alice.mchardy@helmholtz-hzi.de","code":""},{"path":"https://genomenet.github.io/deepG/reference/early_stopping_time_cb.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop training callback — early_stopping_time_cb","title":"Stop training callback — early_stopping_time_cb","text":"Stop training specified time.","code":""},{"path":"https://genomenet.github.io/deepG/reference/early_stopping_time_cb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop training callback — early_stopping_time_cb","text":"","code":"early_stopping_time_cb(stop_time = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/early_stopping_time_cb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop training callback — early_stopping_time_cb","text":"stop_time Time seconds stop training.","code":""},{"path":"https://genomenet.github.io/deepG/reference/early_stopping_time_cb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stop training callback — early_stopping_time_cb","text":"Keras callback stops training specified time.","code":""},{"path":"https://genomenet.github.io/deepG/reference/early_stopping_time_cb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stop training callback — early_stopping_time_cb","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") est <- early_stopping_time_cb(stop_time = 60) }"},{"path":"https://genomenet.github.io/deepG/reference/ecoli_small.html","id":null,"dir":"Reference","previous_headings":"","what":"Ecoli subset — ecoli_small","title":"Ecoli subset — ecoli_small","text":"Subset E. coli genome evaluation. Can loaded workspace via data(ecoli_small).","code":""},{"path":"https://genomenet.github.io/deepG/reference/ecoli_small.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ecoli subset — ecoli_small","text":"","code":"data(ecoli_small)"},{"path":"https://genomenet.github.io/deepG/reference/ecoli_small.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Ecoli subset — ecoli_small","text":"character 326.73 kB","code":""},{"path":"https://genomenet.github.io/deepG/reference/ecoli_small.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Ecoli subset — ecoli_small","text":"https://www.science.org/doi/10.1126/science.277.5331.1453","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","title":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","text":"Compute MAE MSE, given predictions true targets. Outputs columnwise average.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","text":"","code":"evaluate_linear(y_true, y_pred, label_names = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","text":"y_true Matrix true labels. y_pred Matrix predictions. label_names Names corresponding labels. Length must equal number columns y.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_linear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","text":"list evaluation results.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_linear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate matrices of true targets and predictions from layer with linear activation. — evaluate_linear","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") y_true <- matrix(rnorm(n = 12), ncol = 3) y_pred <- matrix(rnorm(n = 12), ncol = 3) evaluate_linear(y_true, y_pred) }"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","title":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","text":"Returns evaluation metric like confusion matrix, loss, AUC, AUPRC, MAE, MSE (depending output layer).","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","text":"","code":"evaluate_model(   path_input,   model = NULL,   batch_size = 100,   step = 1,   padding = FALSE,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   vocabulary_label = list(c(\"a\", \"c\", \"g\", \"t\")),   number_batches = 10,   format = \"fasta\",   target_middle = FALSE,   mode = \"lm\",   output_format = \"target_right\",   ambiguous_nuc = \"zero\",   evaluate_all_files = FALSE,   verbose = TRUE,   max_iter = 20000,   target_from_csv = NULL,   max_samples = NULL,   proportion_per_seq = NULL,   concat_seq = NULL,   seed = 1234,   auc = FALSE,   auprc = FALSE,   path_pred_list = NULL,   exact_num_samples = NULL,   activations = NULL,   shuffle_file_order = FALSE,   include_seq = FALSE,   ... )"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","text":"path_input Input directory fasta, fastq rds files located. model keras model. batch_size Number samples per batch. step often take sample. padding Whether pad sequences short one sample zeros. vocabulary Vector allowed characters. Character outside vocabulary get encoded specified ambiguous_nuc. vocabulary_label List labels targets output layer. number_batches many batches evaluate. format File format, \"fasta\", \"fastq\" \"rds\". target_middle Whether model language model separate input layers. mode Either \"lm\" language model \"label_header\", \"label_csv\" \"label_folder\" label classification. output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. evaluate_all_files Boolean, TRUE iterate files path_input . number_batches overwritten. verbose Boolean. max_iter Stop max_iter number iterations failed produce new batch. target_from_csv Path csv file target mapping. One column called \"file\" entries row targets. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. seed Sets seed set.seed function reproducible results. auc Whether include AUC metric. output layer activation \"softmax\", possible 2 targets. Computes average output layer sigmoid activation multiple targets. auprc Whether include AUPRC metric. output layer activation \"softmax\", possible 2 targets. Computes average output layer sigmoid activation multiple targets. path_pred_list Path store list predictions (output output layers) corresponding true labels rds file. exact_num_samples Exact number samples evaluate. want evaluate number samples divisible batch_size. Useful want evaluate data set exactly ones know number samples already. vector mode = \"label_folder\" (length vocabulary_label) else integer. activations List containing output formats output layers (softmax, sigmoid linear). NULL, estimated model. shuffle_file_order Logical, whether go files randomly sequentially. include_seq Whether store input. applies path_pred_list NULL. ... generator options. See get_generator.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","text":"list evaluation results. list element corresponds output layer model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluates a trained model on fasta, fastq or rds files — evaluate_model","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy data path_input <- tempfile() dir.create(path_input) create_dummy_data(file_path = path_input,                   num_files = 3,                   seq_length = 11,                    num_seq = 5,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\")) # create model model <- create_model_lstm_cnn(layer_lstm = 8, layer_dense = 4, maxlen = 10, verbose = FALSE) # evaluate evaluate_model(path_input = path_input,   model = model,   step = 11,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   vocabulary_label = list(c(\"a\", \"c\", \"g\", \"t\")),   mode = \"lm\",   output_format = \"target_right\",   evaluate_all_files = TRUE,   verbose = FALSE)    }"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","title":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","text":"Compute accuracy, binary crossentropy (optionally) AUC AUPRC, given predictions true targets. Outputs columnwise average.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","text":"","code":"evaluate_sigmoid(y, y_conf, auc = FALSE, auprc = FALSE, label_names = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","text":"y Matrix true target. y_conf Matrix predictions. auc Whether include AUC metric. auprc Whether include AUPRC metric. label_names Names corresponding labels. Length must equal number columns y.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","text":"list evaluation results.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_sigmoid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate matrices of true targets and predictions from layer with sigmoid activation. — evaluate_sigmoid","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") y <- matrix(sample(c(0, 1), 30, replace = TRUE), ncol = 3) y_conf <- matrix(runif(n = 30), ncol = 3) evaluate_sigmoid(y, y_conf, auc = TRUE, auprc = TRUE) }"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","title":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","text":"Compute confusion matrix, accuracy, categorical crossentropy (optionally) AUC AUPRC, given predictions true targets. AUC AUPRC possible 2 targets.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","text":"","code":"evaluate_softmax(y, y_conf, auc = FALSE, auprc = FALSE, label_names = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/evaluate_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","text":"y Matrix true target. y_conf Matrix predictions. auc Whether include AUC metric. possible 2 targets. auprc Whether include AUPRC metric. possible 2 targets. label_names Names corresponding labels. Length must equal number columns y.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_softmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","text":"list evaluation results.","code":""},{"path":"https://genomenet.github.io/deepG/reference/evaluate_softmax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate matrices of true targets and predictions from layer with softmax activation. — evaluate_softmax","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") y <- matrix(c(1, 0, 0, 0, 1, 1), ncol = 2) y_conf <- matrix(c(0.3, 0.5, 0.1, 0.7, 0.5, 0.9), ncol = 2) evaluate_softmax(y, y_conf, auc = TRUE, auprc = TRUE, label_names = c(\"A\", \"B\"))  }"},{"path":"https://genomenet.github.io/deepG/reference/exp_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential Decay — exp_decay","title":"Exponential Decay — exp_decay","text":"Compute learning Rate given epoch using Exponential Decay.","code":""},{"path":"https://genomenet.github.io/deepG/reference/exp_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential Decay — exp_decay","text":"","code":"exp_decay(lrmax = 0.005, mult = 0.1, epoch = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/exp_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential Decay — exp_decay","text":"lrmax Upper limit range learning rate. mult Factor, number epochs restart decreased new step. epoch Epoch, learning rate shall calculated.","code":""},{"path":"https://genomenet.github.io/deepG/reference/exp_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exponential Decay — exp_decay","text":"numeric value.","code":""},{"path":"https://genomenet.github.io/deepG/reference/exp_decay.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Exponential Decay — exp_decay","text":"","code":"exp_decay(lrmax = 0.005, mult = 0.1, epoch = 8)  #> [1] 0.002246645"},{"path":"https://genomenet.github.io/deepG/reference/f1_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"F1 metric — f1_wrapper","title":"F1 metric — f1_wrapper","text":"Compute F1 metric. loss \"categorical_crossentropy\", number targets must 2. loss \"binary_crossentropy\" number targets > 1, flatten y_true y_pred matrices single vector (rather computing separate F1 scores class).","code":""},{"path":"https://genomenet.github.io/deepG/reference/f1_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"F1 metric — f1_wrapper","text":"","code":"f1_wrapper(num_targets = 2, loss = \"binary_crossentropy\")"},{"path":"https://genomenet.github.io/deepG/reference/f1_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"F1 metric — f1_wrapper","text":"num_targets Size model output. loss Loss function model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/f1_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"F1 metric — f1_wrapper","text":"keras metric.","code":""},{"path":"https://genomenet.github.io/deepG/reference/f1_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"F1 metric — f1_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\")  y_true <- c(1,0,0,1,1,0,1,0,0)   y_pred <-  c(0.9,0.05,0.05,0.9,0.05,0.05,0.9,0.05,0.05)  f1_metric <- f1_wrapper(3L, \"binary_crossentropy\") f1_metric$update_state(y_true, y_pred) f1_metric$result()    # add metric to a model  num_targets <- 1 model <- create_model_lstm_cnn(maxlen = 20,                                layer_lstm = 8,                                bal_acc = FALSE,                                last_layer_activation = \"sigmoid\",                                loss_fn = \"binary_crossentropy\",                                layer_dense = c(8, num_targets))  f1_metric <- f1_wrapper(num_targets, loss = model$loss) model %>% keras::compile(loss = model$loss,                           optimizer = model$optimizer,                          metrics = c(model$metrics, f1_metric)) }"},{"path":"https://genomenet.github.io/deepG/reference/focal_loss_multiclass.html","id":null,"dir":"Reference","previous_headings":"","what":"Focal loss for two or more labels — focal_loss_multiclass","title":"Focal loss for two or more labels — focal_loss_multiclass","text":"Focal loss two labels","code":""},{"path":"https://genomenet.github.io/deepG/reference/focal_loss_multiclass.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Focal loss for two or more labels — focal_loss_multiclass","text":"","code":"focal_loss_multiclass(y_true, y_pred, gamma = 2.5, alpha = c(1))"},{"path":"https://genomenet.github.io/deepG/reference/focal_loss_multiclass.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Focal loss for two or more labels — focal_loss_multiclass","text":"y_true Vector true values. y_pred Vector predicted values. gamma Focusing parameter. alpha Vector weighting factors.","code":""},{"path":"https://genomenet.github.io/deepG/reference/focal_loss_multiclass.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Focal loss for two or more labels — focal_loss_multiclass","text":"function implementing focal loss.","code":""},{"path":"https://genomenet.github.io/deepG/reference/focal_loss_multiclass.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Focal loss for two or more labels — focal_loss_multiclass","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") y_true <- matrix(c(0, 1, 0, 0, 0, 1), nrow = 2, byrow = TRUE) y_pred <- matrix(c(0.15, 0.8, 0.05,                    0.08, 0.02, 0.9), nrow = 2, byrow = TRUE)  fl <- focal_loss_multiclass(y_true, y_pred) fl$numpy() }"},{"path":"https://genomenet.github.io/deepG/reference/generator_dummy.html","id":null,"dir":"Reference","previous_headings":"","what":"Random data generator — generator_dummy","title":"Random data generator — generator_dummy","text":"Creates random input/target list repeatedly returns list.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_dummy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random data generator — generator_dummy","text":"","code":"generator_dummy(model, batch_size)"},{"path":"https://genomenet.github.io/deepG/reference/generator_dummy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random data generator — generator_dummy","text":"model keras model. batch_size Number samples one batch.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_dummy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random data generator — generator_dummy","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_dummy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random data generator — generator_dummy","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_lstm_cnn(   maxlen = 10,   layer_lstm = c(4),   layer_dense = c(5)) gen <- generator_dummy(model, 12) z <- gen() x <- z[[1]] y <- z[[2]] dim(x) dim(y) }"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder.html","id":null,"dir":"Reference","previous_headings":"","what":"Data generator for fasta/fasta files — generator_fasta_label_folder","title":"Data generator for fasta/fasta files — generator_fasta_label_folder","text":"Iterates folder containing fasta/fastq files produces encoding predictor sequences target variables. Files path_corpus belong one class.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data generator for fasta/fasta files — generator_fasta_label_folder","text":"","code":"generator_fasta_label_folder(   path_corpus,   format = \"fasta\",   batch_size = 256,   maxlen = 250,   max_iter = 10000,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   verbose = FALSE,   shuffle_file_order = FALSE,   step = 1,   seed = 1234,   shuffle_input = FALSE,   file_limit = NULL,   path_file_log = NULL,   reverse_complement = TRUE,   reverse_complement_encoding = FALSE,   num_targets,   ones_column,   ambiguous_nuc = \"zero\",   proportion_per_seq = NULL,   read_data = FALSE,   use_quality_score = FALSE,   padding = TRUE,   added_label_path = NULL,   add_input_as_seq = NULL,   skip_amb_nuc = NULL,   max_samples = NULL,   concat_seq = NULL,   file_filter = NULL,   use_coverage = NULL,   proportion_entries = NULL,   sample_by_file_size = FALSE,   n_gram = NULL,   n_gram_stride = 1,   masked_lm = NULL,   add_noise = NULL,   return_int = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data generator for fasta/fasta files — generator_fasta_label_folder","text":"path_corpus Input directory fasta files located path single file ending fasta fastq (specified format argument). Can also list directories /files. format File format, either \"fasta\" \"fastq\". batch_size Number samples one batch. maxlen Length predictor sequence. max_iter Stop max_iter number iterations failed produce new batch. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. verbose Whether show messages. shuffle_file_order Logical, whether go files randomly sequentially. step often take sample. seed Sets seed set.seed function reproducible results. shuffle_input Whether shuffle entries every fasta/fastq file extracting samples. file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. path_file_log Write name files csv file path specified. reverse_complement Boolean, every new file decide randomly use original data reverse complement. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. num_targets Number columns target matrix. ones_column column target matrix contains ones. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). read_data TRUE first element output list length 2, containing one part paired read. Maxlen 2*length one read. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). padding Whether pad sequences short one sample zeros. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. file_filter Vector file names use path_corpus. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. sample_by_file_size Sample new file weighted file size (bigger files likely). n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). return_int Whether return integer encoding one-hot encoding. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data generator for fasta/fasta files — generator_fasta_label_folder","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data generator for fasta/fasta files — generator_fasta_label_folder","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy fasta files path_input_1 <- tempfile() dir.create(path_input_1) create_dummy_data(file_path = path_input_1,                    num_files = 2,                   seq_length = 7,                   num_seq = 1,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\"))  gen <- generator_fasta_label_folder(path_corpus = path_input_1, batch_size = 2,                                     num_targets = 3, ones_column = 2, maxlen = 7) z <- gen() dim(z[[1]]) z[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Generator wrapper — generator_fasta_label_folder_wrapper","title":"Generator wrapper — generator_fasta_label_folder_wrapper","text":"Combines generators created generator_initialize single generator.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generator wrapper — generator_fasta_label_folder_wrapper","text":"","code":"generator_fasta_label_folder_wrapper(   val,   batch_size = NULL,   path = NULL,   voc_len = NULL,   maxlen = NULL,   gen_list = NULL,   set_learning = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generator wrapper — generator_fasta_label_folder_wrapper","text":"val Train validation generator. batch_size Number samples one batch. path Path input files. voc_len Length vocabulary. maxlen Length predictor sequence. gen_list List generator functions. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1).","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generator wrapper — generator_fasta_label_folder_wrapper","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_folder_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generator wrapper — generator_fasta_label_folder_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create two folders with dummy fasta files path_input_1 <- tempfile() dir.create(path_input_1) create_dummy_data(file_path = path_input_1, num_files = 2, seq_length = 5,                   num_seq = 2, vocabulary = c(\"a\", \"c\", \"g\", \"t\")) path_input_2 <- tempfile() dir.create(path_input_2) create_dummy_data(file_path = path_input_2, num_files = 3, seq_length = 7,                   num_seq = 5, vocabulary = c(\"a\", \"c\", \"g\", \"t\"))  maxlen <- 5 p <- c(path_input_1, path_input_1) gen_list <- generator_initialize(directories = p,                                  batch_size = 4, maxlen = maxlen) gen <- generator_fasta_label_folder_wrapper(val = FALSE, batch_size = 8,                                             path = p, voc_len = 4,                                              maxlen = maxlen,                                             gen_list = gen_list) z <- gen() dim(z[[1]]) z[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_header_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","title":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","text":"Iterates folder containing fasta/fastq files produces encoding predictor sequences target variables. Targets read fasta headers separate csv file.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_header_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","text":"","code":"generator_fasta_label_header_csv(   path_corpus,   format = \"fasta\",   batch_size = 256,   maxlen = 250,   max_iter = 10000,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   verbose = FALSE,   shuffle_file_order = FALSE,   step = 1,   seed = 1234,   shuffle_input = FALSE,   file_limit = NULL,   path_file_log = NULL,   vocabulary_label = c(\"x\", \"y\", \"z\"),   reverse_complement = TRUE,   ambiguous_nuc = \"zero\",   proportion_per_seq = NULL,   read_data = FALSE,   use_quality_score = FALSE,   padding = TRUE,   skip_amb_nuc = NULL,   max_samples = NULL,   concat_seq = NULL,   added_label_path = NULL,   add_input_as_seq = NULL,   target_from_csv = NULL,   target_split = NULL,   file_filter = NULL,   use_coverage = NULL,   proportion_entries = NULL,   sample_by_file_size = FALSE,   reverse_complement_encoding = FALSE,   n_gram = NULL,   n_gram_stride = 1,   add_noise = NULL,   return_int = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_header_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","text":"path_corpus Input directory fasta files located path single file ending fasta fastq (specified format argument). Can also list directories /files. format File format, either \"fasta\" \"fastq\". batch_size Number samples one batch. maxlen Length predictor sequence. max_iter Stop max_iter number iterations failed produce new batch. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. verbose Whether show messages. shuffle_file_order Logical, whether go files randomly sequentially. step often take sample. seed Sets seed set.seed function reproducible results. shuffle_input Whether shuffle entries every fasta/fastq file extracting samples. file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. path_file_log Write name files csv file path specified. vocabulary_label Character vector possible targets. Targets outside vocabulary_label get discarded. reverse_complement Boolean, every new file decide randomly use original data reverse complement. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). read_data TRUE first element input list length 2, containing one part paired read. Maxlen 2*length one read. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). padding Whether pad sequences short one sample zeros. skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). target_from_csv Path csv file target mapping. One column called \"file\" entries row targets. target_split target gets read csv file, list names divide target tensor list tensors. Example: csv file header names \"file\", \"label_1\", \"label_2\", \"label_3\" target_split = list(c(\"label_1\", \"label_2\"), \"label_3\"), divide target matrix list length 2, first element contains columns named \"label_1\" \"label_2\" second entry contains column named \"label_3\". file_filter Vector file names use path_corpus. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. sample_by_file_size Sample new file weighted file size (bigger files likely). reverse_complement_encoding Whether use original sequence reverse complement two input sequences. n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). return_int Whether return integer encoding one-hot encoding. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_header_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_label_header_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data generator for fasta/fastq files and label targets — generator_fasta_label_header_csv","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") path_input <- tempfile() dir.create(path_input) # create 2 fasta files called 'file_1.fasta', 'file_2.fasta' create_dummy_data(file_path = path_input,                    num_files = 2,                   seq_length = 5,                   num_seq = 1,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\")) dummy_labels <- data.frame(file = c('file_1.fasta', 'file_2.fasta'), # dummy labels                            label1 = c(0, 1),                            label2 = c(1, 0)) target_from_csv <- tempfile(fileext = '.csv') write.csv(dummy_labels, target_from_csv, row.names = FALSE) gen <- generator_fasta_label_header_csv(path_corpus = path_input, batch_size = 2,                                          maxlen = 5, target_from_csv = target_from_csv) z <- gen() dim(z[[1]]) z[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_lm.html","id":null,"dir":"Reference","previous_headings":"","what":"Language model generator for fasta/fastq files — generator_fasta_lm","title":"Language model generator for fasta/fastq files — generator_fasta_lm","text":"Iterates folder containing fasta/fastq files produces encoding predictor sequences target variables. take sequence fixed size use part sequence input part target.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Language model generator for fasta/fastq files — generator_fasta_lm","text":"","code":"generator_fasta_lm(   path_corpus,   format = \"fasta\",   batch_size = 256,   maxlen = 250,   max_iter = 10000,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   verbose = FALSE,   shuffle_file_order = FALSE,   step = 1,   seed = 1234,   shuffle_input = FALSE,   file_limit = NULL,   path_file_log = NULL,   reverse_complement = FALSE,   output_format = \"target_right\",   ambiguous_nuc = \"zeros\",   use_quality_score = FALSE,   proportion_per_seq = NULL,   padding = TRUE,   added_label_path = NULL,   add_input_as_seq = NULL,   skip_amb_nuc = NULL,   max_samples = NULL,   concat_seq = NULL,   target_len = 1,   file_filter = NULL,   use_coverage = NULL,   proportion_entries = NULL,   sample_by_file_size = FALSE,   n_gram = NULL,   n_gram_stride = 1,   add_noise = NULL,   return_int = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Language model generator for fasta/fastq files — generator_fasta_lm","text":"path_corpus Input directory fasta files located path single file ending fasta fastq (specified format argument). Can also list directories /files. format File format, either \"fasta\" \"fastq\". batch_size Number samples one batch. maxlen Length predictor sequence. max_iter Stop max_iter number iterations failed produce new batch. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. verbose Whether show messages. shuffle_file_order Logical, whether go files randomly sequentially. step often take sample. seed Sets seed set.seed function reproducible results. shuffle_input Whether shuffle entries every fasta/fastq file extracting samples. file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. path_file_log Write name files csv file path specified. reverse_complement Boolean, every new file decide randomly use original data reverse complement. output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). padding Whether pad sequences short one sample zeros. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. target_len Number nucleotides predict language model. file_filter Vector file names use path_corpus. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. sample_by_file_size Sample new file weighted file size (bigger files likely). n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). return_int Whether return integer encoding one-hot encoding. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Language model generator for fasta/fastq files — generator_fasta_lm","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_fasta_lm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Language model generator for fasta/fastq files — generator_fasta_lm","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy fasta files path_input_1 <- tempfile() dir.create(path_input_1) create_dummy_data(file_path = path_input_1,                   num_files = 2,                   seq_length = 8,                   num_seq = 1,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\"))  gen <- generator_fasta_lm(path_corpus = path_input_1, batch_size = 2,                                    maxlen = 7) z <- gen() dim(z[[1]]) z[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_initialize.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","title":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","text":"Initializes generators defined generator_fasta_label_folder function. Targets get encoded order directories. Number classes given length directories.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_initialize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","text":"","code":"generator_initialize(   directories,   format = \"fasta\",   batch_size = 256,   maxlen = 250,   max_iter = 10000,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   verbose = FALSE,   shuffle_file_order = FALSE,   step = 1,   seed = 1234,   shuffle_input = FALSE,   file_limit = NULL,   path_file_log = NULL,   reverse_complement = FALSE,   reverse_complement_encoding = FALSE,   val = FALSE,   ambiguous_nuc = \"zero\",   proportion_per_seq = NULL,   target_middle = FALSE,   read_data = FALSE,   use_quality_score = FALSE,   padding = TRUE,   added_label_path = NULL,   add_input_as_seq = NULL,   skip_amb_nuc = NULL,   max_samples = NULL,   file_filter = NULL,   concat_seq = NULL,   use_coverage = NULL,   set_learning = NULL,   proportion_entries = NULL,   sample_by_file_size = FALSE,   n_gram = NULL,   n_gram_stride = 1,   add_noise = NULL,   return_int = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_initialize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","text":"directories Vector paths folder containing fasta files. Files one folder belong one class. format File format, either \"fasta\" \"fastq\". batch_size Number samples one batch. maxlen Length predictor sequence. max_iter Stop max_iter number iterations failed produce new batch. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. verbose Whether show messages. shuffle_file_order Logical, whether go files randomly sequentially. step often take sample. seed Sets seed set.seed function reproducible results. shuffle_input Whether shuffle entries every fasta/fastq file extracting samples. file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. path_file_log Write name files csv file path specified. reverse_complement Boolean, every new file decide randomly use original data reverse complement. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. val Logical, call initialized generator \"genY\" \"genValY\" Y integer 1 length directories. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). target_middle Split input sequence two sequences removing nucleotide middle. input x_1,..., x_(n+1), input gets split input_1 = x_1,..., x_m input_2 = x_(n+1),..., x_(m+2) m = ceiling((n+1)/2) n = maxlen. Note x_(m+1) used. read_data true first element output list length 2, containing one part paired read. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). padding Whether pad sequences short one sample zeros. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. file_filter Vector file names use path_corpus. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. sample_by_file_size Sample new file weighted file size (bigger files likely). n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). return_int Whether return integer encoding one-hot encoding. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_initialize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","text":"List generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_initialize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializes generators defined by generator_fasta_label_folder function — generator_initialize","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create two folders with dummy fasta files path_input_1 <- tempfile() dir.create(path_input_1) create_dummy_data(file_path = path_input_1, num_files = 2, seq_length = 5,                   num_seq = 2, vocabulary = c(\"a\", \"c\", \"g\", \"t\")) path_input_2 <- tempfile() dir.create(path_input_2) create_dummy_data(file_path = path_input_2, num_files = 3, seq_length = 7,                   num_seq = 5, vocabulary = c(\"a\", \"c\", \"g\", \"t\"))  gen_list <- generator_initialize(directories = c(path_input_1, path_input_1),                                         batch_size = 4, maxlen = 5) z1 <- gen_list[[1]]() z1[[1]] z1[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_random.html","id":null,"dir":"Reference","previous_headings":"","what":"Randomly select samples from fasta files — generator_random","title":"Randomly select samples from fasta files — generator_random","text":"Generator generator_fasta_lm, generator_fasta_label_header_csv generator_fasta_label_folder randomly choose consecutive sequence samples max_samples argument supplied. generator_random choose samples random.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_random.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Randomly select samples from fasta files — generator_random","text":"","code":"generator_random(   train_type = \"label_folder\",   output_format = NULL,   seed = 123,   format = \"fasta\",   reverse_complement = TRUE,   path = NULL,   batch_size = c(100),   maxlen = 4,   ambiguous_nuc = \"equal\",   padding = FALSE,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   number_target_nt = 1,   n_gram = NULL,   n_gram_stride = NULL,   sample_by_file_size = TRUE,   max_samples = 1,   skip_amb_nuc = NULL,   vocabulary_label = NULL,   target_from_csv = NULL,   target_split = NULL,   max_iter = 1000,   verbose = TRUE,   set_learning = NULL,   shuffle_input = TRUE,   reverse_complement_encoding = FALSE,   proportion_entries = NULL,   masked_lm = NULL,   concat_seq = NULL,   return_int = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_random.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Randomly select samples from fasta files — generator_random","text":"train_type Either \"lm\", \"lm_rds\", \"masked_lm\" language model; \"label_header\", \"label_folder\", \"label_csv\", \"label_rds\" classification \"dummy_gen\". Language model trained predict character(s) sequence. \"label_header\"/\"label_folder\"/\"label_csv\" trained predict corresponding class given sequence input. \"label_header\", class read fasta headers. \"label_folder\", class read folder, .e. files one folder must belong class. \"label_csv\", targets read csv file. file one column named \"file\". targets correspond entries row (except \"file\" column). Example: currently working file called \".fasta\" corresponding label \"label_1\", row csv file \"label_rds\", generator iterate set .rds files containing list input target tensors. implemented model multiple inputs. \"lm_rds\", generator iterate set .rds files split tensor according target_len argument (targets last target_len nucleotides sequence). \"dummy_gen\", generator creates random data repeatedly feeds model. \"masked_lm\", generator maskes parts input. See masked_lm argument details. output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" seed Sets seed set.seed function reproducible results. format File format, either \"fasta\" \"fastq\". reverse_complement Boolean, every new file decide randomly use original data reverse complement. path Path training data. train_type label_folder, vector list entry corresponds class (list elements can directories /individual files). train_type label_folder, can single directory file list directories /files. batch_size Number samples one batch. maxlen Length predictor sequence. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. padding Whether pad sequences short one sample zeros. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. number_target_nt Number target nucleotides language model. n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). sample_by_file_size Sample new file weighted file size (bigger files likely). max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. vocabulary_label Character vector possible targets. Targets outside vocabulary_label get discarded. target_from_csv Path csv file target mapping. One column called \"file\" entries row targets. target_split target gets read csv file, list names divide target tensor list tensors. Example: csv file header names \"file\", \"label_1\", \"label_2\", \"label_3\" target_split = list(c(\"label_1\", \"label_2\"), \"label_3\"), divide target matrix list length 2, first element contains columns named \"label_1\" \"label_2\" second entry contains column named \"label_3\". max_iter Stop max_iter number iterations failed produce new batch. verbose Whether show messages. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). shuffle_input Whether shuffle entries every fasta/fastq file extracting samples. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. return_int Whether return integer encoding one-hot encoding. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_random.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Randomly select samples from fasta files — generator_random","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_random.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Randomly select samples from fasta files — generator_random","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") path_input <- tempfile() dir.create(path_input) # create 2 fasta files called 'file_1.fasta', 'file_2.fasta' create_dummy_data(file_path = path_input,                   num_files = 2,                   seq_length = 5,                   num_seq = 1,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\")) dummy_labels <- data.frame(file = c('file_1.fasta', 'file_2.fasta'), # dummy labels                            label1 = c(0, 1),                            label2 = c(1, 0)) target_from_csv <- tempfile(fileext = '.csv') write.csv(dummy_labels, target_from_csv, row.names = FALSE) gen <- generator_random(path = path_input, batch_size = 2,                         vocabulary_label = c('label_a', 'label_b'),                         train_type = 'label_csv',                         maxlen = 5, target_from_csv = target_from_csv) z <- gen() dim(z[[1]]) z[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/generator_rds.html","id":null,"dir":"Reference","previous_headings":"","what":"Rds data generator — generator_rds","title":"Rds data generator — generator_rds","text":"Creates training batches rds files. Rds files must contain list length 2 (input/target) length 1 (language model). target_len NULL take last target_len entries first list element targets rest input.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_rds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rds data generator — generator_rds","text":"","code":"generator_rds(   rds_folder,   batch_size,   path_file_log = NULL,   max_samples = NULL,   proportion_per_seq = NULL,   target_len = NULL,   seed = NULL,   delete_used_files = FALSE,   reverse_complement = FALSE,   sample_by_file_size = FALSE,   n_gram = NULL,   n_gram_stride = 1,   reverse_complement_encoding = FALSE,   add_noise = NULL,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/generator_rds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rds data generator — generator_rds","text":"rds_folder Path input files. batch_size Number samples one batch. path_file_log Write name files csv file path specified. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). target_len Number target nucleotides language model. seed Sets seed set.seed function reproducible results. delete_used_files Whether delete file used. applies rds files. reverse_complement Boolean, every new file decide randomly use original data reverse complement. sample_by_file_size Sample new file weighted file size (bigger files likely). n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). reverse_complement_encoding Whether use original sequence reverse complement two input sequences. add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_rds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rds data generator — generator_rds","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/generator_rds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rds data generator — generator_rds","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create 3 rds files rds_folder <- tempfile() dir.create(rds_folder) batch_size <- 7 maxlen <- 11 voc_len <- 4 for (i in 1:3) {   x <- sample(0:(voc_len-1), maxlen*batch_size, replace = TRUE)   x <- keras::to_categorical(x, num_classes = voc_len)   x <- array(x, dim = c(batch_size, maxlen, voc_len))   y <- sample(0:2, batch_size ,replace = TRUE)   y <- keras::to_categorical(y, num_classes = 3)   xy_list <- list(x, y)   file_name <- paste0(rds_folder, \"/file_\", i, \".rds\")   saveRDS(xy_list, file_name)  }  # create generator gen <- generator_rds(rds_folder, batch_size = 2) z <- gen() x <- z[[1]] y <- z[[2]] x[1, , ] y[1, ] }"},{"path":"https://genomenet.github.io/deepG/reference/get_class_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate frequency of different classes — get_class_weight","title":"Estimate frequency of different classes — get_class_weight","text":"Count number nucleotides class use estimation relation class distribution. Outputs list class relations. Can used input class_weigth train_model function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_class_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate frequency of different classes — get_class_weight","text":"","code":"get_class_weight(   path,   vocabulary_label = NULL,   format = \"fasta\",   file_proportion = 1,   train_type = \"label_folder\",   named_list = FALSE,   csv_path = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/get_class_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate frequency of different classes — get_class_weight","text":"path Path training data. train_type label_folder, vector list entry corresponds class (list elements can directories /individual files). train_type label_folder, can single directory file list directories /files. vocabulary_label Character vector possible targets. Targets outside vocabulary_label get discarded. format File format, either \"fasta\" \"fastq\". file_proportion Proportion files randomly sample estimating class distributions. train_type Either \"lm\", \"lm_rds\", \"masked_lm\" language model; \"label_header\", \"label_folder\", \"label_csv\", \"label_rds\" classification \"dummy_gen\". Language model trained predict character(s) sequence. \"label_header\"/\"label_folder\"/\"label_csv\" trained predict corresponding class given sequence input. \"label_header\", class read fasta headers. \"label_folder\", class read folder, .e. files one folder must belong class. \"label_csv\", targets read csv file. file one column named \"file\". targets correspond entries row (except \"file\" column). Example: currently working file called \".fasta\" corresponding label \"label_1\", row csv file \"label_rds\", generator iterate set .rds files containing list input target tensors. implemented model multiple inputs. \"lm_rds\", generator iterate set .rds files split tensor according target_len argument (targets last target_len nucleotides sequence). \"dummy_gen\", generator creates random data repeatedly feeds model. \"masked_lm\", generator maskes parts input. See masked_lm argument details. named_list Whether give class weight list names \"0\", \"1\", ... . csv_path train_type = \"label_csv\", path csv file containing labels.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_class_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate frequency of different classes — get_class_weight","text":"list numeric values (class weights).","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_class_weight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate frequency of different classes — get_class_weight","text":"","code":"# create dummy data path_1 <- tempfile() path_2 <- tempfile()  for (current_path in c(path_1, path_2)) {      dir.create(current_path)   # create twice as much data for first class   num_files <- ifelse(current_path == path_1, 6, 3)   create_dummy_data(file_path = current_path,                     num_files = num_files,                     seq_length = 10,                     num_seq = 5,                     vocabulary = c(\"a\", \"c\", \"g\", \"t\")) }   class_weight <- get_class_weight(   path = c(path_1, path_2),   vocabulary_label = c(\"A\", \"B\"),   format = \"fasta\",   file_proportion = 1,   train_type = \"label_folder\",   csv_path = NULL)  class_weight #> $`0` #>    A  #> 0.75  #>  #> $`1` #>   B  #> 1.5  #>"},{"path":"https://genomenet.github.io/deepG/reference/get_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for generator functions — get_generator","title":"Wrapper for generator functions — get_generator","text":"detailed description see data generator tutorial. choose one generators generator_fasta_lm, generator_fasta_label_folder, generator_fasta_label_header_csv, generator_rds, generator_random, generator_dummy generator_fasta_lm according train_type random_sampling arguments.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for generator functions — get_generator","text":"","code":"get_generator(   path = NULL,   train_type,   batch_size,   maxlen,   step = NULL,   shuffle_file_order = FALSE,   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),   seed = 1,   proportion_entries = NULL,   shuffle_input = FALSE,   format = \"fasta\",   path_file_log = NULL,   reverse_complement = FALSE,   n_gram = NULL,   n_gram_stride = NULL,   output_format = \"target_right\",   ambiguous_nuc = \"zero\",   proportion_per_seq = NULL,   skip_amb_nuc = NULL,   use_quality_score = FALSE,   padding = FALSE,   added_label_path = NULL,   target_from_csv = NULL,   add_input_as_seq = NULL,   max_samples = NULL,   concat_seq = NULL,   target_len = 1,   file_filter = NULL,   use_coverage = NULL,   sample_by_file_size = FALSE,   add_noise = NULL,   random_sampling = FALSE,   set_learning = NULL,   file_limit = NULL,   reverse_complement_encoding = FALSE,   read_data = FALSE,   target_split = NULL,   path_file_logVal = NULL,   model = NULL,   vocabulary_label = NULL,   masked_lm = NULL,   val = FALSE,   return_int = FALSE,   verbose = TRUE,   delete_used_files = FALSE,   reshape_xy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/get_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for generator functions — get_generator","text":"path Path training data. train_type label_folder, vector list entry corresponds class (list elements can directories /individual files). train_type label_folder, can single directory file list directories /files. train_type Either \"lm\", \"lm_rds\", \"masked_lm\" language model; \"label_header\", \"label_folder\", \"label_csv\", \"label_rds\" classification \"dummy_gen\". Language model trained predict character(s) sequence. \"label_header\"/\"label_folder\"/\"label_csv\" trained predict corresponding class given sequence input. \"label_header\", class read fasta headers. \"label_folder\", class read folder, .e. files one folder must belong class. \"label_csv\", targets read csv file. file one column named \"file\". targets correspond entries row (except \"file\" column). Example: currently working file called \".fasta\" corresponding label \"label_1\", row csv file \"label_rds\", generator iterate set .rds files containing list input target tensors. implemented model multiple inputs. \"lm_rds\", generator iterate set .rds files split tensor according target_len argument (targets last target_len nucleotides sequence). \"dummy_gen\", generator creates random data repeatedly feeds model. \"masked_lm\", generator maskes parts input. See masked_lm argument details. batch_size Number samples used one network update. maxlen Length predictor sequence. step Frequency sampling steps. shuffle_file_order Boolean, whether go files sequentially shuffle beforehand. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. seed Sets seed reproducible results. proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. shuffle_input Whether shuffle entries file. format File format, \"fasta\", \"fastq\", \"rds\" \"fasta.tar.gz\", \"fastq.tar.gz\" tar.gz files. path_file_log Write name files used training csv file path specified. reverse_complement Boolean, every new file decide randomly use original data reverse complement. n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). padding Whether pad sequences short one sample zeros. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. target_from_csv Path csv file target mapping. One column called \"file\" entries row targets. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. target_len Number nucleotides predict language model. file_filter Vector file names use path_corpus. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. sample_by_file_size Sample new file weighted file size (bigger files likely). add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). random_sampling Whether samples taken random positions using max_samples argument. FALSE random samples taken consecutive subsequence. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. read_data TRUE first element output list length 2, containing one part paired read. Maxlen 2*length one read. target_split target gets read csv file, list names divide target tensor list tensors. Example: csv file header names \"file\", \"label_1\", \"label_2\", \"label_3\" target_split = list(c(\"label_1\", \"label_2\"), \"label_3\"), divide target matrix list length 2, first element contains columns named \"label_1\" \"label_2\" second entry contains column named \"label_3\". path_file_logVal Path csv file logging used validation files. model keras model. vocabulary_label Character vector possible targets. Targets outside vocabulary_label get discarded train_type = \"label_header\". masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. val Logical, call initialized generator \"genY\" \"genValY\" Y integer 1 length directories. return_int Whether return integer encoding one-hot encoding. verbose Whether show messages. delete_used_files Whether delete file used. applies rds files. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for generator functions — get_generator","text":"generator function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_generator.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for generator functions — get_generator","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy fasta files fasta_path <- tempfile() dir.create(fasta_path) create_dummy_data(file_path = fasta_path,                   num_files = 3,                   seq_length = 10,                   num_seq = 5,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\"))  gen <- get_generator(path = fasta_path,                      maxlen = 5, train_type = \"lm\",                      output_format = \"target_right\",                      step = 3, batch_size = 7) z <- gen() x <- z[[1]] y <- z[[2]] dim(x) dim(y) }"},{"path":"https://genomenet.github.io/deepG/reference/get_output_activations.html","id":null,"dir":"Reference","previous_headings":"","what":"Get activation functions of output layers — get_output_activations","title":"Get activation functions of output layers — get_output_activations","text":"Get activation functions output layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_output_activations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get activation functions of output layers — get_output_activations","text":"","code":"get_output_activations(model)"},{"path":"https://genomenet.github.io/deepG/reference/get_output_activations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get activation functions of output layers — get_output_activations","text":"model keras model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_output_activations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get activation functions of output layers — get_output_activations","text":"Character vector names activation functions model outputs.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_output_activations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get activation functions of output layers — get_output_activations","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <-  create_model_lstm_cnn(   maxlen = 50,   layer_lstm = 8,   layer_dense = c(64, 2),   verbose = FALSE) get_output_activations(model) }"},{"path":"https://genomenet.github.io/deepG/reference/get_start_ind.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes start position of samples — get_start_ind","title":"Computes start position of samples — get_start_ind","text":"Helper function data generators. Computes start positions sequence samples can extracted, given maxlen, step size ambiguous nucleotide constraints.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_start_ind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes start position of samples — get_start_ind","text":"","code":"get_start_ind(   seq_vector,   length_vector,   maxlen,   step,   train_mode = \"label\",   discard_amb_nuc = FALSE,   vocabulary = c(\"A\", \"C\", \"G\", \"T\") )"},{"path":"https://genomenet.github.io/deepG/reference/get_start_ind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes start position of samples — get_start_ind","text":"seq_vector Vector character sequences. length_vector Length sequences seq_vector. maxlen Length one predictor sequence. step Distance samples one entry seq_vector. train_mode Either \"lm\" language model \"label\" label classification. discard_amb_nuc Whether discard samples contain characters outside vocabulary. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_start_ind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes start position of samples — get_start_ind","text":"numeric vector.","code":""},{"path":"https://genomenet.github.io/deepG/reference/get_start_ind.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes start position of samples — get_start_ind","text":"","code":"seq_vector <- c(\"AAACCCNNNGGGTTT\") get_start_ind(   seq_vector = seq_vector,   length_vector = nchar(seq_vector),   maxlen = 4,   step = 2,   train_mode = \"label\",   discard_amb_nuc = TRUE,   vocabulary = c(\"A\", \"C\", \"G\", \"T\")) #> [1]  1  3 10 12"},{"path":"https://genomenet.github.io/deepG/reference/heatmaps_integrated_grad.html","id":null,"dir":"Reference","previous_headings":"","what":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","title":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","text":"Creates heatmap output integrated_gradients function. first row contains column-wise absolute sums IG scores second row sums. Rows 3 6 contain IG scores position nucleotide. last row contains nucleotide information.","code":""},{"path":"https://genomenet.github.io/deepG/reference/heatmaps_integrated_grad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","text":"","code":"heatmaps_integrated_grad(integrated_grads, input_seq)"},{"path":"https://genomenet.github.io/deepG/reference/heatmaps_integrated_grad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","text":"integrated_grads Matrix integrated gradient scores (output integrated_gradients function). input_seq Input sequence model. input_seq input corresponding integrated_gradients call computed input integrated_grads argument.","code":""},{"path":"https://genomenet.github.io/deepG/reference/heatmaps_integrated_grad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","text":"list heatmaps.","code":""},{"path":"https://genomenet.github.io/deepG/reference/heatmaps_integrated_grad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Heatmap of integrated gradient scores — heatmaps_integrated_grad","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") && requireNamespace(\"ComplexHeatmap\", quietly = TRUE) library(reticulate) model <- create_model_lstm_cnn(layer_lstm = 8, layer_dense = 3, maxlen = 20, verbose = FALSE) random_seq <- sample(0:3, 20, replace = TRUE) input_seq <- array(keras::to_categorical(random_seq), dim = c(1, 20, 4)) ig <- integrated_gradients(   input_seq = input_seq,   target_class_idx = 3,   model = model) heatmaps_integrated_grad(integrated_grads = ig,                          input_seq = input_seq)   }"},{"path":"https://genomenet.github.io/deepG/reference/int_to_n_gram.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","title":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","text":"Input sequence integers vocabulary size voc_size. Returns vector integers corresponding n-gram encoding. Integers greater voc_size get encoded voc_size^n + 1.","code":""},{"path":"https://genomenet.github.io/deepG/reference/int_to_n_gram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","text":"","code":"int_to_n_gram(int_seq, n, voc_size = 4)"},{"path":"https://genomenet.github.io/deepG/reference/int_to_n_gram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","text":"int_seq Integer sequence n Length n-gram aggregation voc_size Size vocabulary.","code":""},{"path":"https://genomenet.github.io/deepG/reference/int_to_n_gram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","text":"numeric vector.","code":""},{"path":"https://genomenet.github.io/deepG/reference/int_to_n_gram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode sequence of integers to sequence of n-gram — int_to_n_gram","text":"","code":"int_to_n_gram(int_seq = c(1,1,2,4,4), n = 2, voc_size = 4) #> [1]  1  2  8 16"},{"path":"https://genomenet.github.io/deepG/reference/integrated_gradients.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute integrated gradients — integrated_gradients","title":"Compute integrated gradients — integrated_gradients","text":"Computes integrated gradients scores model input sequence. can used visualize part input import models decision. Code R implementation python code . Tensorflow implementation based paper.","code":""},{"path":"https://genomenet.github.io/deepG/reference/integrated_gradients.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute integrated gradients — integrated_gradients","text":"","code":"integrated_gradients(   m_steps = 50,   baseline_type = \"zero\",   input_seq,   target_class_idx,   model,   pred_stepwise = FALSE,   num_baseline_repeats = 1 )"},{"path":"https://genomenet.github.io/deepG/reference/integrated_gradients.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute integrated gradients — integrated_gradients","text":"m_steps Number steps baseline original input. baseline_type Baseline sequence, either \"zero\" zeros \"shuffle\" random permutation input_seq. input_seq Input tensor. target_class_idx Index class compute gradient model Model compute gradient . pred_stepwise Whether predictions batch size 1 rather . Can used input big handle . supported single input layer. num_baseline_repeats Number different baseline estimations baseline_type \"shuffle\" (estimate integrated gradient repeatedly different shuffles). Final result average num_baseline single calculations.","code":""},{"path":"https://genomenet.github.io/deepG/reference/integrated_gradients.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute integrated gradients — integrated_gradients","text":"tensorflow tensor.","code":""},{"path":"https://genomenet.github.io/deepG/reference/integrated_gradients.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute integrated gradients — integrated_gradients","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") library(reticulate) model <- create_model_lstm_cnn(layer_lstm = 8, layer_dense = 3, maxlen = 20, verbose = FALSE) random_seq <- sample(0:3, 20, replace = TRUE) input_seq <- array(keras::to_categorical(random_seq), dim = c(1, 20, 4)) integrated_gradients(   input_seq = input_seq,   target_class_idx = 3,   model = model)    }"},{"path":"https://genomenet.github.io/deepG/reference/layer_aggregate_time_dist_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregation layer — layer_aggregate_time_dist_wrapper","title":"Aggregation layer — layer_aggregate_time_dist_wrapper","text":"Aggregate output time distribution representations using sum, max /mean function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_aggregate_time_dist_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregation layer — layer_aggregate_time_dist_wrapper","text":"","code":"layer_aggregate_time_dist_wrapper(   load_r6 = FALSE,   method = \"sum\",   multi_in = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/layer_aggregate_time_dist_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregation layer — layer_aggregate_time_dist_wrapper","text":"load_r6 Whether load R6 layer class. method least one options, \"sum\", \"max\" \"mean\". multi_in Whether aggregate model multiple inputs (shared weights).","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_aggregate_time_dist_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aggregation layer — layer_aggregate_time_dist_wrapper","text":"keras layer applying pooling operation(s).","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_aggregate_time_dist_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Aggregation layer — layer_aggregate_time_dist_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") l <- layer_aggregate_time_dist_wrapper()  }"},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_embedding_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer for positional embedding — layer_pos_embedding_wrapper","title":"Layer for positional embedding — layer_pos_embedding_wrapper","text":"Positional encoding layer learned embedding.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_embedding_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer for positional embedding — layer_pos_embedding_wrapper","text":"","code":"layer_pos_embedding_wrapper(   maxlen = 100,   vocabulary_size = 4,   load_r6 = FALSE,   embed_dim = 64 )"},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_embedding_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer for positional embedding — layer_pos_embedding_wrapper","text":"maxlen Length predictor sequence. vocabulary_size Number unique character vocabulary. load_r6 Whether load R6 layer class. embed_dim Dimension token embedding. embedding set 0. used input one-hot encoded (integer sequence).","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_embedding_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Layer for positional embedding — layer_pos_embedding_wrapper","text":"keras layer implementing positional embedding.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_embedding_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Layer for positional embedding — layer_pos_embedding_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") l <- layer_pos_embedding_wrapper() }"},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_sinusoid_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer for positional encoding — layer_pos_sinusoid_wrapper","title":"Layer for positional encoding — layer_pos_sinusoid_wrapper","text":"Positional encoding layer sine/cosine matrix different frequencies.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_sinusoid_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer for positional encoding — layer_pos_sinusoid_wrapper","text":"","code":"layer_pos_sinusoid_wrapper(   maxlen = 100,   vocabulary_size = 4,   n = 10000,   load_r6 = FALSE,   embed_dim = 64 )"},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_sinusoid_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer for positional encoding — layer_pos_sinusoid_wrapper","text":"maxlen Length predictor sequence. vocabulary_size Number unique character vocabulary. n Frequency sine waves positional encoding. applied pos_encoding = \"sinusoid\". load_r6 Whether load R6 layer class. embed_dim Dimension token embedding. embedding set 0. used input one-hot encoded (integer sequence).","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_sinusoid_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Layer for positional encoding — layer_pos_sinusoid_wrapper","text":"keras layer implementing positional encoding using sine/cosine waves.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_pos_sinusoid_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Layer for positional encoding — layer_pos_sinusoid_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") l <- layer_pos_sinusoid_wrapper()  }"},{"path":"https://genomenet.github.io/deepG/reference/layer_transformer_block_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Transformer block — layer_transformer_block_wrapper","title":"Transformer block — layer_transformer_block_wrapper","text":"Create transformer block. Consists self attention, dense layers, layer normalization, recurrent connection dropout.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_transformer_block_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transformer block — layer_transformer_block_wrapper","text":"","code":"layer_transformer_block_wrapper(   num_heads = 2,   head_size = 4,   dropout_rate = 0,   ff_dim = 64,   vocabulary_size = 4,   load_r6 = FALSE,   embed_dim = 64 )"},{"path":"https://genomenet.github.io/deepG/reference/layer_transformer_block_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transformer block — layer_transformer_block_wrapper","text":"num_heads Number attention heads. head_size Dimensions attention key. dropout_rate Rate randomly drop connections. ff_dim Units first dense layer attention blocks. vocabulary_size Number unique character vocabulary. load_r6 Whether return layer class. embed_dim Dimension token embedding. embedding set 0. used input one-hot encoded (integer sequence).","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_transformer_block_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transformer block — layer_transformer_block_wrapper","text":"keras layer implementing transformer block.","code":""},{"path":"https://genomenet.github.io/deepG/reference/layer_transformer_block_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transformer block — layer_transformer_block_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") l <- layer_transformer_block_wrapper() }"},{"path":"https://genomenet.github.io/deepG/reference/load_cp.html","id":null,"dir":"Reference","previous_headings":"","what":"Load checkpoint — load_cp","title":"Load checkpoint — load_cp","text":"Load checkpoint directory. Chooses best checkpoint based condition. Condition can best accuracy, best loss, last epoch number specified epoch number.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_cp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load checkpoint — load_cp","text":"","code":"load_cp(   cp_path,   cp_filter = \"last_ep\",   ep_index = NULL,   compile = FALSE,   learning_rate = 0.01,   solver = \"adam\",   re_compile = FALSE,   loss = \"categorical_crossentropy\",   add_custom_object = NULL,   margin = 1,   verbose = TRUE,   mirrored_strategy = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/load_cp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load checkpoint — load_cp","text":"cp_path directory containing checkpoints single checkpoint file. directory, choose checkpoint based cp_filter ep_index. cp_filter Condition choose checkpoint cp_path directory. Either \"acc\" best validation accuracy, \"loss\" best validation loss \"last_ep\" last epoch. ep_index Load checkpoint specific epoch number. NULL, priority cp_filter. compile Whether load compiled model. learning_rate Learning rate optimizer. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". re_compile Whether compile model parameters learning_rate, solver loss. loss Loss function. used model gets compiled. add_custom_object Named list custom objects. margin Margin contrastive loss, see loss_cl. verbose Whether print chosen checkpoint path. mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_cp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load checkpoint — load_cp","text":"keras model loaded checkpoint.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_cp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load checkpoint — load_cp","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_lstm_cnn(layer_lstm = 8) checkpoint_folder <- tempfile() dir.create(checkpoint_folder) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.007-val_loss11.07-val_acc0.6.hdf5')) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.019-val_loss8.74-val_acc0.7.hdf5')) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.025-val_loss0.03-val_acc0.8.hdf5')) model <- load_cp(cp_path = checkpoint_folder, cp_filter = \"last_ep\") }"},{"path":"https://genomenet.github.io/deepG/reference/load_prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Read states from h5 file — load_prediction","title":"Read states from h5 file — load_prediction","text":"Reads h5 file created  predict_model function.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read states from h5 file — load_prediction","text":"","code":"load_prediction(   h5_path,   rows = NULL,   verbose = FALSE,   get_sample_position = FALSE,   get_seq = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/load_prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read states from h5 file — load_prediction","text":"h5_path Path h5 file. rows Range rows read. NULL read everything. verbose Boolean. get_sample_position Return position sample corresponding state TRUE. get_seq Return nucleotide sequence TRUE.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read states from h5 file — load_prediction","text":"list data frames, containing model predictions sequence positions.","code":""},{"path":"https://genomenet.github.io/deepG/reference/load_prediction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read states from h5 file — load_prediction","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # make prediction for single sequence and write to h5 file model <- create_model_lstm_cnn(maxlen = 20, layer_lstm = 8, layer_dense = 2, verbose = FALSE) vocabulary <- c(\"a\", \"c\", \"g\", \"t\") sequence <- paste(sample(vocabulary, 200, replace = TRUE), collapse = \"\") output_file <- tempfile(fileext = \".h5\") predict_model(output_format = \"one_seq\", model = model, step = 10,               sequence = sequence, filename = output_file, mode = \"label\") load_prediction(h5_path = output_file) }"},{"path":"https://genomenet.github.io/deepG/reference/loss_cl.html","id":null,"dir":"Reference","previous_headings":"","what":"Contrastive loss — loss_cl","title":"Contrastive loss — loss_cl","text":"Contrastive loss used : https://keras.io/examples/vision/siamese_contrastive/.","code":""},{"path":"https://genomenet.github.io/deepG/reference/loss_cl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contrastive loss — loss_cl","text":"","code":"loss_cl(margin = 1)"},{"path":"https://genomenet.github.io/deepG/reference/loss_cl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contrastive loss — loss_cl","text":"margin Integer, baseline distance pairs classified dissimilar.","code":""},{"path":"https://genomenet.github.io/deepG/reference/loss_cl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Contrastive loss — loss_cl","text":"function implementing contrastive loss.","code":""},{"path":"https://genomenet.github.io/deepG/reference/loss_cl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Contrastive loss — loss_cl","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") cl <- loss_cl(margin=1) }"},{"path":"https://genomenet.github.io/deepG/reference/merge_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge two models — merge_models","title":"Merge two models — merge_models","text":"Combine two models certain layers add dense layer(s) afterwards.","code":""},{"path":"https://genomenet.github.io/deepG/reference/merge_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge two models — merge_models","text":"","code":"merge_models(   models,   layer_names,   layer_dense,   solver = \"adam\",   learning_rate = 1e-04,   freeze_base_model = c(FALSE, FALSE),   model_seed = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/merge_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge two models — merge_models","text":"models List two models. layer_names Vector length 2 names layers merge. layer_dense Vector specifying number neurons per dense layer last LSTM CNN layer (LSTM used). solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". learning_rate Learning rate optimizer. freeze_base_model Boolean vector length 2. Whether freeze weights individual models. model_seed Set seed model parameters tensorflow NULL.","code":""},{"path":"https://genomenet.github.io/deepG/reference/merge_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge two models — merge_models","text":"keras model merging two input models.","code":""},{"path":"https://genomenet.github.io/deepG/reference/merge_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge two models — merge_models","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model_1 <- create_model_lstm_cnn(layer_lstm = c(64, 64), maxlen = 50, layer_dense = c(32, 4),                                  verbose = FALSE) model_2 <- create_model_lstm_cnn(layer_lstm = c(32), maxlen = 40,                                   layer_dense = c(8, 2), verbose = FALSE) # get names of second to last layers num_layers_1 <- length(model_1$get_config()$layers) layer_name_1 <- model_1$get_config()$layers[[num_layers_1 - 1]]$name num_layers_2 <- length(model_2$get_config()$layers) layer_name_2 <- model_2$get_config()$layers[[num_layers_2 - 1]]$name # merge models model <- merge_models(models = list(model_1, model_2),                       layer_names = c(layer_name_1, layer_name_2),                       layer_dense = c(6, 2),                        freeze_base_model = c(FALSE, FALSE))  }"},{"path":"https://genomenet.github.io/deepG/reference/model_card_cb.html","id":null,"dir":"Reference","previous_headings":"","what":"Create model card — model_card_cb","title":"Create model card — model_card_cb","text":"Log information model, hyperparameters, generator options, training data, scores etc","code":""},{"path":"https://genomenet.github.io/deepG/reference/model_card_cb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create model card — model_card_cb","text":"","code":"model_card_cb(model_card_path = NULL, run_name, argumentList)"},{"path":"https://genomenet.github.io/deepG/reference/model_card_cb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create model card — model_card_cb","text":"model_card_path Directory model card logs. run_name Name training run. argumentList List training arguments.","code":""},{"path":"https://genomenet.github.io/deepG/reference/model_card_cb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create model card — model_card_cb","text":"Keras callback writing model cards every epoch.","code":""},{"path":"https://genomenet.github.io/deepG/reference/model_card_cb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create model card — model_card_cb","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model_card_cb <- function(model_card_path = NULL, run_name, argumentList) mc <- model_card_cb(model_card_path = tempdir(), run_name = 'run_1',                     argumentList = list(learning_rate = 0.01))  }"},{"path":"https://genomenet.github.io/deepG/reference/n_gram_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Get distribution of n-grams — n_gram_dist","title":"Get distribution of n-grams — n_gram_dist","text":"Get distribution next character given previous n nucleotides.","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get distribution of n-grams — n_gram_dist","text":"","code":"n_gram_dist(   path_input,   n = 2,   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),   format = \"fasta\",   file_sample = NULL,   step = 1,   nuc_dist = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/n_gram_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get distribution of n-grams — n_gram_dist","text":"path_input Path folder containing fasta files single fasta file. n Size n gram. vocabulary Vector allowed characters, samples outside vocabulary get discarded. format File format, either \"fasta\" \"fastq\". file_sample integer, size random sample files path_input. step often take sample. nuc_dist Nucleotide distribution.","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get distribution of n-grams — n_gram_dist","text":"Returns matrix distributions nucleotides given previous n nucleotides. data frame n-gram predictions.","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get distribution of n-grams — n_gram_dist","text":"","code":"temp_dir <- tempfile() dir.create(temp_dir) create_dummy_data(file_path = temp_dir,                   num_files = 3,                   seq_length = 80,                   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                   num_seq = 2)  m <- n_gram_dist(path_input = temp_dir,                  n = 3,                  step = 1,                  nuc_dist = FALSE) head(round(m, 2)) #>        A    C    G    T #> AAA 0.25 0.50 0.12 0.12 #> CAA 0.73 0.00 0.00 0.27 #> GAA 0.29 0.29 0.29 0.14 #> TAA 0.40 0.40 0.20 0.00 #> ACA 0.71 0.14 0.14 0.00 #> CCA 0.00 0.29 0.29 0.43"},{"path":"https://genomenet.github.io/deepG/reference/n_gram_of_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","title":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","text":"One-hot encoding matrix n-gram encoding matrix","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_of_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","text":"","code":"n_gram_of_matrix(input_matrix, n = 3)"},{"path":"https://genomenet.github.io/deepG/reference/n_gram_of_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","text":"input_matrix Matrix one 1 per row zeros otherwise. n Length one n-gram.","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_of_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","text":"Matrix one-hot encodings.","code":""},{"path":"https://genomenet.github.io/deepG/reference/n_gram_of_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"One-hot encoding matrix to n-gram encoding matrix — n_gram_of_matrix","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") x <- c(0,0,1,3,3)  input_matrix <- keras::to_categorical(x, 4) n_gram_of_matrix(input_matrix, n = 2)  }"},{"path":"https://genomenet.github.io/deepG/reference/noisy_loss_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Loss function for label noise — noisy_loss_wrapper","title":"Loss function for label noise — noisy_loss_wrapper","text":"Implements approach paper code . Can used labeled data contains noise, .e. data labeled wrong.","code":""},{"path":"https://genomenet.github.io/deepG/reference/noisy_loss_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loss function for label noise — noisy_loss_wrapper","text":"","code":"noisy_loss_wrapper(noise_matrix)"},{"path":"https://genomenet.github.io/deepG/reference/noisy_loss_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loss function for label noise — noisy_loss_wrapper","text":"noise_matrix Matrix noise distribution.","code":""},{"path":"https://genomenet.github.io/deepG/reference/noisy_loss_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loss function for label noise — noisy_loss_wrapper","text":"function implementing noisy loss.","code":""},{"path":"https://genomenet.github.io/deepG/reference/noisy_loss_wrapper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Loss function for label noise — noisy_loss_wrapper","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # If first label contains 5% wrong labels and second label no noise noise_matrix <- matrix(c(0.95, 0.05, 0, 1), nrow = 2, byrow = TRUE) noisy_loss <- noisy_loss_wrapper(noise_matrix) }"},{"path":"https://genomenet.github.io/deepG/reference/one_hot_to_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","title":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","text":"Return character sequence corresponding one-hot elements matrix tensor.","code":""},{"path":"https://genomenet.github.io/deepG/reference/one_hot_to_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","text":"","code":"one_hot_to_seq(   m,   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),   amb_enc = \"zero\",   amb_char = \"N\",   paste_chars = TRUE )"},{"path":"https://genomenet.github.io/deepG/reference/one_hot_to_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","text":"m One-hot encoding matrix 3d array element first axis one-hot matrix. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. amb_enc Either \"zero\" \"equal\". oov tokens treated one-hot encoding. amb_char Char use oov positions. paste_chars Whether return vector single sequence.","code":""},{"path":"https://genomenet.github.io/deepG/reference/one_hot_to_seq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","text":"string.","code":""},{"path":"https://genomenet.github.io/deepG/reference/one_hot_to_seq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Char sequence corresponding to one-hot matrix. — one_hot_to_seq","text":"","code":"m <- matrix(c(1,0,0,0,0,1,0,0), 2) one_hot_to_seq(m) #> [1] \"AG\""},{"path":"https://genomenet.github.io/deepG/reference/parenthesis.html","id":null,"dir":"Reference","previous_headings":"","what":"Parenthesis data — parenthesis","title":"Parenthesis data — parenthesis","text":"Training dataset synthetic parenthesis language. Can loaded workspace via data(parenthesis).","code":""},{"path":"https://genomenet.github.io/deepG/reference/parenthesis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parenthesis data — parenthesis","text":"","code":"data(parenthesis)"},{"path":"https://genomenet.github.io/deepG/reference/parenthesis.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Parenthesis data — parenthesis","text":"Large character 1.00 MB","code":""},{"path":"https://genomenet.github.io/deepG/reference/parenthesis.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Parenthesis data — parenthesis","text":"https://github.com/philippmuench","code":""},{"path":"https://genomenet.github.io/deepG/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://genomenet.github.io/deepG/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://genomenet.github.io/deepG/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://genomenet.github.io/deepG/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot confusion matrix — plot_cm","title":"Plot confusion matrix — plot_cm","text":"Plot confusion matrix, either absolute numbers percentages per column (true labels).","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot confusion matrix — plot_cm","text":"","code":"plot_cm(   cm,   perc = FALSE,   cm_labels,   round_dig = 2,   text_size = 1,   highlight_diag = TRUE )"},{"path":"https://genomenet.github.io/deepG/reference/plot_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot confusion matrix — plot_cm","text":"cm confusion matrix perc Whether use absolute numbers percentages. cm_labels Labels corresponding confusion matrix entries. round_dig round numbers. text_size Size text annotations. highlight_diag Whether highlight entries diagonal.","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot confusion matrix — plot_cm","text":"ggplot confusion matrix.","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot confusion matrix — plot_cm","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") cm <- matrix(c(90, 1, 0, 2, 7, 1, 8, 3, 1), nrow = 3, byrow = TRUE) plot_cm(cm, perc = TRUE, cm_labels = paste0('label_', 1:3), text_size = 8) }"},{"path":"https://genomenet.github.io/deepG/reference/plot_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot ROC — plot_roc","title":"Plot ROC — plot_roc","text":"Compute ROC AUC target prediction matrix plot ROC. Target/prediction matrix one column output layer sigmoid activation two columns softmax activation.","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot ROC — plot_roc","text":"","code":"plot_roc(y_true, y_conf, path_roc_plot = NULL, return_plot = TRUE)"},{"path":"https://genomenet.github.io/deepG/reference/plot_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot ROC — plot_roc","text":"y_true Matrix true labels. y_conf Matrix predictions. path_roc_plot store ROC plot. return_plot Whether return plot.","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot ROC — plot_roc","text":"ggplot ROC curve.","code":""},{"path":"https://genomenet.github.io/deepG/reference/plot_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot ROC — plot_roc","text":"","code":"y_true <- matrix(c(1, 0, 0, 0, 1, 1), ncol = 1) y_conf <- matrix(runif(n = nrow(y_true)), ncol = 1) p <- plot_roc(y_true, y_conf, return_plot = TRUE) #> You may need to call library(ggplot2) if you want to add layers, etc. #> Loading required namespace: ggplot2 p"},{"path":"https://genomenet.github.io/deepG/reference/predict_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","title":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","text":"Removes layers (optional) pretrained model calculates states fasta/fastq file nucleotide sequence. Writes states h5 csv file (access content h5 output load_prediction function). several options process input file: \"one_seq\", computes prediction sequence argument fasta/fastq file. Combines fasta entries file one sequence. means predictor sequences can contain elements one fasta entry. \"by_entry\", output separate file fasta/fastq entry. Names output files : output_dir + \"Nr\" + + filename + output_type, number fasta entry. \"by_entry_one_file\", store prediction fasta entries one h5 file. \"one_pred_per_entry\", make one prediction entry either picking random sample long sequences pad sequence short sequences.","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","text":"","code":"predict_model(   model,   output_format = \"one_seq\",   layer_name = NULL,   sequence = NULL,   path_input = NULL,   round_digits = NULL,   filename = \"states.h5\",   step = 1,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   batch_size = 256,   verbose = TRUE,   return_states = FALSE,   output_type = \"h5\",   padding = \"none\",   use_quality = FALSE,   quality_string = NULL,   mode = \"label\",   lm_format = \"target_right\",   output_dir = NULL,   format = \"fasta\",   include_seq = FALSE,   reverse_complement_encoding = FALSE,   ambiguous_nuc = \"zero\",   ... )"},{"path":"https://genomenet.github.io/deepG/reference/predict_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","text":"model keras model. output_format Either \"one_seq\", \"by_entry\", \"by_entry_one_file\", \"one_pred_per_entry\". layer_name Name layer get output . NULL, use last layer. sequence Character string, ignores path_input argument given. path_input Path fasta file. round_digits Number decimal places. filename Filename store states . file output argument NULL. output_format = \"by_entry\", adds \"nr\" + \"\" name, entry number. step Frequency sampling steps. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. batch_size Number samples used one network update. verbose Boolean. return_states Return predictions data frame. supported output_format \"one_seq\". output_type \"h5\" \"csv\". output_format`` \"by_entries_one_file\", \"one_pred_per_entry\"can \"h5\"`. padding Either \"none\", \"maxlen\", \"standard\" \"self\". \"none\", apply padding skip sequences short. \"maxlen\", pad maxlen number zeros vectors. \"standard\", pad zero vectors sequence shorter maxlen. Pads minimum size required one prediction. \"self\", concatenate sequence sequence long enough one prediction. Example: sequence \"ACGT\" maxlen 10, make prediction \"ACGTACGTAC\". applied sequence shorter maxlen. use_quality Whether use quality scores. quality_string String encoding quality scores (used fastq format). mode Either \"lm\" language model \"label\" label classification. lm_format Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". output_dir Directory file output. format File format, \"fasta\", \"fastq\", \"rds\" \"fasta.tar.gz\", \"fastq.tar.gz\" tar.gz files. include_seq Whether include input sequence h5 file. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. ... arguments sequence encoding seq_encoding_label.","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","text":"return_states = TRUE returns list model predictions position corresponding sequences. additionally include_seq = TRUE, list contains sequence strings. return_states = FALSE returns nothing, just writes output file(s).","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make prediction for nucleotide sequence or entries in fasta/fastq file — predict_model","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # make prediction for single sequence and write to h5 file model <- create_model_lstm_cnn(maxlen = 20, layer_lstm = 8, layer_dense = 2, verbose = FALSE) vocabulary <- c(\"a\", \"c\", \"g\", \"t\") sequence <- paste(sample(vocabulary, 200, replace = TRUE), collapse = \"\") output_file <- tempfile(fileext = \".h5\") predict_model(output_format = \"one_seq\", model = model, step = 10,              sequence = sequence, filename = output_file, mode = \"label\")  # make prediction for fasta file with multiple entries, write output to separate h5 files fasta_path <- tempfile(fileext = \".fasta\") create_dummy_data(file_path = fasta_path, num_files = 1,                  num_seq = 5, seq_length = 100,                  write_to_file_path = TRUE) model <- create_model_lstm_cnn(maxlen = 20, layer_lstm = 8, layer_dense = 2, verbose = FALSE) output_dir <- tempfile() dir.create(output_dir) predict_model(output_format = \"by_entry\", model = model, step = 10, verbose = FALSE,                output_dir = output_dir, mode = \"label\", path_input = fasta_path) list.files(output_dir) }"},{"path":"https://genomenet.github.io/deepG/reference/predict_with_n_gram.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict the next nucleotide using n-gram — predict_with_n_gram","title":"Predict the next nucleotide using n-gram — predict_with_n_gram","text":"Predict next nucleotide using n-gram.","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_with_n_gram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict the next nucleotide using n-gram — predict_with_n_gram","text":"","code":"predict_with_n_gram(   path_input,   distribution_matrix,   default_pred = \"random\",   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),   file_sample = NULL,   format = \"fasta\",   return_data_frames = FALSE,   step = 1 )"},{"path":"https://genomenet.github.io/deepG/reference/predict_with_n_gram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict the next nucleotide using n-gram — predict_with_n_gram","text":"path_input Path folder containing fasta files single fasta file. distribution_matrix data frame containing frequency next nucleotide given previous n nucleotides (output n_gram_dist function). default_pred Either character vocabulary \"random\". used prediction certain n-gram appear . \"random\" assign random prediction. vocabulary Vector allowed characters, samples outside vocabulary get discarded. file_sample integer, size random sample files path_input. format File format, either \"fasta\" \"fastq\". return_data_frames Boolean, whether return data frame input, predictions, target position true target. step often take sample.","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_with_n_gram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict the next nucleotide using n-gram — predict_with_n_gram","text":"List prediction evaluations.","code":""},{"path":"https://genomenet.github.io/deepG/reference/predict_with_n_gram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict the next nucleotide using n-gram — predict_with_n_gram","text":"","code":"# create dummy fasta files temp_dir <- tempfile() dir.create(temp_dir) create_dummy_data(file_path = temp_dir,                   num_files = 3,                   seq_length = 8,                   vocabulary = c(\"A\", \"C\", \"G\", \"T\"),                   num_seq = 2)  m <- n_gram_dist(path_input = temp_dir,                  n = 3,                  step = 1,                  nuc_dist = FALSE)  # use distribution matrix to make predictions for one file predictions <- predict_with_n_gram(path_input = list.files(temp_dir, full.names = TRUE)[1],                                     distribution_matrix = m)  # show accuracy predictions[[1]] #> $accuracy #> [1] 1 #>"},{"path":"https://genomenet.github.io/deepG/reference/remove_add_layers.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove layers from model and add dense layers — remove_add_layers","title":"Remove layers from model and add dense layers — remove_add_layers","text":"Function takes model input removes layers certain layer, specified layer_name argument. Optional add dense layers top pruned model. Model can multiple output layers separate loss/activation functions. can freeze weights pruned model setting freeze_base_model = TRUE.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_add_layers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove layers from model and add dense layers — remove_add_layers","text":"","code":"remove_add_layers(   model = NULL,   layer_name = NULL,   dense_layers = NULL,   shared_dense_layers = NULL,   last_activation = list(\"softmax\"),   output_names = NULL,   losses = NULL,   verbose = TRUE,   dropout = NULL,   dropout_shared = NULL,   freeze_base_model = FALSE,   compile = FALSE,   learning_rate = 0.001,   solver = \"adam\",   flatten = FALSE,   global_pooling = NULL,   model_seed = NULL,   mixed_precision = FALSE,   mirrored_strategy = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/remove_add_layers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove layers from model and add dense layers — remove_add_layers","text":"model keras model. layer_name Name last layer use old model. dense_layers List vectors specifying number units dense layer. list length > 1, model multiple output layers. shared_dense_layers Vector number units dense layer. layers connected top layer argument layer_name. Can used shared dense layers, model multiple output layers. use model just one output layer (use dense_layers). last_activation List activations last entry list entry dense_layers. Either \"softmax\", \"sigmoid\" \"linear\". output_names List names output layer. losses List loss function output. verbose Boolean. dropout List vectors dropout rates new dense layer. dropout_shared Vectors dropout rates dense layer shared_dense_layers. freeze_base_model Whether freeze weights new dense layers. compile Boolean, whether compile new model. learning_rate Learning rate compile = TRUE, default learning rate old model. solver Optimization method, options \"adam\", \"adagrad\", \"rmsprop\" \"sgd\". flatten Whether add flatten layer new dense layers. global_pooling \"max_ch_first\" global max pooling channel first (keras docs), \"max_ch_last\" global max pooling channel last, \"average_ch_first\" global average pooling channel first, \"average_ch_last\" global average pooling channel last NULL global pooling. \"both_ch_first\" \"both_ch_last\" combine average max pooling. \"\" 4 options . model_seed Set seed model parameters tensorflow NULL. mixed_precision Whether use mixed precision (https://www.tensorflow.org/guide/mixed_precision). mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_add_layers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove layers from model and add dense layers — remove_add_layers","text":"keras model; added /removed layers base model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_add_layers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove layers from model and add dense layers — remove_add_layers","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model_1 <- create_model_lstm_cnn(layer_lstm = c(64, 64),                                  maxlen = 50,                                  layer_dense = c(32, 4),                                   verbose = FALSE) # get name of second to last layer  num_layers <- length(model_1$get_config()$layers) layer_name <- model_1$get_config()$layers[[num_layers-1]]$name # add dense layer with multi outputs and separate loss/activation functions model_2 <- remove_add_layers(model = model_1,                              layer_name = layer_name,                              dense_layers = list(c(32, 16, 1), c(8, 1), c(12, 5)),                              losses = list(\"binary_crossentropy\", \"mae\",                                            \"categorical_crossentropy\"),                              last_activation = list(\"sigmoid\", \"linear\", \"softmax\"),                              freeze_base_model = TRUE,                              output_names = list(\"out_1_binary_classsification\",                                                   \"out_2_regression\",                                                   \"out_3_classification\") )  }"},{"path":"https://genomenet.github.io/deepG/reference/remove_checkpoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove checkpoints — remove_checkpoints","title":"Remove checkpoints — remove_checkpoints","text":"Remove n 'best' checkpoints, based condition. Condition can accuracy, loss epoch number.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_checkpoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove checkpoints — remove_checkpoints","text":"","code":"remove_checkpoints(   cp_dir,   metric = \"acc\",   best_n = 1,   ask_before_remove = TRUE )"},{"path":"https://genomenet.github.io/deepG/reference/remove_checkpoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove checkpoints — remove_checkpoints","text":"cp_dir Directory containing checkpoints. metric Either \"acc\", \"loss\" \"last_ep\". Condition checkpoints keep. best_n Number checkpoints keep. ask_before_remove Whether show files keep deleting rest.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_checkpoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove checkpoints — remove_checkpoints","text":"None. Deletes certain files.","code":""},{"path":"https://genomenet.github.io/deepG/reference/remove_checkpoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove checkpoints — remove_checkpoints","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model <- create_model_lstm_cnn(layer_lstm = 8) checkpoint_folder <- tempfile() dir.create(checkpoint_folder) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.007-val_loss11.07-val_acc0.6.hdf5')) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.019-val_loss8.74-val_acc0.7.hdf5')) keras::save_model_hdf5(model, file.path(checkpoint_folder, 'Ep.025-val_loss0.03-val_acc0.8.hdf5')) remove_checkpoints(cp_dir = checkpoint_folder, metric = \"acc\", best_n = 2,                    ask_before_remove = FALSE) list.files(checkpoint_folder)   }"},{"path":"https://genomenet.github.io/deepG/reference/reset_states_cb.html","id":null,"dir":"Reference","previous_headings":"","what":"Reset states callback — reset_states_cb","title":"Reset states callback — reset_states_cb","text":"Reset states start/end validation whenever file changes. Can used stateful LSTM.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reset_states_cb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reset states callback — reset_states_cb","text":"","code":"reset_states_cb(path_file_log, path_file_logVal)"},{"path":"https://genomenet.github.io/deepG/reference/reset_states_cb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reset states callback — reset_states_cb","text":"path_file_log Path log training files. path_file_logVal Path log validation files.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reset_states_cb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reset states callback — reset_states_cb","text":"keras callback resets states LSTM layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reset_states_cb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reset states callback — reset_states_cb","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") rs <- reset_states_cb(path_file_log = tempfile(), path_file_logVal = tempfile()) }"},{"path":"https://genomenet.github.io/deepG/reference/reshape_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace input layer — reshape_input","title":"Replace input layer — reshape_input","text":"Replace first layer model new input layer different shape. works sequential models use CNN LSTM layers.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace input layer — reshape_input","text":"","code":"reshape_input(model, input_shape)"},{"path":"https://genomenet.github.io/deepG/reference/reshape_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace input layer — reshape_input","text":"model keras model. input_shape new input shape vector (without batch size).","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_input.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace input layer — reshape_input","text":"keras model changed input shape input model.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Replace input layer — reshape_input","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") model_1 <-  create_model_lstm_cnn(   maxlen = 50,   kernel_size = c(10, 10),   filters = c(64, 128),   pool_size = c(2, 2),   layer_lstm = c(32),   verbose = FALSE,   layer_dense = c(64, 2)) model <- reshape_input(model_1, input_shape = c(120, 4)) model }"},{"path":"https://genomenet.github.io/deepG/reference/reshape_tensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape tensors for set learning — reshape_tensor","title":"Reshape tensors for set learning — reshape_tensor","text":"Reshape input x target y. Aggregates multiple samples x y single input/target batches.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_tensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape tensors for set learning — reshape_tensor","text":"","code":"reshape_tensor(   x,   y,   new_batch_size,   samples_per_target,   buffer_len = NULL,   reshape_mode = \"time_dist\",   check_y = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/reshape_tensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape tensors for set learning — reshape_tensor","text":"x 3D input tensor. y 2D target tensor. new_batch_size Size first axis input/targets reshaping. samples_per_target many samples use one target buffer_len applies reshape_mode = \"concat\". buffer_len integer, subsequences interspaced buffer_len rows. reshaped x new maxlen: (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). reshape_mode \"time_dist\", \"multi_input\" \"concat\" \"multi_input\", produce samples_per_target separate inputs, length maxlen. \"time_dist\", produce 4D input array. dimensions correspond (new_batch_size, samples_per_target, maxlen, length(vocabulary)). \"concat\", concatenate samples_per_target sequences length maxlen one long sequence check_y Check entries y consistent reshape strategy (label aggregating).","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_tensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape tensors for set learning — reshape_tensor","text":"list 2 tensors.","code":""},{"path":"https://genomenet.github.io/deepG/reference/reshape_tensor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape tensors for set learning — reshape_tensor","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy data batch_size <- 8 maxlen <- 11 voc_len <- 4  x <- sample(0:(voc_len-1), maxlen*batch_size, replace = TRUE) x <- keras::to_categorical(x, num_classes = voc_len) x <- array(x, dim = c(batch_size, maxlen, voc_len)) y <- rep(0:1, each = batch_size/2) y <- keras::to_categorical(y, num_classes = 2) y  # reshape data for multi input model reshaped_data <- reshape_tensor(   x = x,   y = y,   new_batch_size = 2,   samples_per_target = 4,   reshape_mode = \"multi_input\")  length(reshaped_data[[1]]) dim(reshaped_data[[1]][[1]]) reshaped_data[[2]] }"},{"path":"https://genomenet.github.io/deepG/reference/resume_training_from_model_card.html","id":null,"dir":"Reference","previous_headings":"","what":"Continue training from model card — resume_training_from_model_card","title":"Continue training from model card — resume_training_from_model_card","text":"Use information model card resume corresponding checkpoint using training arguments.","code":""},{"path":"https://genomenet.github.io/deepG/reference/resume_training_from_model_card.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Continue training from model card — resume_training_from_model_card","text":"","code":"resume_training_from_model_card(   path_model_card,   seed = NULL,   epoch = NULL,   new_run_name = NULL,   new_args = NULL,   new_compile = NULL,   use_mirrored_strategy = NULL,   unfreeze = FALSE,   verbose = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/resume_training_from_model_card.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Continue training from model card — resume_training_from_model_card","text":"path_model_card Path model card resume training . seed Seed reproducible results. NULL, set random seed. epoch Epoch resume . NULL, use last epoch. new_run_name New run name. NULL, new run name old run name + '_cont'. new_args Named list arguments overwrite. use previous arguments model card otherwise. example, want change batch size padding option: new_args = list(batch_size = 6, padding = TRUE). new_compile List arguments compile model . NULL, use compiled model checkpoint. Example: new_compile = list(loss = 'binary_crossentropy', metrics = 'acc', optimizer = keras::optimizer_adam()) use_mirrored_strategy Whether use distributed mirrored strategy. NULL, use distributed mirrored strategy >1 GPU available. unfreeze TRUE, set trainable attribute model TRUE (unfreeze weights). verbose Whether print training arguments.","code":""},{"path":"https://genomenet.github.io/deepG/reference/resume_training_from_model_card.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Continue training from model card — resume_training_from_model_card","text":"list training metrics.","code":""},{"path":"https://genomenet.github.io/deepG/reference/resume_training_from_model_card.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Continue training from model card — resume_training_from_model_card","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy data and temp directories path_train_1 <- tempfile() path_train_2 <- tempfile() path_val_1 <- tempfile() path_val_2 <- tempfile() path_checkpoint <- tempfile() dir.create(path_checkpoint) path_model_card <- tempfile() dir.create(path_model_card)  for (current_path in c(path_train_1, path_train_2,                        path_val_1, path_val_2)) {   dir.create(current_path)   create_dummy_data(file_path = current_path,                     num_files = 3,                     seq_length = 10,                     num_seq = 5,                     vocabulary = c(\"a\", \"c\", \"g\", \"t\")) }  # create model model <- create_model_lstm_cnn(layer_lstm = 8, layer_dense = 2, maxlen = 5)  # train model run_name <- 'test_run_1' hist <- train_model(train_type = \"label_folder\",                     run_name = run_name,                     path_checkpoint = path_checkpoint,                     model_card = list(path_model_card = path_model_card, description = 'test run'),                     model = model,                     path = c(path_train_1, path_train_2),                     path_val = c(path_val_1, path_val_2),                     batch_size = 8,                     epochs = 3,                     steps_per_epoch = 6,                     vocabulary_label = c(\"label_1\", \"label_2\"))  # resume training resume_training_from_model_card(path_model_card = file.path(path_model_card, run_name)) }"},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_label.html","id":null,"dir":"Reference","previous_headings":"","what":"Encodes integer sequence for label classification. — seq_encoding_label","title":"Encodes integer sequence for label classification. — seq_encoding_label","text":"Returns encoding integer character sequence.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_label.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encodes integer sequence for label classification. — seq_encoding_label","text":"","code":"seq_encoding_label(   sequence = NULL,   maxlen,   vocabulary,   start_ind,   ambiguous_nuc = \"zero\",   nuc_dist = NULL,   quality_vector = NULL,   use_coverage = FALSE,   max_cov = NULL,   cov_vector = NULL,   n_gram = NULL,   n_gram_stride = 1,   masked_lm = NULL,   char_sequence = NULL,   tokenizer = NULL,   adjust_start_ind = FALSE,   return_int = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_label.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encodes integer sequence for label classification. — seq_encoding_label","text":"sequence Sequence integers. maxlen Length predictor sequence. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. start_ind Start positions samples sequence. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"empirical\" \"equal\". See train_model. Note \"discard\" option available function. nuc_dist Nucleotide distribution. quality_vector Vector quality probabilities. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. max_cov Biggest coverage value. applies use_coverage = TRUE. cov_vector Vector coverage values associated input. n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. char_sequence character string. tokenizer keras tokenizer. adjust_start_ind Whether shift values start_ind start 1: example (5,11,25) becomes (1,7,21). return_int Whether return integer encoding one-hot encoding.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_label.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encodes integer sequence for label classification. — seq_encoding_label","text":"list 2 tensors.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_label.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encodes integer sequence for label classification. — seq_encoding_label","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # use integer sequence as input x <- seq_encoding_label(sequence = c(1,0,5,1,3,4,3,1,4,1,2),                         maxlen = 5,                         vocabulary = c(\"a\", \"c\", \"g\", \"t\"),                         start_ind = c(1,3),                         ambiguous_nuc = \"equal\")  x[1,,] # 1,0,5,1,3  x[2,,] # 5,1,3,4,  # use character string as input x <- seq_encoding_label(maxlen = 5,                         vocabulary = c(\"a\", \"c\", \"g\", \"t\"),                         start_ind = c(1,3),                         ambiguous_nuc = \"equal\",                         char_sequence = \"ACTaaTNTNaZ\")  x[1,,] # actaa  x[2,,] # taatn }"},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_lm.html","id":null,"dir":"Reference","previous_headings":"","what":"Encodes integer sequence for language model — seq_encoding_lm","title":"Encodes integer sequence for language model — seq_encoding_lm","text":"Helper function generator_fasta_lm. Encodes integer sequence input/target list according output_format argument.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encodes integer sequence for language model — seq_encoding_lm","text":"","code":"seq_encoding_lm(   sequence = NULL,   maxlen,   vocabulary,   start_ind,   ambiguous_nuc = \"zero\",   nuc_dist = NULL,   quality_vector = NULL,   return_int = FALSE,   target_len = 1,   use_coverage = FALSE,   max_cov = NULL,   cov_vector = NULL,   n_gram = NULL,   n_gram_stride = 1,   output_format = \"target_right\",   char_sequence = NULL,   adjust_start_ind = FALSE,   tokenizer = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encodes integer sequence for language model — seq_encoding_lm","text":"sequence Sequence integers. maxlen Length predictor sequence. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. start_ind Start positions samples sequence. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"empirical\" \"equal\". See train_model. Note \"discard\" option available function. nuc_dist Nucleotide distribution. quality_vector Vector quality probabilities. return_int Whether return integer encoding one-hot encoding. target_len Number nucleotides predict language model. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. max_cov Biggest coverage value. applies use_coverage = TRUE. cov_vector Vector coverage values associated input. n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" char_sequence character string. adjust_start_ind Whether shift values start_ind start 1: example (5,11,25) becomes (1,7,21). tokenizer keras tokenizer.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encodes integer sequence for language model — seq_encoding_lm","text":"list 2 tensors.","code":""},{"path":"https://genomenet.github.io/deepG/reference/seq_encoding_lm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encodes integer sequence for language model — seq_encoding_lm","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # use integer sequence as input   z <- seq_encoding_lm(sequence = c(1,0,5,1,3,4,3,1,4,1,2), maxlen = 5, vocabulary = c(\"a\", \"c\", \"g\", \"t\"), start_ind = c(1,3), ambiguous_nuc = \"equal\", target_len = 1, output_format = \"target_right\")  x <- z[[1]] y <- z[[2]]  x[1,,] # 1,0,5,1,3 y[1,] # 4  x[2,,] # 5,1,3,4, y[2,] # 1  # use character string as input z <- seq_encoding_lm(sequence = NULL, maxlen = 5, vocabulary = c(\"a\", \"c\", \"g\", \"t\"), start_ind = c(1,3), ambiguous_nuc = \"zero\", target_len = 1, output_format = \"target_right\", char_sequence = \"ACTaaTNTNaZ\")   x <- z[[1]] y <- z[[2]]  x[1,,] # actaa y[1,] # t  x[2,,] # taatn y[2,] # t }"},{"path":"https://genomenet.github.io/deepG/reference/sgdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Stochastic Gradient Descent with Warm Restarts — sgdr","title":"Stochastic Gradient Descent with Warm Restarts — sgdr","text":"Compute learning Rate given epoch using Stochastic Gradient Descent Warm Restarts. Implements approach paper.","code":""},{"path":"https://genomenet.github.io/deepG/reference/sgdr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stochastic Gradient Descent with Warm Restarts — sgdr","text":"","code":"sgdr(lrmin = 5e-10, lrmax = 0.05, restart = 50, mult = 1, epoch = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/sgdr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stochastic Gradient Descent with Warm Restarts — sgdr","text":"lrmin Lower limit range learning rate. lrmax Upper limit range learning rate. restart Number epochs restart conducted. mult Factor, number epochs restart increased every restart. epoch Epoch, learning rate shall calculated.","code":""},{"path":"https://genomenet.github.io/deepG/reference/sgdr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stochastic Gradient Descent with Warm Restarts — sgdr","text":"numeric value.","code":""},{"path":"https://genomenet.github.io/deepG/reference/sgdr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stochastic Gradient Descent with Warm Restarts — sgdr","text":"","code":"sgdr(lrmin = 5e-10, lrmax = 5e-2, restart = 50, mult = 1, epoch = 5) #> [1] 0.04877641"},{"path":"https://genomenet.github.io/deepG/reference/split_fasta.html","id":null,"dir":"Reference","previous_headings":"","what":"Split fasta file into smaller files. — split_fasta","title":"Split fasta file into smaller files. — split_fasta","text":"Returns smaller files file name \"_x\" (x integer). example, assume input file called \"abc.fasta\" 100 entries split_n = 50. Function create two files called \"abc_1.fasta\" \"abc_2.fasta\" target_path.","code":""},{"path":"https://genomenet.github.io/deepG/reference/split_fasta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split fasta file into smaller files. — split_fasta","text":"","code":"split_fasta(   path_input,   target_folder,   split_n = 500,   shuffle_entries = TRUE,   delete_input = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/split_fasta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split fasta file into smaller files. — split_fasta","text":"path_input Fasta file split smaller files target_folder Directory output. split_n Maximum number entries use smaller file. shuffle_entries Whether shuffle fasta entries split. delete_input Whether delete original file.","code":""},{"path":"https://genomenet.github.io/deepG/reference/split_fasta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split fasta file into smaller files. — split_fasta","text":"None. Writes files output.","code":""},{"path":"https://genomenet.github.io/deepG/reference/split_fasta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split fasta file into smaller files. — split_fasta","text":"","code":"path_input <- tempfile(fileext = '.fasta') create_dummy_data(file_path = path_input,                   num_files = 1,                   write_to_file_path = TRUE,                   seq_length = 7,                   num_seq = 25,                   vocabulary = c(\"a\", \"c\", \"g\", \"t\")) target_folder <- tempfile() dir.create(target_folder)  # split 25 entries into 5 files split_fasta(path_input = path_input,             target_folder = target_folder,             split_n = 5) length(list.files(target_folder))  #> [1] 5"},{"path":"https://genomenet.github.io/deepG/reference/stepdecay.html","id":null,"dir":"Reference","previous_headings":"","what":"Step Decay — stepdecay","title":"Step Decay — stepdecay","text":"Compute learning Rate given epoch using Step Decay.","code":""},{"path":"https://genomenet.github.io/deepG/reference/stepdecay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Step Decay — stepdecay","text":"","code":"stepdecay(lrmax = 0.005, newstep = 50, mult = 0.7, epoch = NULL)"},{"path":"https://genomenet.github.io/deepG/reference/stepdecay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Step Decay — stepdecay","text":"lrmax Upper limit range learning rate. newstep Number epochs learning rate reduced. mult Factor, number epochs restart decreased new step. epoch Epoch, learning rate shall calculated.","code":""},{"path":"https://genomenet.github.io/deepG/reference/stepdecay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Step Decay — stepdecay","text":"numeric value.","code":""},{"path":"https://genomenet.github.io/deepG/reference/stepdecay.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Step Decay — stepdecay","text":"","code":"stepdecay(lrmax = 0.005, newstep = 50, mult = 0.7, epoch = 3) #> [1] 0.005"},{"path":"https://genomenet.github.io/deepG/reference/summarize_states.html","id":null,"dir":"Reference","previous_headings":"","what":"Create summary of predictions — summarize_states","title":"Create summary of predictions — summarize_states","text":"Create summary data frame confidence predictions 1 several state files data frame. Columns file data frame confidence predictions one class, .e. rows sum 1 nonnegative entries. Output data frame contains average confidence scores, max score percentage votes class.","code":""},{"path":"https://genomenet.github.io/deepG/reference/summarize_states.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create summary of predictions — summarize_states","text":"","code":"summarize_states(   states_path = NULL,   label_names = NULL,   file_type = \"h5\",   df = NULL )"},{"path":"https://genomenet.github.io/deepG/reference/summarize_states.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create summary of predictions — summarize_states","text":"states_path Folder containing state files single file ending file_type. label_names Names predicted classes. file_type \"h5\" \"csv\". df states data frame. Ignore states_dir argument NULL.","code":""},{"path":"https://genomenet.github.io/deepG/reference/summarize_states.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create summary of predictions — summarize_states","text":"data frame predictions summaries.","code":""},{"path":"https://genomenet.github.io/deepG/reference/summarize_states.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create summary of predictions — summarize_states","text":"","code":"m <- c(0.9,  0.1, 0.2, 0.01,        0.05, 0.7, 0.2, 0,        0.05, 0.2, 0.6, 0.99) %>% matrix(ncol = 3)  label_names <- paste0(\"class_\", 1:3) df <- as.data.frame(m) pred_summary <- summarize_states(label_names = label_names, df = df) pred_summary #>    file_name mean_conf_class_1 mean_conf_class_2 mean_conf_class_3 #>       <lgcl>             <num>             <num>             <num> #> 1:        NA            0.3025            0.2375              0.46 #>    max_conf_class_1 max_conf_class_2 max_conf_class_3 vote_perc_class_1 #>               <num>            <num>            <num>             <num> #> 1:              0.9              0.7             0.99              0.25 #>    vote_perc_class_2 vote_perc_class_3 mean_prediction max_prediction #>                <num>             <num>          <char>         <char> #> 1:              0.25               0.5         class_3        class_3 #>    vote_prediction num_prediction #>             <char>          <int> #> 1:         class_3              4"},{"path":"https://genomenet.github.io/deepG/reference/train_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Train neural network on genomic data — train_model","title":"Train neural network on genomic data — train_model","text":"Train neural network genomic data. Data can fasta/fastq files, rds files prepared data set. data given collection fasta, fastq rds files, function create data generator extracts training validation batches files. Function includes several options determine sampling strategy generator preprocessing data. Training progress can visualized tensorboard. Model weights can stored training using checkpoints.","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train neural network on genomic data — train_model","text":"","code":"train_model(   model = NULL,   dataset = NULL,   dataset_val = NULL,   train_val_ratio = 0.2,   run_name = \"run_1\",   initial_epoch = 0,   class_weight = NULL,   print_scores = TRUE,   epochs = 10,   max_queue_size = 100,   steps_per_epoch = 1000,   path_checkpoint = NULL,   path_tensorboard = NULL,   path_log = NULL,   save_best_only = NULL,   save_weights_only = FALSE,   tb_images = FALSE,   path_file_log = NULL,   reset_states = FALSE,   early_stopping_time = NULL,   validation_only_after_training = FALSE,   train_val_split_csv = NULL,   reduce_lr_on_plateau = TRUE,   lr_plateau_factor = 0.9,   patience = 20,   cooldown = 1,   model_card = NULL,   callback_list = NULL,   train_type = \"label_folder\",   path = NULL,   path_val = NULL,   batch_size = 64,   step = NULL,   shuffle_file_order = TRUE,   vocabulary = c(\"a\", \"c\", \"g\", \"t\"),   format = \"fasta\",   ambiguous_nuc = \"zero\",   seed = c(1234, 4321),   file_limit = NULL,   use_coverage = NULL,   set_learning = NULL,   proportion_entries = NULL,   sample_by_file_size = FALSE,   n_gram = NULL,   n_gram_stride = 1,   masked_lm = NULL,   random_sampling = FALSE,   add_noise = NULL,   return_int = FALSE,   maxlen = NULL,   reverse_complement = FALSE,   reverse_complement_encoding = FALSE,   output_format = \"target_right\",   proportion_per_seq = NULL,   read_data = FALSE,   use_quality_score = FALSE,   padding = FALSE,   concat_seq = NULL,   target_len = 1,   skip_amb_nuc = NULL,   max_samples = NULL,   added_label_path = NULL,   add_input_as_seq = NULL,   target_from_csv = NULL,   target_split = NULL,   shuffle_input = TRUE,   vocabulary_label = NULL,   delete_used_files = FALSE,   reshape_xy = NULL,   return_gen = FALSE )"},{"path":"https://genomenet.github.io/deepG/reference/train_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train neural network on genomic data — train_model","text":"model keras model. dataset List training data holding training samples RAM instead using generator. list two entries called \"X\" \"Y\". dataset_val List validation data. two entries called \"X\" \"Y\". train_val_ratio generator defines fraction batches used validation (compared size training data), .e. one validation iteration processes batch_size \\(*\\) steps_per_epoch \\(*\\) train_val_ratio samples. use dataset instead generator dataset_val NULL, splits dataset train/validation data. run_name Name run. Name used identify output callbacks. NULL, use date run name. name already present, add \"_2\" name \"_{x+1}\" name ends _x, x integer. initial_epoch Epoch start training. Note network run (epochs - initial_epochs) rounds epochs rounds. class_weight List weights output. Order correspond vocabulary_label. can use get_class_weight function estimate class weights: class_weights <- get_class_weights(path = path, train_type = train_type) train_type = \"label_csv\" need add path csv file: class_weights <- get_class_weights(path = path, train_type = train_type, csv_path = target_from_csv) print_scores Whether print train/validation scores training. epochs Number iterations. max_queue_size Maximum size generator queue. steps_per_epoch Number training batches per epoch. path_checkpoint Path checkpoints folder NULL. NULL, checkpoints get stored. path_tensorboard Path tensorboard directory NULL. NULL, training tracked tensorboard. path_log Path directory write training scores. File name run_name + \".csv\". output NULL. save_best_only save model improved score. applied argument NULL. Otherwise must list argument monitor save_freq (can use one option). moniter specifies metric use. save_freq, integer specifying often store checkpoint (epochs). save_weights_only Whether save weights . tb_images Whether show custom images (confusion matrix) tensorboard \"IMAGES\" tab. path_file_log Write name files used training csv file path specified. reset_states Whether reset hidden states RNN layer every new input file /validation. early_stopping_time Time seconds stop training. validation_only_after_training Whether skip validation training one validation iteration training. train_val_split_csv csv file specifying train/validation split. csv file contain one column named \"file\" one column named \"type\". \"file\" column contains names fasta/fastq files \"type\" column specifies file used training validation. Entries \"type\" must named \"train\" \"val\", otherwise file used either. path path_val arguments . implemented train_type = \"label_folder\". reduce_lr_on_plateau Whether use learning rate scheduler. lr_plateau_factor Factor decreasing learning rate plateau reached. patience Number epochs waiting decrease validation loss reducing learning rate. cooldown Number epochs without changing learning rate. model_card List arguments training parameters training run. Must contain least entry path_model_card, .e. directory parameters stored. List can contain additional (optional) arguments, example model_card = list(path_model_card = \"/path//logs\", description = \"transfer learning BERT model virus data\", ...) callback_list Add additional callbacks keras::fit call. train_type Either \"lm\", \"lm_rds\", \"masked_lm\" language model; \"label_header\", \"label_folder\", \"label_csv\", \"label_rds\" classification \"dummy_gen\". Language model trained predict character(s) sequence. \"label_header\"/\"label_folder\"/\"label_csv\" trained predict corresponding class given sequence input. \"label_header\", class read fasta headers. \"label_folder\", class read folder, .e. files one folder must belong class. \"label_csv\", targets read csv file. file one column named \"file\". targets correspond entries row (except \"file\" column). Example: currently working file called \".fasta\" corresponding label \"label_1\", row csv file \"label_rds\", generator iterate set .rds files containing list input target tensors. implemented model multiple inputs. \"lm_rds\", generator iterate set .rds files split tensor according target_len argument (targets last target_len nucleotides sequence). \"dummy_gen\", generator creates random data repeatedly feeds model. \"masked_lm\", generator maskes parts input. See masked_lm argument details. path Path training data. train_type label_folder, vector list entry corresponds class (list elements can directories /individual files). train_type label_folder, can single directory file list directories /files. path_val Path validation data. See path argument details. batch_size Number samples used one network update. step Frequency sampling steps. shuffle_file_order Boolean, whether go files sequentially shuffle beforehand. vocabulary Vector allowed characters. Characters outside vocabulary get encoded specified ambiguous_nuc. format File format, \"fasta\", \"fastq\", \"rds\" \"fasta.tar.gz\", \"fastq.tar.gz\" tar.gz files. ambiguous_nuc handle nucleotides outside vocabulary, either \"zero\", \"discard\", \"empirical\" \"equal\". \"zero\", input gets encoded zero vector. \"equal\", input repetition 1/length(vocabulary). \"discard\", samples containing nucleotides outside vocabulary get discarded. \"empirical\", use nucleotide distribution current file. seed Sets seed reproducible results. file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. use_coverage Integer NULL. NULL, use coverage encoding rather one-hot encoding normalize. Coverage information must contained fasta header: must string \"cov_n\" header, n integer. set_learning want assign one label set samples. implemented train_type = \"label_folder\". Input list following parameters samples_per_target: many samples use one target. maxlen: length one sample. reshape_mode: \"time_dist\", \"multi_input\" \"concat\". reshape_mode \"multi_input\", generator produce samples_per_target separate inputs, length maxlen (model samples_per_target input layers). reshape_mode \"time_dist\", generator produce 4D input array. dimensions correspond (batch_size, samples_per_target, maxlen, length(vocabulary)). reshape_mode \"concat\", generator concatenate samples_per_target sequences length maxlen one long sequence. reshape_mode \"concat\", additional buffer_len argument. buffer_len integer, subsequences interspaced buffer_len rows. input length (maxlen \\(*\\) samples_per_target) + buffer_len \\(*\\) (samples_per_target - 1). proportion_entries Proportion fasta entries keep. example, fasta file 50 entries proportion_entries = 0.1, randomly select 5 entries. sample_by_file_size Sample new file weighted file size (bigger files likely). n_gram Integer, encode target nucleotide wise combine n nucleotides . example n=2, \"AA\" ->  (1, 0,..., 0), \"AC\" ->  (0, 1, 0,..., 0), \"TT\" -> (0,..., 0, 1), one-hot vectors length length(vocabulary)^n. n_gram_stride Step size n-gram encoding. AACCGGTT n_gram = 4 n_gram_stride = 2, generator encodes (AACC), (CCGG), (GGTT); n_gram_stride = 4 generator encodes (AACC), (GGTT). masked_lm NULL, input target equal except parts input masked random. Must list following arguments: mask_rate: Rate input mask (rate input replace mask token). random_rate: Rate input set random token. identity_rate: Rate input sample weights applied input output identical. include_sw: Whether include sample weights. block_len (optional): Masked/random/identity regions appear blocks size block_len. random_sampling Whether samples taken random positions using max_samples argument. FALSE random samples taken consecutive subsequence. add_noise NULL list arguments. NULL, list must contain following arguments: noise_type can \"normal\" \"uniform\"; optional arguments sd mean noise_type \"normal\" (default sd=1 mean=0) min, max noise_type \"uniform\" (default min=0, max=1). return_int Whether return integer encoding one-hot encoding. maxlen Length predictor sequence. reverse_complement Boolean, every new file decide randomly use original data reverse complement. reverse_complement_encoding Whether use original sequence reverse complement two input sequences. output_format Determines shape output tensor language model. Either \"target_right\", \"target_middle_lstm\", \"target_middle_cnn\" \"wavenet\". Assume sequence \"AACCGTA\". Output correspond follows \"target_right\": X = \"AACCGT\", Y = \"\" \"target_middle_lstm\": X = (X_1 = \"AAC\", X_2 = \"ATG\"), Y = \"C\" (note reversed order X_2) \"target_middle_cnn\": X = \"AACGTA\", Y = \"C\" \"wavenet\": X = \"AACCGT\", Y = \"ACCGTA\" proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). read_data TRUE first element output list length 2, containing one part paired read. Maxlen 2*length one read. use_quality_score Whether use fastq quality scores. TRUE input one-hot-encoding corresponds probabilities. example (0.97, 0.01, 0.01, 0.01) instead (1, 0, 0, 0). padding Whether pad sequences short one sample zeros. concat_seq Character string NULL. NULL entries file get concatenated one sequence concat_seq string . Example: 1.entry AACC, 2. entry TTTG concat_seq = \"ZZZ\" becomes AACCZZZTTTG. target_len Number nucleotides predict language model. skip_amb_nuc Threshold ambiguous nucleotides accept fasta entry. Complete entry get discarded otherwise. max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. added_label_path Path file additional input labels. csv file one column named \"file\". columns correspond labels. add_input_as_seq Boolean vector specifying entry added_label_path rows csv encoded sequence used directly. row csv file sequence TRUE. example may want add another sequence, say ACCGT. correspond 1,2,2,3,4 csv file (vocabulary = c(\"\", \"C\", \"G\", \"T\")).  add_input_as_seq TRUE, 12234 gets one-hot encoded, added input 3D tensor.  add_input_as_seq FALSE feed network just raw data (2D tensor). target_from_csv Path csv file target mapping. One column called \"file\" entries row targets. target_split target gets read csv file, list names divide target tensor list tensors. Example: csv file header names \"file\", \"label_1\", \"label_2\", \"label_3\" target_split = list(c(\"label_1\", \"label_2\"), \"label_3\"), divide target matrix list length 2, first element contains columns named \"label_1\" \"label_2\" second entry contains column named \"label_3\". shuffle_input Whether shuffle entries file. vocabulary_label Character vector possible targets. Targets outside vocabulary_label get discarded train_type = \"label_header\". delete_used_files Whether delete file used. applies rds files. reshape_xy Can list functions apply input /target. List elements (containing reshape functions) must called x input y target arguments called x y. example: reshape_xy = list(x = function(x, y) {return(x+1)}, y = function(x, y) {return(x+y)}) . rds generator needs additional argument called sw. return_gen Whether return train validation generators (instead training).","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train neural network on genomic data — train_model","text":"list training metrics.","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train neural network on genomic data — train_model","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") # create dummy data path_train_1 <- tempfile() path_train_2 <- tempfile() path_val_1 <- tempfile() path_val_2 <- tempfile()  for (current_path in c(path_train_1, path_train_2,                        path_val_1, path_val_2)) {   dir.create(current_path)   create_dummy_data(file_path = current_path,                     num_files = 3,                     seq_length = 10,                     num_seq = 5,                     vocabulary = c(\"a\", \"c\", \"g\", \"t\")) }  # create model model <- create_model_lstm_cnn(layer_lstm = 8, layer_dense = 2, maxlen = 5)  # train model hist <- train_model(train_type = \"label_folder\",                     model = model,                     path = c(path_train_1, path_train_2),                     path_val = c(path_val_1, path_val_2),                     batch_size = 8,                     epochs = 3,                     steps_per_epoch = 6,                     step = 5,                     format = \"fasta\",                     vocabulary_label = c(\"label_1\", \"label_2\"))   }"},{"path":"https://genomenet.github.io/deepG/reference/train_model_cpc.html","id":null,"dir":"Reference","previous_headings":"","what":"Train CPC inspired model — train_model_cpc","title":"Train CPC inspired model — train_model_cpc","text":"Train CPC (Oord et al.) inspired neural network genomic data.","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model_cpc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train CPC inspired model — train_model_cpc","text":"","code":"train_model_cpc(   train_type = \"CPC\",   encoder = NULL,   context = NULL,   path,   path_val = NULL,   path_checkpoint = NULL,   path_tensorboard = NULL,   train_val_ratio = 0.2,   run_name,   batch_size = 32,   epochs = 100,   steps_per_epoch = 2000,   shuffle_file_order = FALSE,   initial_epoch = 1,   seed = 1234,   path_file_log = TRUE,   train_val_split_csv = NULL,   file_limit = NULL,   proportion_per_seq = NULL,   max_samples = NULL,   maxlen = NULL,   patchlen = NULL,   nopatches = NULL,   step = NULL,   file_filter = NULL,   stride = 0.4,   pretrained_model = NULL,   learningrate = 0.001,   learningrate_schedule = NULL,   k = 5,   stepsmin = 2,   stepsmax = 3,   emb_scale = 0.1 )"},{"path":"https://genomenet.github.io/deepG/reference/train_model_cpc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train CPC inspired model — train_model_cpc","text":"train_type Either \"cpc\", \"Self-GenomeNet\". encoder keras encoder cpc function. context keras context model cpc function. path Path training data. train_type label_folder, vector list entry corresponds class (list elements can directories /individual files). train_type label_folder, can single directory file list directories /files. path_val Path validation data. See path argument details. path_checkpoint Path checkpoints folder NULL. NULL, checkpoints get stored. path_tensorboard Path tensorboard directory NULL. NULL, training tracked tensorboard. train_val_ratio generator defines fraction batches used validation (compared size training data), .e. one validation iteration processes batch_size \\(*\\) steps_per_epoch \\(*\\) train_val_ratio samples. use dataset instead generator dataset_val NULL, splits dataset train/validation data. run_name Name run. Name used identify output callbacks. batch_size Number samples used one network update. epochs Number iterations. steps_per_epoch Number training batches per epoch. shuffle_file_order Boolean, whether go files sequentially shuffle beforehand. initial_epoch Epoch start training. Note network run (epochs - initial_epochs) rounds epochs rounds. seed Sets seed reproducible results. path_file_log Write name files csv file path specified. train_val_split_csv csv file specifying train/validation split. csv file contain one column named \"file\" one column named \"type\". \"file\" column contains names fasta/fastq files \"type\" column specifies file used training validation. Entries \"type\" must named \"train\" \"val\", otherwise file used either. path path_val arguments . implemented train_type = \"label_folder\". file_limit Integer NULL. integer, use specified number randomly sampled files training. Ignored greater number files path. proportion_per_seq Numerical value 0 1. Proportion sequence take samples (use random subsequence). max_samples Maximum number samples use one file. NULL file max_samples samples, randomly choose subset max_samples samples. maxlen Length predictor sequence. patchlen length patch splitting input sequence. nopatches number patches splitting input sequence. step Frequency sampling steps. file_filter Vector file names use path_corpus. stride overlap two patches splitting input sequence. pretrained_model pretrained keras model, training continued learningrate Tensor, floating point value. schedule defines, value gives initial learning rate. Defaults 0.001. learningrate_schedule schedule non-constant learning rate training. Either \"cosine_annealing\", \"step_decay\", \"exp_decay\". k Value k sparse top k categorical accuracy. Defaults 5. stepsmin CPC, patch predicted given another patch. stepsmin defines many patches two ignored prediction. stepsmax maximum distance predicted patch given patch. emb_scale Scales impact patches context.","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model_cpc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train CPC inspired model — train_model_cpc","text":"list training metrics.","code":""},{"path":"https://genomenet.github.io/deepG/reference/train_model_cpc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train CPC inspired model — train_model_cpc","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\")  #create dummy data path_train_1 <- tempfile() path_train_2 <- tempfile() path_val_1 <- tempfile() path_val_2 <- tempfile()  for (current_path in c(path_train_1, path_train_2,                        path_val_1, path_val_2)) {   dir.create(current_path)   deepG::create_dummy_data(file_path = current_path,                            num_files = 3,                            seq_length = 10,                            num_seq = 5,                            vocabulary = c(\"a\", \"c\", \"g\", \"t\")) }  # create model encoder <- function(maxlen = NULL,                     patchlen = NULL,                     nopatches = NULL,                     eval = FALSE) {   if (is.null(nopatches)) {     nopatches <- nopatchescalc(patchlen, maxlen, patchlen * 0.4)   }   inp <- keras::layer_input(shape = c(maxlen, 4))   stridelen <- as.integer(0.4 * patchlen)   createpatches <- inp %>%     keras::layer_reshape(list(maxlen, 4L, 1L), name = \"prep_reshape1\", dtype = \"float32\") %>%     tensorflow::tf$image$extract_patches(       sizes = list(1L, patchlen, 4L, 1L),       strides = list(1L, stridelen, 4L, 1L),       rates = list(1L, 1L, 1L, 1L),       padding = \"VALID\",       name = \"prep_patches\"     ) %>%     keras::layer_reshape(list(nopatches, patchlen, 4L),                          name = \"prep_reshape2\") %>%     tensorflow::tf$reshape(list(-1L, patchlen, 4L),                            name = \"prep_reshape3\")    danQ <- createpatches %>%     keras::layer_conv_1d(       input_shape = c(maxlen, 4L),       filters = 320L,       kernel_size = 26L,       activation = \"relu\"     ) %>%     keras::layer_max_pooling_1d(pool_size = 13L, strides = 13L) %>%     keras::layer_dropout(0.2) %>%     keras::layer_lstm(units = 320, return_sequences = TRUE) %>%     keras::layer_dropout(0.5) %>%     keras::layer_flatten() %>%     keras::layer_dense(925, activation = \"relu\")   patchesback <- danQ %>%     tensorflow::tf$reshape(list(-1L, tensorflow::tf$cast(nopatches, tensorflow::tf$int16), 925L))   keras::keras_model(inp, patchesback) }  context <- function(latents) {   cres <- latents   cres_dim = cres$shape   predictions <-     cres %>%     keras::layer_lstm(       return_sequences = TRUE,       units = 256,  # WAS: 2048,       name = paste(\"context_LSTM_1\",                    sep = \"\"),       activation = \"relu\"     )   return(predictions) }  # train model temp_dir <- tempdir() hist <- train_model_cpc(train_type = \"CPC\",                         ### cpc functions ###                         encoder = encoder,                         context = context,                         #### Generator settings ####                         path_checkpoint = temp_dir,                         path = c(path_train_1, path_train_2),                         path_val = c(path_val_1, path_val_2),                         run_name = \"TEST\",                         batch_size = 8,                         epochs = 3,                         steps_per_epoch = 6,                         patchlen = 100,                         nopatches = 8)                    }"},{"path":"https://genomenet.github.io/deepG/reference/validation_after_training_cb.html","id":null,"dir":"Reference","previous_headings":"","what":"Validation after training callback — validation_after_training_cb","title":"Validation after training callback — validation_after_training_cb","text":"validation end training.","code":""},{"path":"https://genomenet.github.io/deepG/reference/validation_after_training_cb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validation after training callback — validation_after_training_cb","text":"","code":"validation_after_training_cb(gen.val, validation_steps)"},{"path":"https://genomenet.github.io/deepG/reference/validation_after_training_cb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validation after training callback — validation_after_training_cb","text":"gen.val Validation generator validation_steps Number validation steps.","code":""},{"path":"https://genomenet.github.io/deepG/reference/validation_after_training_cb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validation after training callback — validation_after_training_cb","text":"Keras callback, apply validation training.","code":""},{"path":"https://genomenet.github.io/deepG/reference/validation_after_training_cb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validation after training callback — validation_after_training_cb","text":"","code":"if (FALSE) { # reticulate::py_module_available(\"tensorflow\") maxlen <- 20 model <- create_model_lstm_cnn(layer_lstm = 8, maxlen = maxlen) gen <- get_generator(train_type = 'dummy_gen', model = model, batch_size = 4, maxlen = maxlen) vat <- validation_after_training_cb(gen.val = gen, validation_steps = 10) }"},{"path":"https://genomenet.github.io/deepG/news/index.html","id":"deepg-030","dir":"Changelog","previous_headings":"","what":"deepG 0.3.0","title":"deepG 0.3.0","text":"Initial CRAN submission.","code":""}]
