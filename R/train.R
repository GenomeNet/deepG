#' @title Trains a neural network on genomic data. Designed for developing genome based language models (GenomeNet)
#'
#' @description
#' Depth and number of neurons per layer of the netwok can be specified.
#' If a path to a folder where FASTA files are located is provided, batches will be generated using an external generator which
#' is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()})
#' and keeps them in RAM.
#' @inheritParams fastaFileGenerator
#' @inheritParams labelByFolderGenerator
#' @inheritParams fastaLabelGenerator
#' @param train_type Either "lm" for language model, "label_header", "label_folder" or "label_csv". Language model is trained to predict character in sequence.
#' "label_header"/"label_folder" are trained to predict a corresponding class, given a sequence as input. If "label_header", class will be read from fasta headers.
#' If "label_folder", class will be read from folder, i.e. all files in one folder must belong to the same class.
#' If "label_csv", targets are read from a csv file. This file should have one column names "file". The targets then correspond to entries in that row (except "file"
#' column). Example: if we are currently working with a file called "a.fasta", there should be a row in our csv file
#'    file  |  label_1 | label_2
#' "a.fasta"     1          8
#' @param model A keras model.
#' @param built_model Call to a function that creates a model. \code{create_model_function} can be either "create_model_lstm_cnn", "create_model_wavenet"
#' or "create_model_lstm_cnn_target_middle".
#' In \code{function_args} arguments of the corresponding can be specified, if no argument is given default values will be used.
#' Example: \code{built_model = list(create_model_function = "create_model_lstm_cnn", function_args = list(maxlen = 50, lstm_layer_size = 32, layers.lstm = 1)}
#' @param path Path to folder where individual or multiple FASTA or FASTQ files are located for training. If \code{train_type} is \code{label_folder}, should be a vector
#' containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.
#' @param path Path to folder where individual or multiple FASTA or FASTQ files are located for validation. If \code{train_type} is \code{label_folder}, should be a vector
#' containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.
#' @param dataset Dataframe holding training samples in RAM instead of using generator.
#' @param checkpoint_path Path to checkpoints folder.
#' @param validation.split Defines the fraction of the batches that will be used for validation (compared to size of training data), i.e. one validtion iteration
#' processed \code{batch.size} * \code{steps.per.epoch} * \code{validation.split samples}.
#' @param run.name Name of the run (without file ending). Name will be used to identify output from callbacks.
#' @param batch.size Number of samples that are used for one network update.
#' @param epochs Number of iterations.
#' @param max.queue.size Queue on fit_generator().
#' @param reduce_lr_on_plateau Whether to use learning rate scheduler.
#' @param lr.plateau.factor Factor of decreasing learning rate when plateau is reached.
#' @param patience Number of epochs waiting for decrease in loss before reducing learning rate.
#' @param cooldown Number of epochs without changing learning rate.
#' @param steps.per.epoch Number of batches per epoch.
#' @param step Frequency of sampling steps.
#' @param randomFiles Boolean, whether to go through files sequentially or shuffle beforehand.
#' @param vocabulary Vector of allowed characters. Character outside vocabulary get encoded as specified in \code{ambiguous_nuc}.
#' @param initial_epoch Epoch at which to start training. Note that network
#' will run for (\code{epochs} - \code{initial_epochs}) rounds and not \code{epochs} rounds.
#' @param tensorboard.log Path to tensorboard log directory.
#' @param save_best_only Only save model that improved on best val_loss score.
#' @param save_weights_only Whether to save weights only.
#' @param seed Sets seed for set.seed function, for reproducible results when using \code{randomFiles} or \code{shuffleFastaEntries}
#' @param shuffleFastaEntries Logical, shuffle entries in file.
#' @param output List of optional outputs, no output if none is TRUE.
#' @param tb_images Boolean, whether to show plots in tensorboard. Note this doubles the time needed for validation step.
#' @param format File format, "fasta" or "fastq".
#' @param fileLog Write name of files used for training to csv file if path is specified.
#' @param labelVocabulary Character vector of possible targets. Targets outside \code{labelVocabulary} will get discarded if
#' \code{train_type = "label_header"}.
#' @param numberOfFiles Use only specified number of files, ignored if greater than number of files in \code{path}.
#' @param reverseComplementEncoding Logical, use both original sequence and reverse.complement as two input sequences.
#' @param output_format Determines shape of output tensor for language model (if \code{train_type = "lm"}).
#' Either "target_right", "target_middle_lstm", "target_middle_cnn" or "wavenet".
#' Assume a sequence "AACCGTA". Output correspond as follows
#' "target_right": X = "AACCGT", Y = "A"
#' "target_middle_lstm": X = (X_1 = "AAC", X_2 = "ATG"), Y = "C" (note reversed order of X_2)
#' "target_middle_cnn": X = "AACGTA", Y = "C" (nucleotide in middle encoded as 0-vector)
#' "wavenet": X = "AACCGT", Y = "ACCGTA"
#' "dummy_gen": generator creates random data
#' @param reset_states Boolean, whether to reset hidden states of RNN layer at every new input file.
#' @param proportion_per_file Numerical value between 0 and 1. Proportion of possible samples to take from one file. Takes samples from random subsequence.
#' @param read_data If true the first element of output is a list of length 2, each containing one part of paired read. Maxlen should be 2*length of one read.
#' @param use_quality_score Whether to use fastq qualitiy scores. If TRUE input is not one-hot-encoding but corresponds to probabilities.
#' For example (0.97, 0.01, 0.01, 0.01) instead of (1, 0, 0, 0).
#' @param padding Whether to pad sequences too short for one sample with zeros.
#' @param early_stopping_time Time in seconds after which to stop training.
#' @param validation_only_after_training Boolean, whether to skip validation during training and only do one validation after training.
#' @param skip_amb_nuc Threshold of ambiguous nucleotides to accept in fasta entry. Complete entry will get discarded otherwise.
#' @param class_weight Vector with number of samples for each class in training data. Order should correspond to \code{labelVocabulary}.
#' You can use \code{get_class_weight} function to estimates class weights: class_weights <- get_class_weights(path = path, train_type = train_type)
#' If train_type = "label_csv" you need to add path to csv file:
#' class_weights <- get_class_weights(path = path, train_type = train_type, csv_path = target_from_csv)
#' @param print_scores Whether to print train/validation scores during training.  
#' @param train_val_split_csv A csv file specifying train/validation split. csv file should contain one column named "file" and one columnn named 
#' "type". The "file" column contains names of fasta/fastq files and "type" column specifies if file is used for training or validation. 
#' Entries in "type" must be named "train" or "val", otherwise file will not be used for either. path and path.val arguments should be the same.
#' Not implemented for train_type = "label_folder".
#' @export
trainNetwork <- function(train_type = "lm",
                         built_model = list(create_model_function = NULL, function_args = list()),
                         model = NULL,
                         path = NULL,
                         path.val = NULL,
                         dataset = NULL,
                         checkpoint_path = NULL,
                         validation.split = 0.2,
                         run.name = "run",
                         batch.size = 64,
                         epochs = 10,
                         max.queue.size = 100,
                         reduce_lr_on_plateau = TRUE,
                         lr.plateau.factor = 0.9,
                         patience = 20,
                         cooldown = 1,
                         steps.per.epoch = 1000,
                         step = 1,
                         randomFiles = TRUE,
                         initial_epoch = 0,
                         vocabulary = c("a", "c", "g", "t"),
                         tensorboard.log = NULL,
                         save_best_only = TRUE,
                         save_weights_only = FALSE,
                         seed = c(1234, 4321),
                         shuffleFastaEntries = TRUE,
                         output = list(none = FALSE,
                                       checkpoints = FALSE,
                                       tensorboard = FALSE,
                                       log = FALSE,
                                       serialize_model = FALSE,
                                       full_model = FALSE
                         ),
                         tb_images = TRUE,
                         format = "fasta",
                         fileLog = NULL,
                         labelVocabulary = NULL,
                         numberOfFiles = NULL,
                         reverseComplements = FALSE,
                         reverseComplementEncoding = FALSE,
                         output_format = "target_right",
                         reset_states = FALSE,
                         ambiguous_nuc = "equal",
                         proportion_per_file = NULL,
                         read_data = FALSE,
                         use_quality_score = FALSE,
                         padding = FALSE,
                         early_stopping_time = NULL,
                         added_label_path = NULL,
                         add_input_as_seq = NULL,
                         target_from_csv = NULL,
                         target_split = NULL,
                         validation_only_after_training = FALSE,
                         skip_amb_nuc = NULL,
                         max_samples = NULL,
                         split_seq = FALSE,
                         class_weight = NULL,
                         concat_seq = NULL,
                         target_len = 1,
                         print_scores = TRUE,
                         train_val_split_csv = NULL) {
  
  tensorflow::tf$random$set_seed(seed[1])
  stopifnot(train_type %in% c("lm", "label_header", "label_folder", "label_csv"))
  stopifnot(ambiguous_nuc %in% c("zero", "equal", "discard", "empirical"))
  if (train_type == "label_csv") {
    train_type <- "label_header"
    if (is.null(target_from_csv)) {
      stop('You need to add a path to csv file for target_from_csv when using train_type = "label_csv"')
    }
    if (!is.null(labelVocabulary)) {
      message("Reading labelVocabulary from csv header")
      output_label_csv <- read.csv2(target_from_csv, header = TRUE, stringsAsFactors = FALSE)
      if (dim(output_label_csv)[2] == 1) {
        output_label_csv <- read.csv(target_from_csv, header = TRUE, stringsAsFactors = FALSE)
      } 
      labelVocabulary <- names(output_label_csv)
      labelVocabulary <- labelVocabulary[labelVocabulary != "file"]
    }
  }
  
  if (!is.null(skip_amb_nuc)) {
    if((skip_amb_nuc > 1) | (skip_amb_nuc <0)) {
      stop("skip_amb_nuc should be between 0 and 1 or NULL")
    }
  }
  
  if (!is.null(proportion_per_file)) {
    if(any(proportion_per_file > 1) | any(proportion_per_file  < 0)) {
      stop("proportion_per_file should be between 0 and 1 or NULL")
    }
  }
  
  if (!is.null(class_weight) && (length(class_weight) != length(labelVocabulary))) {
    stop("class_weight and labelVocabulary must have same length")
  }
  
  # train validation split via csv file 
  if (!is.null(train_val_split_csv)) {
    if (train_type == "label_folder") {
      stop('train_val_split_csv not implemented for train_type = "label_folder"')
    }
    if (is.null(path.val)) {
      path.val <- path 
    } else {
      if (!all(unlist(path.val) %in% unlist(path))) {
        warning("Train/validation split split done via file in train_val_split_csv. Only using files from path argument.")
      }
      path.val <- path
    }
    
    train_val_file <- read.csv2(train_val_split_csv, header = TRUE, stringsAsFactors = FALSE)
    if (dim(train_val_file)[2] == 1) {
      train_val_file <- read.csv(train_val_split_csv, header = TRUE, stringsAsFactors = FALSE)
    }
    train_val_file <- dplyr::distinct(train_val_file)
    
    if (!all(c("file", "type") %in% names(train_val_file))) {
      stop("Column names of train_val_split_csv file must be 'file' and 'type'") 
    }  
    
    if (length(train_val_file$file) != length(unique(train_val_file$file))) {
      stop("In train_val_split_csv all entires in 'file' column must be unique")
    }
    
    train_files <- train_val_file %>% dplyr::filter(type == "train")
    train_files <- as.character(train_files$file)
    val_files <- train_val_file %>% dplyr::filter(type == "val")
    val_files <- as.character(val_files$file)
  } else {
    train_files <- NULL
    val_files <- NULL
  }
  
  wavenet_format <- FALSE ; target_middle <- FALSE ; cnn_format <- FALSE
  if (train_type == "lm") {
    stopifnot(output_format %in% c("target_right", "target_middle_lstm", "target_middle_cnn", "wavenet", "dummy_gen"))
    if (output_format == "target_middle_lstm") target_middle <- TRUE
    if (output_format == "target_middle_cnn") cnn_format <- TRUE
    if (output_format == "wavenet") wavenet_format <- TRUE
  }
  
  if (is.null(built_model$create_model_function) + is.null(model) == 0) {
    stop("Two models were specified. Set either model or built_model$create_model_function argument to NULL.")
  }
  
  if (train_type == "lm") {
    labelGen <- FALSE
    labelByFolder <- FALSE
  }
  
  if (train_type == "label_header") {
    labelGen <- TRUE
    labelByFolder <- FALSE
    if (is.null(target_from_csv)) stopifnot(!is.null(labelVocabulary))
  }
  
  if (train_type == "label_folder") {
    labelGen <- TRUE
    labelByFolder <- TRUE
    stopifnot(!is.null(labelVocabulary))
    stopifnot(length(path) == length(labelVocabulary))
  }
  
  if (output$none) {
    output$checkpoints <- FALSE
    output$tensorboard <- FALSE
    output$log <- FALSE
    output$serialize_model <- FALSE
    output$full_model <- FALSE
  }
  
  # set model arguments
  if (!is.null(built_model[[1]])) {
    if (built_model[[1]] == "create_model_lstm_cnn_target_middle") {
      if (!read_data){
        # target_middle <- TRUE
        # wavenet_format <- FALSE
      }
    }
    if (built_model[[1]] == "create_model_lstm_cnn") {
      #target_middle <- FALSE
      #wavenet_format <- FALSE
    }
    if (built_model[[1]] == "create_model_wavenet") {
      #target_middle <- TRUE
      #wavenet_format <- TRUE
    }
    new_arguments <- names(built_model[[2]])
    default_arguments <- formals(built_model[[1]])
    # overwrite default arguments
    for (arg in new_arguments) {
      default_arguments[arg] <- built_model[[2]][arg]
    }
    # create model
    if (built_model[[1]] == "create_model_lstm_cnn") {
      formals(create_model_lstm_cnn) <- default_arguments
      model <- create_model_lstm_cnn()
    }
    
    if (built_model[[1]] == "create_model_lstm_cnn_target_middle") {
      formals(create_model_lstm_cnn_target_middle) <- default_arguments
      model <- create_model_lstm_cnn_target_middle()
    }
    
    if (built_model[[1]] == "create_model_wavenet") {
      if (!wavenet_format) {
        warning("Argument wavenet_format should be TRUE when using wavenet architecture.")
      }
      
      formals(create_model_wavenet) <- default_arguments
      model <- create_model_wavenet()
    }
  }
  
  model_weights <- model$get_weights()
  
  # function arguments
  argumentList <- as.list(match.call(expand.dots=FALSE))
  
  label.vocabulary.size <- length(labelVocabulary)
  vocabulary.size <- length(vocabulary)
  
  # extract maxlen from model
  num_in_layers <- length(model$inputs)
  if (num_in_layers == 1) {
    maxlen <- model$input$shape[[2]]
  } else {
    if (!target_middle & !read_data & !split_seq) {
      maxlen <- model$input[[num_in_layers]]$shape[[2]]
    } else {
      maxlen <- model$inputs[[num_in_layers - 1]]$shape[[2]] + model$inputs[[num_in_layers]]$shape[[2]]
    }
  }
  
  # get solver and learning rate
  solver <- stringr::str_to_lower(model$optimizer$get_config()["name"])
  learning.rate <- keras::k_eval(model$optimizer$lr)
  if (solver == "adam") {
    optimizer <-  keras::optimizer_adam(lr = learning.rate)
  }
  if (solver == "adagrad") {
    optimizer <- keras::optimizer_adagrad(lr = learning.rate)
  }
  if (solver == "rmsprop") {
    optimizer <- keras::optimizer_rmsprop(lr = learning.rate)
  }
  if (solver == "sgd") {
    optimizer <- keras::optimizer_sgd(lr = learning.rate)
  }
  
  if (labelByFolder) {
    if (length(path) == 1) warning("Training with just one label")
  }
  
  if (output$checkpoints) {
    # create folder for checkpoints using run.name
    # filenames contain epoch, validation loss and validation accuracy
    checkpoint_dir <- paste0(checkpoint_path, "/", run.name, "_checkpoints")
    dir.create(checkpoint_dir, showWarnings = FALSE)
    if (!is.list(model$output)) {
      filepath_checkpoints <- file.path(checkpoint_dir, "Ep.{epoch:03d}-val_loss{val_loss:.2f}-val_acc{val_acc:.3f}.hdf5")
    } else {
      filepath_checkpoints <- file.path(checkpoint_dir, "Ep.{epoch:03d}.hdf5")
      if (save_best_only) {
        warning("save_best_only not implemented for multi target. Setting save_best_only to FALSE")
        save_best_only <- FALSE
      }
    }
  }
  
  # Check if fileLog is unique
  if (!is.null(fileLog) && dir.exists(fileLog)) {
    stop(paste0("fileLog entry is already present. Please give this file a unique name."))
  }
  
  # Check if run.name is unique
  if (output$tensorboard && dir.exists(file.path(tensorboard.log, run.name))) {
    stop(paste0("Tensorboard entry '", run.name , "' is already present. Please give your run a unique name."))
  }
  
  # add empty hparam dict if non exists
  if (!reticulate::py_has_attr(model, "hparam")) {
    model$hparam <- reticulate::dict()
  }
  
  # tempory file to log training data
  removeLog <- FALSE
  if (is.null(fileLog)) {
    removeLog <- TRUE
    fileLog <- tempfile(pattern = "", fileext = ".csv")
  } else {
    if (!endsWith(fileLog, ".csv")) fileLog <- paste0(fileLog, ".csv")
  }
  if (reset_states) {
    fileLogVal <- tempfile(pattern = "", fileext = ".csv")
  } else {
    fileLogVal <- NULL
  }
  
  # if no dataset is supplied, external fasta generator will generate batches
  if (is.null(dataset)) {
    message("Starting fasta generator...")
    
    if (output_format == "dummy_gen") {
      gen <- dummy_gen(model, batch.size)
      gen.val <- dummy_gen(model, batch.size)
      removeLog <- FALSE
    } else {
      
      if (!labelGen) {
        
        # generator for training
        gen <- fastaFileGenerator(corpus.dir = path, batch.size = batch.size,
                                  maxlen = maxlen, step = step, randomFiles = randomFiles,
                                  vocabulary = vocabulary, seed = seed[1],
                                  shuffleFastaEntries = shuffleFastaEntries, format = format,
                                  fileLog = fileLog, reverseComplements = reverseComplements,
                                  output_format = output_format, ambiguous_nuc = ambiguous_nuc,
                                  proportion_per_file = proportion_per_file, skip_amb_nuc = skip_amb_nuc,
                                  use_quality_score = use_quality_score, padding = padding,
                                  added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                  max_samples = max_samples, concat_seq = concat_seq, target_len = target_len,
                                  file_filter = train_files)
        
        # generator for validation
        gen.val <- fastaFileGenerator(corpus.dir = path.val, batch.size = batch.size,
                                      maxlen = maxlen, step = step, randomFiles = randomFiles,
                                      vocabulary = vocabulary, seed = seed[2],
                                      shuffleFastaEntries = shuffleFastaEntries, format = format,
                                      fileLog = fileLogVal, reverseComplements = FALSE,
                                      output_format = output_format, skip_amb_nuc = skip_amb_nuc,
                                      ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                      use_quality_score = use_quality_score, padding = padding,
                                      added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                      max_samples = max_samples, concat_seq = concat_seq, target_len = target_len,
                                      file_filter = val_files)
        
        # label generator
      } else {
        # label by folder
        if (labelByFolder) {
          #' @param reverseComplementEncoding Logical, use both original sequence and reverse.complement as two input sequences.
          
          # initialize training generators
          initializeGenerators(directories = path, format = format, batch.size = batch.size, maxlen = maxlen, vocabulary = vocabulary,
                               verbose = FALSE, randomFiles = randomFiles, step = step, showWarnings = FALSE, seed = seed[1],
                               shuffleFastaEntries = shuffleFastaEntries, numberOfFiles = numberOfFiles, skip_amb_nuc = skip_amb_nuc,
                               fileLog = fileLog, reverseComplements = reverseComplements, reverseComplementEncoding = reverseComplementEncoding, val = FALSE, ambiguous_nuc = ambiguous_nuc,
                               proportion_per_file = proportion_per_file, read_data = read_data, use_quality_score = use_quality_score,
                               padding = padding, max_samples = max_samples, split_seq = split_seq, concat_seq = concat_seq,
                               added_label_path = added_label_path, add_input_as_seq = add_input_as_seq)
          
          # initialize validation generators
          initializeGenerators(directories = path.val, format = format, batch.size = batch.size, maxlen = maxlen,
                               vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                               showWarnings = FALSE, seed = seed[2], shuffleFastaEntries = shuffleFastaEntries, skip_amb_nuc = skip_amb_nuc,
                               numberOfFiles = NULL, fileLog = fileLogVal, reverseComplements = FALSE, reverseComplementEncoding = reverseComplementEncoding, val = TRUE,
                               ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file, read_data = read_data,
                               use_quality_score = use_quality_score, padding = padding, max_samples = max_samples,
                               split_seq = split_seq, concat_seq = concat_seq, added_label_path = added_label_path,
                               add_input_as_seq = add_input_as_seq)
          
          gen <- labelByFolderGeneratorWrapper(val = FALSE, path = path)
          gen.val <- labelByFolderGeneratorWrapper(val = TRUE, path = path.val)
          
        } else {
          
          # generator for training
          gen <- fastaLabelGenerator(corpus.dir = path, format = format, batch.size = batch.size, maxlen = maxlen,
                                     vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                                     showWarnings = FALSE, seed = seed[1], shuffleFastaEntries = shuffleFastaEntries,
                                     fileLog = fileLog, labelVocabulary = labelVocabulary, reverseComplements = reverseComplements,
                                     ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                     read_data = read_data, use_quality_score = use_quality_score, padding = padding,
                                     added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                     skip_amb_nuc = skip_amb_nuc, max_samples = max_samples, concat_seq = concat_seq,
                                     target_from_csv = target_from_csv, target_split = target_split, file_filter = train_files)
          
          # generator for validation
          gen.val <- fastaLabelGenerator(corpus.dir = path.val, format = format, batch.size = batch.size, maxlen = maxlen,
                                         vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                                         showWarnings = FALSE, seed = seed[2], shuffleFastaEntries = shuffleFastaEntries,
                                         fileLog = fileLogVal, labelVocabulary = labelVocabulary, reverseComplements = FALSE,
                                         ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                         added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                         read_data = read_data, use_quality_score = use_quality_score, padding = padding,
                                         skip_amb_nuc = skip_amb_nuc, max_samples = max_samples, concat_seq = concat_seq,
                                         target_from_csv = target_from_csv, target_split = target_split, file_filter = val_files)
          
        }
      }
    }
    
    # callbacks
    callbacks <- vector("list")
    
    if (reduce_lr_on_plateau) {
      if (is.list(model$outputs)) {
        monitor <- "val_loss"
      } else {
        monitor <- "val_acc"
      }
      callbacks[[1]] <- reduce_lr_cb(patience = patience, cooldown = cooldown,
                                     lr.plateau.factor = lr.plateau.factor,
                                     monitor = monitor)
    }
    
    if (output$log) {
      callbacks <- c(callbacks, log_cb(run.name))
    }
    
    if (!output$tensorboard) tb_images <- FALSE
    if (output$tensorboard) {
      
      # count files in path
      num_train_files <- count_files(path = path, format = format, train_type = train_type)
      
      complete_tb <- tensorboard_complete_cb(default_arguments = default_arguments, model = model, tensorboard.log = tensorboard.log, run.name = run.name, train_type = train_type,
                                             model_path = model_path, path = path, validation.split = validation.split, batch.size = batch.size, epochs = epochs,
                                             max.queue.size = max.queue.size, lr.plateau.factor = lr.plateau.factor, patience = patience, cooldown = cooldown,
                                             steps.per.epoch = steps.per.epoch, step = step, randomFiles = randomFiles, initial_epoch = initial_epoch, vocabulary = vocabulary,
                                             learning.rate = learning.rate, shuffleFastaEntries = shuffleFastaEntries, labelVocabulary = labelVocabulary, solver = solver,
                                             numberOfFiles = numberOfFiles, reverseComplements = reverseComplements, wavenet_format = wavenet_format,  cnn_format = cnn_format,
                                             create_model_function = built_model$create_model_function, vocabulary.size = vocabulary.size, gen_cb = gen_cb, argumentList = argumentList,
                                             maxlen = maxlen, labelGen = labelGen, labelByFolder = labelByFolder, label.vocabulary.size = label.vocabulary.size, tb_images = FALSE,
                                             target_middle = target_middle, num_train_files = num_train_files, fileLog = fileLog, proportion_per_file = proportion_per_file,
                                             skip_amb_nuc = skip_amb_nuc, max_samples = max_samples)
      callbacks <- c(callbacks, complete_tb)
    }
    
    if (output$checkpoints) {
      if (wavenet_format) {
        # can only save weights for wavenet
        save_weights_only <- TRUE
      }
      callbacks <- c(callbacks, checkpoint_cb(filepath = filepath_checkpoints, save_weights_only = save_weights_only,
                                              save_best_only = save_best_only))
    }
    
    if (reset_states) {
      callbacks <- c(callbacks, reset_states_cb(fileLog = fileLog, fileLogVal = fileLogVal))
    }
    
    if (!is.null(early_stopping_time)) {
      callbacks <- c(callbacks, early_stopping_cb(early_stopping_patience = early_stopping_patience,
                                                  early_stopping_time = early_stopping_time))
    }
    
    # skip validation callback
    if (validation_only_after_training | is.null(validation.split) || validation.split == 0) {
      validation_data <- NULL
    } else {
      validation_data <- gen.val
    }
    validation_steps <- ceiling(steps.per.epoch * validation.split)
    
    if (validation_only_after_training) {
      callbacks <- c(callbacks, validation_after_training_cb(gen.val = gen.val, validation_steps = validation_steps))
    }
    
    if (tb_images) {
      if (is.list(model$output)) {
        warning("Tensorboard images (confusion matrix) not implemented for model with multiple outputs.
               Setting tb_images to FALSE")
        tb_images <- FALSE
      }
      
      if (model$loss == "binary_crossentropy") {
        warning("Tensorboard images (confusion matrix) not implemented for sigmoid activation in last layer.
               Setting tb_images to FALSE")
        tb_images <- FALSE
      }
    }
    
    if (tb_images) {
      
      if (!reticulate::py_has_attr(model, "cm_log")) {
        model$cm_log <- tempfile(pattern = "", fileext = ".csv")
      }
      if (train_type == "lm") {
        confMatLabels <- vocabulary
      } else {
        confMatLabels <- labelVocabulary
      }
      
      #add f1-score for binary classification
      cm_log <- tempfile(pattern = "", fileext = ".csv")
      model$cm_log <- cm_log
      num_targets <- ifelse(train_type == "lm", length(vocabulary), length(labelVocabulary))
      # add f1-score for binary classification
      f1 <- keras::custom_metric("f1", function(y_true, y_pred) {
        
        true <- keras::k_argmax(y_true)
        pred <- keras::k_argmax(y_pred)
        if (!is.list(dim(y_true))) {
          df <- data.frame(as.array(true), as.array(pred))
          write.table(x = df, file = model$cm_log, append = TRUE, col.names = FALSE, row.names = FALSE)
        }
        if (num_targets == 2) {
          labels <- tensorflow::tf$math$argmax(y_true, axis = 1L)
          predictions <- tensorflow::tf$math$argmax(y_pred, axis = 1L)
          TP <- tensorflow::tf$cast(tensorflow::tf$math$count_nonzero(predictions * labels), dtype = "float32")
          #TN <-  tensorflow::tf$cast(tensorflow::tf$math$count_nonzero((predictions - 1L) * (labels - 1L)), dtype = "float32")
          FP <-  tensorflow::tf$cast(tensorflow::tf$math$count_nonzero(predictions * (labels - 1L)), dtype = "float32")
          FN <-  tensorflow::tf$cast(tensorflow::tf$math$count_nonzero((predictions - 1L) * labels), dtype = "float32")
          precision <- tensorflow::tf$math$divide_no_nan(TP, TP + FP)
          recall <- tensorflow::tf$math$divide_no_nan(TP, TP + FN)
          two <- tensorflow::tf$constant(2)
          A <- two * precision * recall
          B <- precision + recall
          f1_score <- tensorflow::tf$math$divide_no_nan(A, B)
          return(f1_score)
        } else {
          return(Inf)
        }
      })
      
      contains_f1_metric <- FALSE
      
      for (i in 1:length(model$metrics)) {
        if (model$metrics[[i]]$name == "f1") contains_f1_metric   <- TRUE
      }
      
      if (contains_f1_metric) {
        model_metrics <- model$metrics
      } else {
        model_metrics <- c(model$metrics, f1)
      }
      
      model %>% keras::compile(loss = model$loss,
                               optimizer = model$optimizer, metrics = model_metrics)
      
      callbacks <- c(callbacks, conf_matrix_cb(path = model$cm_log, tensorboard.log = tensorboard.log,
                                               run.name = run.name, confMatLabels = confMatLabels))
    }
    
    # training
    message("Start training ...")
    
    if (!is.null(class_weight)) {
      weight_list <- list()
      weight_list[["0"]] <- 1
      for (i in 2:(length(class_weight))) {
        weight_list[[as.character(i-1)]] <- class_weight[1]/class_weight[i]
      }
      class_weight <- weight_list
    }
    
    model <- keras::set_weights(model, model_weights)
    history <-
      model %>% keras::fit_generator(
        generator = gen,
        validation_data = validation_data,
        validation_steps = validation_steps,
        steps_per_epoch = steps.per.epoch,
        max_queue_size = max.queue.size,
        epochs = epochs,
        initial_epoch = initial_epoch,
        callbacks = callbacks,
        class_weight = class_weight,
        verbose = print_scores
      )
    
    if (validation_only_after_training) {
      history$val_loss <- model$val_loss
      history$val_acc <- model$val_acc
      model$val_loss <- NULL
      model$val_acc <- NULL
    }
    
  } else {
    model <- keras::set_weights(model, model_weights)
    message("Start training ...")
    history <- model %>% keras::fit(
      dataset$X,
      dataset$Y,
      batch_size = batch.size,
      validation_split = validation.split,
      epochs = epochs)
  }
  
  if (removeLog) {
    file.remove(fileLog)
  }
  
  # save final model
  message("Training done.")
  
  if (output$serialize_model) {
    Rmodel <-
      keras::serialize_model(model, include_optimizer = TRUE)
    save(Rmodel, file = paste0(run.name, "_full_model.Rdata"))
  }
  
  if (output$full_model) {
    keras::save_model_hdf5(
      model,
      paste0(run.name, "_full_model.hdf5"),
      overwrite = TRUE,
      include_optimizer = TRUE
    )
  }
  return(history)
}

