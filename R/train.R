#' @title Trains a neural network on genomic data. Designed for developing genome based language models (GenomeNet)
#'
#' @description
#' Depth and number of neurons per layer of the netwok can be specified.
#' If a path to a folder where FASTA files are located is provided, batches will be generated using an external generator which
#' is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()})
#' and keeps them in RAM.
#' @inheritParams fastaFileGenerator
#' @inheritParams labelByFolderGenerator
#' @inheritParams fastaLabelGenerator
#' @param train_type Either "lm" for language model, "label_header", "label_folder", "label_csv" or "label_rds". Language model is trained to predict character in sequence.
#' "label_header"/"label_folder" are trained to predict a corresponding class, given a sequence as input. If "label_header", class will be read from fasta headers.
#' If "label_folder", class will be read from folder, i.e. all files in one folder must belong to the same class.
#' If "label_csv", targets are read from a csv file. This file should have one column names "file". The targets then correspond to entries in that row (except "file"
#' column). Example: if we are currently working with a file called "a.fasta", there should be a row in our csv file
#'    file  |  label_1 | label_2
#' "a.fasta"     1          8
#' If "label_rds", generator will iterate over set of .rds files containing each a list of input and target tensors. Not implemented for model
#' with multiple inputs. If "lm_rds", generator will iterate over set of .rds files and will split tensor according to target_len argument
#' (targets are last target_len nucleotides of each sequence).
#' @param model A keras model.
#' @param built_model Call to a function that creates a model. \code{create_model_function} can be either "create_model_lstm_cnn", "create_model_wavenet"
#' or "create_model_lstm_cnn_target_middle".
#' In \code{function_args} arguments of the corresponding can be specified, if no argument is given default values will be used.
#' Example: \code{built_model = list(create_model_function = "create_model_lstm_cnn", function_args = list(maxlen = 50, lstm_layer_size = 32, layers.lstm = 1)}
#' @param path Path to folder where individual or multiple FASTA or FASTQ files are located for training. If \code{train_type} is \code{label_folder}, should be a vector
#' containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.
#' @param path.val Path to folder where individual or multiple FASTA or FASTQ files are located for validation. If \code{train_type} is \code{label_folder}, should be a vector
#' containing a path for each class. If \code{train_type} is not \code{label_folder}, can be a list of directories.
#' @param dataset List of training data, holding training samples in RAM instead of using generator. Should be list with two entries called "X" and "Y".
#' @param dataset_val List of validation data. Should be list with two entries.
#' @param checkpoint_path Path to checkpoints folder.
#' @param validation.split For generator, defines the fraction of batches that will be used for validation (compared to size of training data), i.e. one validtion iteration
#' processes \code{batch.size} x \code{steps.per.epoch} x \code{validation.split} samples. If you use dataset instead of generator and \code{dataset_val} is NULL, splits \code{dataset}
#' into train/validation data.
#' @param run.name Name of the run (without file ending). Name will be used to identify output from callbacks.
#' @param batch.size Number of samples that are used for one network update.
#' @param epochs Number of iterations.
#' @param max.queue.size Queue on fit_generator().
#' @param reduce_lr_on_plateau Whether to use learning rate scheduler.
#' @param lr.plateau.factor Factor of decreasing learning rate when plateau is reached.
#' @param patience Number of epochs waiting for decrease in loss before reducing learning rate.
#' @param cooldown Number of epochs without changing learning rate.
#' @param steps.per.epoch Number of batches per epoch.
#' @param step Frequency of sampling steps.
#' @param randomFiles Boolean, whether to go through files sequentially or shuffle beforehand.
#' @param vocabulary Vector of allowed characters. Character outside vocabulary get encoded as specified in \code{ambiguous_nuc}.
#' @param initial_epoch Epoch at which to start training. Note that network
#' will run for (\code{epochs} - \code{initial_epochs}) rounds and not \code{epochs} rounds.
#' @param tensorboard.log Path to tensorboard log directory.
#' @param save_best_only Only save model that improved on best val_loss score.
#' @param save_weights_only Whether to save weights only.
#' @param seed Sets seed for set.seed function, for reproducible results when using \code{randomFiles} or \code{shuffleFastaEntries}
#' @param shuffleFastaEntries Logical, shuffle entries in file.
#' @param output List of optional outputs, no output if none is TRUE.
#' @param tb_images Boolean, whether to show plots in tensorboard.
#' @param format File format, "fasta" or "fastq".
#' @param fileLog Write name of files used for training to csv file if path is specified.
#' @param labelVocabulary Character vector of possible targets. Targets outside \code{labelVocabulary} will get discarded if
#' \code{train_type = "label_header"}.
#' @param numberOfFiles Use only specified number of files, ignored if greater than number of files in \code{path}.
#' @param reverseComplementEncoding Logical, use both original sequence and reverse.complement as two input sequences.
#' @param output_format Determines shape of output tensor for language model (if \code{train_type = "lm"}).
#' Either "target_right", "target_middle_lstm", "target_middle_cnn" or "wavenet".
#' Assume a sequence "AACCGTA". Output correspond as follows
#' "target_right": X = "AACCGT", Y = "A"
#' "target_middle_lstm": X = (X_1 = "AAC", X_2 = "ATG"), Y = "C" (note reversed order of X_2)
#' "target_middle_cnn": X = "AACGTA", Y = "C" (nucleotide in middle encoded as 0-vector)
#' "wavenet": X = "AACCGT", Y = "ACCGTA"
#' "dummy_gen": generator creates random data
#' @param reset_states Boolean, whether to reset hidden states of RNN layer at every new input file.
#' @param proportion_per_file Numerical value between 0 and 1. Proportion of possible samples to take from one file. Takes samples from random subsequence.
#' @param read_data If true the first element of output is a list of length 2, each containing one part of paired read. Maxlen should be 2*length of one read.
#' @param use_quality_score Whether to use fastq qualitiy scores. If TRUE input is not one-hot-encoding but corresponds to probabilities.
#' For example (0.97, 0.01, 0.01, 0.01) instead of (1, 0, 0, 0).
#' @param padding Whether to pad sequences too short for one sample with zeros.
#' @param early_stopping_time Time in seconds after which to stop training.
#' @param validation_only_after_training Boolean, whether to skip validation during training and only do one validation after training.
#' @param skip_amb_nuc Threshold of ambiguous nucleotides to accept in fasta entry. Complete entry will get discarded otherwise.
#' @param class_weight List of weights for output. Order should correspond to \code{labelVocabulary}.
#' You can use \code{get_class_weight} function to estimates class weights: class_weights <- get_class_weights(path = path, train_type = train_type)
#' If train_type = "label_csv" you need to add path to csv file:
#' class_weights <- get_class_weights(path = path, train_type = train_type, csv_path = target_from_csv)
#' @param print_scores Whether to print train/validation scores during training.
#' @param train_val_split_csv A csv file specifying train/validation split. csv file should contain one column named "file" and one columnn named
#' "type". The "file" column contains names of fasta/fastq files and "type" column specifies if file is used for training or validation.
#' Entries in "type" must be named "train" or "val", otherwise file will not be used for either. path and path.val arguments should be the same.
#' Not implemented for train_type = "label_folder".
#' @param set_learning When you want to assign one label to set of samples. Only implemented for train_type = "label_folder".
#' Input is a list with the following parameters
#' (1) \code{samples_per_target}, how many samples to use for one target; (2) \code{maxlen} length of one sample
#' (3) \code{reshape_mode} "time_dist" or "multi_input".
#' @param n_gram Encode n nucleotides at once.
#' @export
trainNetwork <- function(train_type = "lm",
                         built_model = list(create_model_function = NULL, function_args = list()),
                         model = NULL,
                         path = NULL,
                         path.val = NULL,
                         dataset = NULL,
                         dataset_val = NULL,
                         checkpoint_path = NULL,
                         validation.split = 0.2,
                         run.name = "run",
                         batch.size = 64,
                         epochs = 10,
                         max.queue.size = 100,
                         reduce_lr_on_plateau = TRUE,
                         lr.plateau.factor = 0.9,
                         patience = 20,
                         cooldown = 1,
                         steps.per.epoch = 1000,
                         step = 1,
                         randomFiles = TRUE,
                         initial_epoch = 0,
                         vocabulary = c("a", "c", "g", "t"),
                         tensorboard.log = NULL,
                         save_best_only = TRUE,
                         save_weights_only = FALSE,
                         seed = c(1234, 4321),
                         shuffleFastaEntries = TRUE,
                         output = list(none = FALSE,
                                       checkpoints = FALSE,
                                       tensorboard = FALSE,
                                       log = FALSE,
                                       serialize_model = FALSE,
                                       full_model = FALSE
                         ),
                         tb_images = FALSE,
                         format = "fasta",
                         fileLog = NULL,
                         labelVocabulary = NULL,
                         numberOfFiles = NULL,
                         reverseComplements = FALSE,
                         reverseComplementEncoding = FALSE,
                         output_format = "target_right",
                         reset_states = FALSE,
                         ambiguous_nuc = "equal",
                         proportion_per_file = NULL,
                         read_data = FALSE,
                         use_quality_score = FALSE,
                         padding = FALSE,
                         early_stopping_time = NULL,
                         added_label_path = NULL,
                         add_input_as_seq = NULL,
                         target_from_csv = NULL,
                         target_split = NULL,
                         validation_only_after_training = FALSE,
                         skip_amb_nuc = NULL,
                         max_samples = NULL,
                         split_seq = FALSE,
                         class_weight = NULL,
                         concat_seq = NULL,
                         target_len = 1,
                         print_scores = TRUE,
                         train_val_split_csv = NULL,
                         use_coverage = NULL,
                         set_learning = NULL,
                         proportion_entries = NULL,
                         sample_by_file_size = FALSE,
                         n_gram = NULL,
                         n_gram_stride = 1,
                         gen.val = NULL) {

  train_with_gen <- is.null(dataset)
  wavenet_format <- FALSE ; target_middle <- FALSE ; cnn_format <- FALSE
  if (train_with_gen) {
    stopifnot(train_type %in% c("lm", "label_header", "label_folder", "label_csv", "label_rds", "lm_rds"))
    stopifnot(ambiguous_nuc %in% c("zero", "equal", "discard", "empirical"))
    stopifnot(length(vocabulary) == length(unique(vocabulary)))
    stopifnot(length(labelVocabulary) == length(unique(labelVocabulary)))
    labelByFolder <- FALSE

    if (train_type == "label_header") target_from_csv <- NULL
    if (train_type == "label_csv") {
      train_type <- "label_header"
      if (is.null(target_from_csv)) {
        stop('You need to add a path to csv file for target_from_csv when using train_type = "label_csv"')
      }
      if (!is.null(labelVocabulary)) {
        message("Reading labelVocabulary from csv header")
        output_label_csv <- read.csv2(target_from_csv, header = TRUE, stringsAsFactors = FALSE)
        if (dim(output_label_csv)[2] == 1) {
          output_label_csv <- read.csv(target_from_csv, header = TRUE, stringsAsFactors = FALSE)
        }
        labelVocabulary <- names(output_label_csv)
        labelVocabulary <- labelVocabulary[labelVocabulary != "file"]
      }
    }

    if (!is.null(skip_amb_nuc)) {
      if((skip_amb_nuc > 1) | (skip_amb_nuc <0)) {
        stop("skip_amb_nuc should be between 0 and 1 or NULL")
      }
    }

    if (!is.null(proportion_per_file)) {
      if(any(proportion_per_file > 1) | any(proportion_per_file  < 0)) {
        stop("proportion_per_file should be between 0 and 1 or NULL")
      }
    }

    if (!is.null(class_weight) && (length(class_weight) != length(labelVocabulary))) {
      stop("class_weight and labelVocabulary must have same length")
    }

    if (!is.null(concat_seq)) {
      if (!is.null(use_coverage)) stop("Coverage encoding not implemented for concat_seq")
      if (train_type == "lm") stop("Concatenation not implemented for language model")
    }

    # train validation split via csv file
    if (!is.null(train_val_split_csv)) {
      if (train_type == "label_folder") {
        stop('train_val_split_csv not implemented for train_type = "label_folder"')
      }
      if (is.null(path.val)) {
        path.val <- path
      } else {
        if (!all(unlist(path.val) %in% unlist(path))) {
          warning("Train/validation split split done via file in train_val_split_csv. Only using files from path argument.")
        }
        path.val <- path
      }

      train_val_file <- read.csv2(train_val_split_csv, header = TRUE, stringsAsFactors = FALSE)
      if (dim(train_val_file)[2] == 1) {
        train_val_file <- read.csv(train_val_split_csv, header = TRUE, stringsAsFactors = FALSE)
      }
      train_val_file <- dplyr::distinct(train_val_file)

      if (!all(c("file", "type") %in% names(train_val_file))) {
        stop("Column names of train_val_split_csv file must be 'file' and 'type'")
      }

      if (length(train_val_file$file) != length(unique(train_val_file$file))) {
        stop("In train_val_split_csv all entires in 'file' column must be unique")
      }

      train_files <- train_val_file %>% dplyr::filter(type == "train")
      train_files <- as.character(train_files$file)
      val_files <- train_val_file %>% dplyr::filter(type == "val" | type == "validation")
      val_files <- as.character(val_files$file)
    } else {
      train_files <- NULL
      val_files <- NULL
    }

    if (train_type == "lm") {
      stopifnot(output_format %in% c("target_right", "target_middle_lstm", "target_middle_cnn", "wavenet", "dummy_gen"))
      if (output_format == "target_middle_lstm") target_middle <- TRUE
      if (output_format == "target_middle_cnn") cnn_format <- TRUE
      if (output_format == "wavenet") wavenet_format <- TRUE
    }


    if (train_type == "lm") {
      labelGen <- FALSE
    }

    if (train_type == "label_header") {
      labelGen <- TRUE
      if (is.null(target_from_csv)) stopifnot(!is.null(labelVocabulary))
    }

    if (train_type == "label_folder") {
      labelGen <- TRUE
      labelByFolder <- TRUE
      stopifnot(!is.null(labelVocabulary))
      stopifnot(length(path) == length(labelVocabulary))
    }

  }

  if (is.null(built_model$create_model_function) + is.null(model) == 0) {
    stop("Two models were specified. Set either model or built_model$create_model_function argument to NULL.")
  }

  if (output$none) {
    output$checkpoints <- FALSE
    output$tensorboard <- FALSE
    output$log <- FALSE
    output$serialize_model <- FALSE
    output$full_model <- FALSE
  }

  # set model arguments
  if (!is.null(built_model[[1]])) {

    new_arguments <- names(built_model[[2]])
    default_arguments <- formals(built_model[[1]])
    # overwrite default arguments
    for (arg in new_arguments) {
      default_arguments[arg] <- built_model[[2]][arg]
    }
    # create model
    if (built_model[[1]] == "create_model_lstm_cnn") {
      formals(create_model_lstm_cnn) <- default_arguments
      model <- create_model_lstm_cnn()
    }

    if (built_model[[1]] == "create_model_lstm_cnn_target_middle") {
      formals(create_model_lstm_cnn_target_middle) <- default_arguments
      model <- create_model_lstm_cnn_target_middle()
    }

    if (built_model[[1]] == "create_model_wavenet") {
      if (!wavenet_format) {
        warning("Argument wavenet_format should be TRUE when using wavenet architecture.")
      }

      formals(create_model_wavenet) <- default_arguments
      model <- create_model_wavenet()
    }
  }

  model_weights <- model$get_weights()

  # function arguments
  argumentList <- as.list(match.call(expand.dots=FALSE))

  # extract maxlen from model
  if (is.null(set_learning)) {
    num_in_layers <- length(model$inputs)
    if (num_in_layers == 1) {
      maxlen <- model$input$shape[[2]]
    } else {
      if (!target_middle & !read_data & !split_seq) {
        maxlen <- model$input[[num_in_layers]]$shape[[2]]
      } else {
        maxlen <- model$inputs[[num_in_layers - 1]]$shape[[2]] + model$inputs[[num_in_layers]]$shape[[2]]
      }
    }
    samples_per_target <- NULL
    new_batch_size <- NULL
    reshape_mode <- NULL
  } else {
    reshape_mode <- set_learning$reshape_mode
    samples_per_target <- set_learning$samples_per_target
    buffer_len <- set_learning$buffer_len
    maxlen <- set_learning$maxlen

    if (reshape_mode == "concat") {
      buffer_size <- ifelse(is.null(set_learning$buffer_len), 0, set_learning$buffer_len)
      concat_maxlen <- (maxlen * samples_per_target) + (buffer_size * (samples_per_target - 1))
      if (any(c("z", "Z") %in% vocabulary) & !is.null(set_learning$buffer_len)) {
        stop("'Z' is used as token for separating sequences and can not be in vocabulary.")
      }
      if (!is.null(set_learning$buffer_len)) {
        vocabulary <- c(vocabulary, "Z")
      }

    }

    if (any(batch.size[1] != batch.size)) {
      stop("Set learning only implemented for uniform batch size for all classes.")
    }
    if (reshape_mode == "multi_input" & (length(model$inputs) != samples_per_target)) {
      stop_text <- paste("Model should have as many input layers as samples_per_target. Model has",
                         length(model$inputs), "input layers but samples_per_target =", samples_per_target)
      stop(stop_text)
    }
    new_batch_size <- batch.size
    batch.size <- samples_per_target * batch.size
  }

  label.vocabulary.size <- length(labelVocabulary)
  vocabulary.size <- length(vocabulary)

  # get solver and learning rate
  solver <- stringr::str_to_lower(model$optimizer$get_config()["name"])
  learning.rate <- keras::k_eval(model$optimizer$lr)
  if (solver == "adam") {
    optimizer <-  keras::optimizer_adam(lr = learning.rate)
  }
  if (solver == "adagrad") {
    optimizer <- keras::optimizer_adagrad(lr = learning.rate)
  }
  if (solver == "rmsprop") {
    optimizer <- keras::optimizer_rmsprop(lr = learning.rate)
  }
  if (solver == "sgd") {
    optimizer <- keras::optimizer_sgd(lr = learning.rate)
  }

  if (is.null(dataset) && labelByFolder) {
    if (length(path) == 1) warning("Training with just one label")
  }

  if (output$checkpoints) {
    # create folder for checkpoints using run.name
    # filenames contain epoch, validation loss and validation accuracy
    checkpoint_dir <- paste0(checkpoint_path, "/", run.name, "_checkpoints")
    dir.create(checkpoint_dir, showWarnings = FALSE)
    if (!is.list(model$output)) {
      filepath_checkpoints <- file.path(checkpoint_dir, "Ep.{epoch:03d}-val_loss{val_loss:.2f}-val_acc{val_acc:.3f}.hdf5")
    } else {
      filepath_checkpoints <- file.path(checkpoint_dir, "Ep.{epoch:03d}.hdf5")
      if (save_best_only) {
        warning("save_best_only not implemented for multi target. Setting save_best_only to FALSE")
        save_best_only <- FALSE
      }
    }
  }

  # Check if fileLog is unique
  if (!is.null(fileLog) && dir.exists(fileLog)) {
    stop(paste0("fileLog entry is already present. Please give this file a unique name."))
  }

  # Check if run.name is unique
  if (output$tensorboard && dir.exists(file.path(tensorboard.log, run.name))) {
    #tb_files <- list.files(file.path(tensorboard.log, run.name), full.names = TRUE, recursive = TRUE)
    #has_train_logs <- ifelse(dir.exists(file.path(tensorboard.log, run.name, "train")), TRUE, FALSE)
    #if (sum(file.size(tb_files)) > 100 | has_train_val_logs) {
    stop(paste0("Tensorboard entry '", run.name , "' is already present. Please give your run a unique name."))
    #} else {
    #  warning("Overwriting old tensorboard folder named '", run.name, "'. Folder did not contain any training logs.")
    #}
  }

  # add empty hparam dict if non exists
  if (!reticulate::py_has_attr(model, "hparam")) {
    model$hparam <- reticulate::dict()
  }

  # tempory file to log training data
  removeLog <- FALSE
  if (is.null(fileLog)) {
    removeLog <- TRUE
    fileLog <- tempfile(pattern = "", fileext = ".csv")
  } else {
    if (!endsWith(fileLog, ".csv")) fileLog <- paste0(fileLog, ".csv")
    #fileLogVal <- tempfile(pattern = "", fileext = ".csv")
  }
  if (reset_states) {
    fileLogVal <- tempfile(pattern = "", fileext = ".csv")
  } else {
    fileLogVal <- NULL
  }


  # if no dataset is supplied, external fasta generator will generate batches
  if (train_with_gen) {
    message("Starting fasta generator...")

    if (output_format == "dummy_gen") {
      gen <- dummy_gen(model, ifelse(is.null(set_learning), batch.size, new_batch_size))
      if (is.null(gen.val)) gen.val <- dummy_gen(model, ifelse(is.null(set_learning), batch.size, new_batch_size))
      removeLog <- FALSE
    } else {

      if (train_type == "lm") {

        # generator for training
        gen <- fastaFileGenerator(corpus.dir = path, batch.size = batch.size,
                                  maxlen = maxlen, step = step, randomFiles = randomFiles,
                                  vocabulary = vocabulary, seed = seed[1], proportion_entries = proportion_entries,
                                  shuffleFastaEntries = shuffleFastaEntries, format = format,
                                  fileLog = fileLog, reverseComplements = reverseComplements, n_gram_stride = n_gram_stride,
                                  output_format = output_format, ambiguous_nuc = ambiguous_nuc,
                                  proportion_per_file = proportion_per_file, skip_amb_nuc = skip_amb_nuc,
                                  use_quality_score = use_quality_score, padding = padding, n_gram = n_gram,
                                  added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                  max_samples = max_samples, concat_seq = NULL, target_len = target_len,
                                  file_filter = train_files, use_coverage = use_coverage, sample_by_file_size = sample_by_file_size)

        # generator for validation
        if (is.null(gen.val)) gen.val <- fastaFileGenerator(corpus.dir = path.val, batch.size = batch.size,
                                      maxlen = maxlen, step = step, randomFiles = randomFiles, n_gram_stride = n_gram_stride,
                                      vocabulary = vocabulary, seed = seed[2], proportion_entries = proportion_entries,
                                      shuffleFastaEntries = shuffleFastaEntries, format = format, n_gram = n_gram,
                                      fileLog = fileLogVal, reverseComplements = FALSE, sample_by_file_size = sample_by_file_size,
                                      output_format = output_format, skip_amb_nuc = skip_amb_nuc,
                                      ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                      use_quality_score = use_quality_score, padding = padding,
                                      added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                      max_samples = max_samples, concat_seq = NULL, target_len = target_len,
                                      file_filter = val_files, use_coverage = use_coverage)

      }
      # label by folder
      if (train_type == "label_folder") {

        # initialize training generators
        initializeGenerators(directories = path, format = format, batch.size = batch.size, maxlen = maxlen, vocabulary = vocabulary,
                             verbose = FALSE, randomFiles = randomFiles, step = step, showWarnings = FALSE, seed = seed[1],
                             shuffleFastaEntries = shuffleFastaEntries, numberOfFiles = numberOfFiles, skip_amb_nuc = skip_amb_nuc,
                             fileLog = fileLog, reverseComplements = reverseComplements, reverseComplementEncoding = reverseComplementEncoding, val = FALSE, ambiguous_nuc = ambiguous_nuc,
                             proportion_per_file = proportion_per_file, read_data = read_data, use_quality_score = use_quality_score,
                             padding = padding, max_samples = max_samples, split_seq = split_seq, concat_seq = concat_seq,
                             added_label_path = added_label_path, add_input_as_seq = add_input_as_seq, use_coverage = use_coverage,
                             set_learning = set_learning, proportion_entries = proportion_entries,
                             sample_by_file_size = sample_by_file_size, n_gram = n_gram)

        # initialize validation generators
        if (is.null(gen.val)) initializeGenerators(directories = path.val, format = format, batch.size = batch.size, maxlen = maxlen,
                             vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                             showWarnings = FALSE, seed = seed[2], shuffleFastaEntries = shuffleFastaEntries, skip_amb_nuc = skip_amb_nuc,
                             numberOfFiles = NULL, fileLog = fileLogVal, reverseComplements = FALSE, reverseComplementEncoding = reverseComplementEncoding, val = TRUE,
                             ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file, read_data = read_data,
                             use_quality_score = use_quality_score, padding = padding, max_samples = max_samples,
                             split_seq = split_seq, concat_seq = concat_seq, added_label_path = added_label_path,
                             add_input_as_seq = add_input_as_seq, use_coverage = use_coverage, set_learning = set_learning,
                             proportion_entries = proportion_entries, sample_by_file_size = sample_by_file_size,
                             n_gram = n_gram)

        gen <- labelByFolderGeneratorWrapper(val = FALSE, path = path,  new_batch_size = new_batch_size,
                                             samples_per_target = samples_per_target,
                                             batch.size = batch.size, voc_len = length(vocabulary),
                                             maxlen = maxlen, reshape_mode = reshape_mode,
                                             buffer_len = buffer_len, concat_maxlen = concat_maxlen)
        if (is.null(gen.val)) gen.val <- labelByFolderGeneratorWrapper(val = TRUE, path = path.val, new_batch_size = new_batch_size,
                                                 samples_per_target = samples_per_target,
                                                 batch.size = batch.size, voc_len = length(vocabulary),
                                                 maxlen = maxlen, reshape_mode = reshape_mode,
                                                 buffer_len = buffer_len,
                                                 concat_maxlen = concat_maxlen)
      }

      if (train_type == "label_csv" | train_type == "label_header") {

        # generator for training
        gen <- fastaLabelGenerator(corpus.dir = path, format = format, batch.size = batch.size, maxlen = maxlen,
                                   vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                                   showWarnings = FALSE, seed = seed[1], shuffleFastaEntries = shuffleFastaEntries,
                                   fileLog = fileLog, labelVocabulary = labelVocabulary, reverseComplements = reverseComplements,
                                   ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                   read_data = read_data, use_quality_score = use_quality_score, padding = padding,
                                   added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                   skip_amb_nuc = skip_amb_nuc, max_samples = max_samples, concat_seq = concat_seq,
                                   target_from_csv = target_from_csv, target_split = target_split, file_filter = train_files,
                                   use_coverage = use_coverage, proportion_entries = proportion_entries,
                                   sample_by_file_size = sample_by_file_size, n_gram = n_gram)

        # generator for validation
        gen.val <- fastaLabelGenerator(corpus.dir = path.val, format = format, batch.size = batch.size, maxlen = maxlen,
                                       vocabulary = vocabulary, verbose = FALSE, randomFiles = randomFiles, step = step,
                                       showWarnings = FALSE, seed = seed[2], shuffleFastaEntries = shuffleFastaEntries,
                                       fileLog = fileLogVal, labelVocabulary = labelVocabulary, reverseComplements = FALSE,
                                       ambiguous_nuc = ambiguous_nuc, proportion_per_file = proportion_per_file,
                                       added_label_path = added_label_path, add_input_as_seq = add_input_as_seq,
                                       read_data = read_data, use_quality_score = use_quality_score, padding = padding,
                                       skip_amb_nuc = skip_amb_nuc, max_samples = max_samples, concat_seq = concat_seq,
                                       target_from_csv = target_from_csv, target_split = target_split, file_filter = val_files,
                                       use_coverage = use_coverage, proportion_entries = proportion_entries,
                                       sample_by_file_size = sample_by_file_size, n_gram = n_gram)

      }

      if (train_type %in% c("label_rds", "lm_rds")) {
        reverseComplements <- FALSE
        step <- 1
        if (train_type == "label_rds") target_len <- NULL
        gen <- gen_rds(rds_folder = path, batch_size = batch.size, fileLog = fileLog,
                       max_samples = max_samples, proportion_per_file = proportion_per_file,
                       target_len = target_len, n_gram = n_gram, n_gram_stride = n_gram_stride)
        gen.val <- gen_rds(rds_folder = path.val, batch_size = batch.size,
                           max_samples = max_samples, proportion_per_file = proportion_per_file,
                           target_len = target_len, n_gram = n_gram, n_gram_stride = n_gram_stride)

      }
    }
  }

  # callbacks
  callbacks <- list()
  callback_names <- NULL

  if (reduce_lr_on_plateau) {
    if (is.list(model$outputs)) {
      monitor <- "val_loss"
    } else {
      monitor <- "val_acc"
    }
    callbacks[[1]] <- reduce_lr_cb(patience = patience, cooldown = cooldown,
                                   lr.plateau.factor = lr.plateau.factor,
                                   monitor = monitor)
    callback_names <- c("reduce_lr", callback_names)
  }

  if (output$log) {
    callbacks <- c(callbacks, log_cb(run.name))
    callback_names <- c("log", callback_names)
  }

  if (!output$tensorboard) tb_images <- FALSE
  if (output$tensorboard) {

    # add balanced acc score
    metrics <- model$metrics
    if (train_with_gen) {
      num_targets <- ifelse(train_type == "lm", length(vocabulary), length(labelVocabulary))
    } else {
      num_targets <- dim(dataset$Y)[2]
    }
    contains_macro_acc_metric <- FALSE
    for (i in 1:length(model$metrics)) {
      if (model$metrics[[i]]$name == "balanced_acc") contains_macro_acc_metric <- TRUE
    }

    if (!contains_macro_acc_metric) {
      if (tb_images) {
        if (!reticulate::py_has_attr(model, "cm_dir")) {
          cm_dir <- file.path(tempdir(), paste(sample(letters, 7), collapse = ""))
          dir.create(cm_dir)
          model$cm_dir <- cm_dir
        }
        metrics <- c(metrics, balanced_acc_wrapper(num_targets = num_targets, cm_dir = model$cm_dir))
      }
    }

    # count files in path
    if (train_type == "label_rds" | train_type == "lm_rds") format <- "rds"
    if (train_with_gen) {
      num_train_files <- count_files(path = path, format = format, train_type = train_type)
    } else {
      num_train_files <- 1
    }

    complete_tb <- tensorboard_complete_cb(default_arguments = default_arguments, model = model, tensorboard.log = tensorboard.log, run.name = run.name, train_type = train_type,
                                           model_path = model_path, path = path, validation.split = validation.split, batch.size = batch.size, epochs = epochs,
                                           max.queue.size = max.queue.size, lr.plateau.factor = lr.plateau.factor, patience = patience, cooldown = cooldown,
                                           steps.per.epoch = steps.per.epoch, step = step, randomFiles = randomFiles, initial_epoch = initial_epoch, vocabulary = vocabulary,
                                           learning.rate = learning.rate, shuffleFastaEntries = shuffleFastaEntries, labelVocabulary = labelVocabulary, solver = solver,
                                           numberOfFiles = numberOfFiles, reverseComplements = reverseComplements, wavenet_format = wavenet_format,  cnn_format = cnn_format,
                                           create_model_function = built_model$create_model_function, vocabulary.size = vocabulary.size, gen_cb = gen_cb, argumentList = argumentList,
                                           maxlen = maxlen, labelGen = labelGen, labelByFolder = labelByFolder, label.vocabulary.size = label.vocabulary.size, tb_images = FALSE,
                                           target_middle = target_middle, num_train_files = num_train_files, fileLog = fileLog, proportion_per_file = proportion_per_file,
                                           skip_amb_nuc = skip_amb_nuc, max_samples = max_samples, proportion_entries = proportion_entries,
                                           train_with_gen = train_with_gen)
    callbacks <- c(callbacks, complete_tb)
    callback_names <- c(callback_names, names(complete_tb))
  }

  if (output$checkpoints) {
    if (wavenet_format) {
      # can only save weights for wavenet
      save_weights_only <- TRUE
    }
    callbacks <- c(callbacks, checkpoint_cb(filepath = filepath_checkpoints, save_weights_only = save_weights_only,
                                            save_best_only = save_best_only))
    callback_names <- c(callback_names, "checkpoint")
  }

  if (reset_states) {
    callbacks <- c(callbacks, reset_states_cb(fileLog = fileLog, fileLogVal = fileLogVal))
    callback_names <- c(callback_names, "reset_states")
  }

  if (!is.null(early_stopping_time)) {
    callbacks <- c(callbacks, early_stopping_cb(early_stopping_patience = early_stopping_patience,
                                                early_stopping_time = early_stopping_time))
    callback_names <- c(callback_names, "early_stopping")
  }

  # skip validation callback
  if (validation_only_after_training | is.null(validation.split) || validation.split == 0) {
    validation_data <- NULL
  } else {
    if (train_with_gen) {
      validation_data <- gen.val
    }
  }
  validation_steps <- ceiling(steps.per.epoch * validation.split)

#  if (validation_only_after_training) {
    if (!train_with_gen) stop("Validation after training only implemented for generator")
    if (validation_steps > 0) callbacks <- c(callbacks, validation_after_training_cb(gen.val = gen.val, validation_steps = validation_steps))
    callback_names <- c(callback_names, "validation_after_training")
#  }

  if (tb_images) {
    if (is.list(model$output)) {
      warning("Tensorboard images (confusion matrix) not implemented for model with multiple outputs.
               Setting tb_images to FALSE")
      tb_images <- FALSE
    }

    if (model$loss == "binary_crossentropy") {
      warning("Tensorboard images (confusion matrix) not implemented for sigmoid activation in last layer.
               Setting tb_images to FALSE")
      tb_images <- FALSE
    }

    # if (!train_with_gen) {
    #   warning("Tensorboard images (confusion matrix) only implemented for generator.
    #            Setting tb_images to FALSE")
    #   tb_images <- FALSE
    # }
  }

  if (tb_images) {

    if (train_with_gen) {
      if (train_type == "lm") {
        confMatLabels <- vocabulary
      } else {
        confMatLabels <- labelVocabulary
      }
    } else {
      confMatLabels <- labelVocabulary
    }

    model %>% keras::compile(loss = model$loss,
                             optimizer = model$optimizer, metrics = metrics)

    callbacks <- c(callbacks, conf_matrix_cb(tensorboard.log = tensorboard.log,
                                             run.name = run.name,
                                             confMatLabels = confMatLabels,
                                             cm_dir = model$cm_dir))
    callback_names <- c(callback_names, "conf_matrix")
  }

  #names(callbacks) <- callback_names

  # training
  message("Start training ...")
  if (train_with_gen) {

    model <- keras::set_weights(model, model_weights)
    history <-
      model %>% keras::fit(
        x = gen,
        validation_data = validation_data,
        validation_steps = validation_steps,
        steps_per_epoch = steps.per.epoch,
        max_queue_size = max.queue.size,
        epochs = epochs,
        initial_epoch = initial_epoch,
        callbacks = callbacks,
        class_weight = class_weight,
        verbose = print_scores
      )

    # if (validation_only_after_training) {
    if (validation_steps) {
      history$val_loss <- model$val_loss
      history$val_acc <- model$val_acc
      model$val_loss <- NULL
      model$val_acc <- NULL
    }
  } else {

    model <- keras::set_weights(model, model_weights)
    if (!is.null(dataset_val)) validation.split <- 0.0

    history <- keras::fit(
      object = model,
      x = dataset$X,
      y = dataset$Y,
      batch_size = batch.size,
      validation_split = validation.split,
      validation_data = list(dataset_val[[1]], dataset_val[[2]]),
      callbacks = callbacks,
      epochs = epochs,
      verbose = print_scores)
  }

  if (removeLog & file.exists(fileLog)) {
    file.remove(fileLog)
  }

  # save final model
  message("Training done.")

  if (output$serialize_model) {
    Rmodel <-
      keras::serialize_model(model, include_optimizer = TRUE)
    save(Rmodel, file = paste0(run.name, "_full_model.Rdata"))
  }

  if (output$full_model) {
    keras::save_model_hdf5(
      model,
      paste0(run.name, "_full_model.hdf5"),
      overwrite = TRUE,
      include_optimizer = TRUE
    )
  }
  return(history)
}
va
